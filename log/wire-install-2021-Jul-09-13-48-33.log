[13:48:33] # Logging to: wire-install-2021-Jul-09-13-48-33.log in log/ and tmp/
[13:48:33] # Shutting down virtual machine "Wire Demo Client Clone"
[13:48:35] # Deleting virtual machine "Wire Demo Client Clone"
[13:48:36] # Creating virtual machine "Wire Demo Client Clone" from snapshot
[13:48:37] # Starting virtual machine "Wire Demo Client Clone"
[13:48:45] # Initial install of tmux on 192.168.1.121
[13:48:59] # Running command «sudo date --set="Fri Jul 09 2021 13:48:40 GMT+0200 (Central European Summer Time)"» on 192.168.1.121
        wire@wire-client:~$ sudo date --set="Fri Jul 09 2021 13:48:40 GMT+0200 (Central
        European Summer Time)"
        [sudo] password for wire:
        Fri Jul  9 11:48:40 UTC 2021
[13:49:08] # Running command «sudo date --set="Fri Jul 09 2021 13:48:59 GMT+0200 (Central European Summer Time)"» on 95.216.208.159
        wire@arthur-demo:~$ sudo date --set="Fri Jul 09 2021 13:48:59 GMT+0200 (Central
        European Summer Time)"
        Fri Jul  9 13:48:59 CEST 2021
[13:49:16] # Running command «cat /proc/cpuinfo | grep processor | wc -l» on 192.168.1.121
        wire@wire-client:~$ cat /proc/cpuinfo | grep processor | wc -l
        2
[13:49:25] # Running command «free -m && uptime» on 192.168.1.121
        wire@wire-client:~$ free -m && uptime
                      total        used        free      shared  buff/cache   available
        Mem:           2928         111        2623           0         194        2671
        Swap:          3943           0        3943
         11:49:06 up 2 min,  0 users,  load average: 0.55, 0.34, 0.13
[13:49:32] # Running command «cat /proc/cpuinfo | grep processor | wc -l» on 95.216.208.159
        wire@arthur-demo:~$ cat /proc/cpuinfo | grep processor | wc -l
        8
[13:49:40] # Running command «free -m && uptime» on 95.216.208.159
        wire@arthur-demo:~$ free -m && uptime
                      total        used        free      shared  buff/cache   available
        Mem:          15661        5591         324           4        9745       10135
        Swap:             0           0           0
         13:49:36 up 1 day, 18:40,  0 users,  load average: 1.22, 0.95, 1.05
[13:49:49] # Running command «cd» on 192.168.1.121
        wire@wire-client:~$ cd
[13:49:58] # Running command «cat /etc/hostname» on 192.168.1.121
        wire@wire-client:~$ cat /etc/hostname
        wire-client
[13:50:06] # Running command «pwd» on 192.168.1.121
        wire@wire-client:~$ pwd
        /home/wire
[13:50:14] # Running command «date» on 192.168.1.121
        wire@wire-client:~$ date
        Fri Jul  9 11:50:10 UTC 2021
[13:50:22] # Running command «uptime» on 192.168.1.121
        wire@wire-client:~$ uptime
         11:50:18 up 3 min,  0 users,  load average: 0.42, 0.34, 0.14
[13:50:45] # Running command «sudo apt update» on 192.168.1.121
        wire@wire-client:~$ sudo apt update
        [sudo] password for wire:
        Hit:1 http://fr.archive.ubuntu.com/ubuntu bionic InRelease
        Get:2 http://fr.archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]
        Get:3 http://fr.archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]
        Get:4 http://fr.archive.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]
        Get:5 http://fr.archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [21
        31 kB]
        Get:6 http://fr.archive.ubuntu.com/ubuntu bionic-updates/main Translation-en [42
        2 kB]
        Get:7 http://fr.archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packag
        es [389 kB]
        Get:8 http://fr.archive.ubuntu.com/ubuntu bionic-updates/restricted Translation-
        en [52.8 kB]
        Get:9 http://fr.archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages
         [1739 kB]
        Get:10 http://fr.archive.ubuntu.com/ubuntu bionic-updates/universe Translation-e
        n [371 kB]
        Get:11 http://fr.archive.ubuntu.com/ubuntu bionic-security/main amd64 Packages [
        1784 kB]
        Get:12 http://fr.archive.ubuntu.com/ubuntu bionic-security/main Translation-en [
        329 kB]
        Get:13 http://fr.archive.ubuntu.com/ubuntu bionic-security/restricted amd64 Pack
        ages [365 kB]
        Get:14 http://fr.archive.ubuntu.com/ubuntu bionic-security/restricted Translatio
        n-en [48.9 kB]
        Get:15 http://fr.archive.ubuntu.com/ubuntu bionic-security/universe amd64 Packag
        es [1130 kB]
        Get:16 http://fr.archive.ubuntu.com/ubuntu bionic-security/universe Translation-
        en [256 kB]
        Fetched 9270 kB in 3s (3079 kB/s)
        Reading package lists... Done
        Building dependency tree
        Reading state information... Done
        71 packages can be upgraded. Run 'apt list --upgradable' to see them.
[13:51:59] # Running command «sudo dpkg --configure -a» on 192.168.1.121
        wire@wire-client:~$ sudo dpkg --configure -a
        Setting up initramfs-tools (0.130ubuntu3.9) ...
        update-initramfs: deferring update (trigger activated)
        Processing triggers for initramfs-tools (0.130ubuntu3.9) ...
        update-initramfs: Generating /boot/initrd.img-4.15.0-144-generic
[13:53:31] # Running command «sudo apt install docker.io» on 192.168.1.121
        wire@wire-client:~$ sudo apt install docker.io
        Reading package lists... Done
        Building dependency tree
        Reading state information... Done
        The following additional packages will be installed:
          bridge-utils containerd pigz runc ubuntu-fan
        Suggested packages:
          ifupdown aufs-tools cgroupfs-mount | cgroup-lite debootstrap docker-doc
          rinse zfs-fuse | zfsutils
        The following NEW packages will be installed:
          bridge-utils containerd docker.io pigz runc ubuntu-fan
        0 upgraded, 6 newly installed, 0 to remove and 71 not upgraded.
        Need to get 72.1 MB of archives.
        After this operation, 351 MB of additional disk space will be used.
        Do you want to continue? [Y/n] y
        Get:1 http://fr.archive.ubuntu.com/ubuntu bionic/universe amd64 pigz amd64 2.4-1
         [57.4 kB]
        Get:2 http://fr.archive.ubuntu.com/ubuntu bionic/main amd64 bridge-utils amd64 1
        .5-15ubuntu1 [30.1 kB]
        Get:3 http://fr.archive.ubuntu.com/ubuntu bionic-updates/universe amd64 runc amd
        64 1.0.0~rc95-0ubuntu1~18.04.1 [4087 kB]
        Get:4 http://fr.archive.ubuntu.com/ubuntu bionic-updates/universe amd64 containe
        rd amd64 1.4.4-0ubuntu1~18.04.2 [31.0 MB]
        Get:5 http://fr.archive.ubuntu.com/ubuntu bionic-updates/universe amd64 docker.i
        o amd64 20.10.2-0ubuntu1~18.04.2 [36.9 MB]
        Get:6 http://fr.archive.ubuntu.com/ubuntu bionic/main amd64 ubuntu-fan all 0.12.
        10 [34.7 kB]
        Fetched 72.1 MB in 3s (25.1 MB/s)
        Preconfiguring packages ...
        Selecting previously unselected package pigz.
        (Reading database ... 67143 files and directories currently installed.)
        Preparing to unpack .../0-pigz_2.4-1_amd64.deb ...
        Unpacking pigz (2.4-1) ...
        Selecting previously unselected package bridge-utils.
        Preparing to unpack .../1-bridge-utils_1.5-15ubuntu1_amd64.deb ...
        Unpacking bridge-utils (1.5-15ubuntu1) ...
        Selecting previously unselected package runc.
        Preparing to unpack .../2-runc_1.0.0~rc95-0ubuntu1~18.04.1_amd64.deb ...
        Unpacking runc (1.0.0~rc95-0ubuntu1~18.04.1) ...
        Selecting previously unselected package containerd.
        Preparing to unpack .../3-containerd_1.4.4-0ubuntu1~18.04.2_amd64.deb ...
        Unpacking containerd (1.4.4-0ubuntu1~18.04.2) ...
        Selecting previously unselected package docker.io.
        Preparing to unpack .../4-docker.io_20.10.2-0ubuntu1~18.04.2_amd64.deb ...
        Unpacking docker.io (20.10.2-0ubuntu1~18.04.2) ...
        Selecting previously unselected package ubuntu-fan.
        Preparing to unpack .../5-ubuntu-fan_0.12.10_all.deb ...
        Unpacking ubuntu-fan (0.12.10) ...
        Setting up runc (1.0.0~rc95-0ubuntu1~18.04.1) ...
        Setting up containerd (1.4.4-0ubuntu1~18.04.2) ...
        Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service →
         /lib/systemd/system/containerd.service.
        Setting up bridge-utils (1.5-15ubuntu1) ...
        Setting up ubuntu-fan (0.12.10) ...
        Created symlink /etc/systemd/system/multi-user.target.wants/ubuntu-fan.service →
         /lib/systemd/system/ubuntu-fan.service.
        Setting up pigz (2.4-1) ...
        Setting up docker.io (20.10.2-0ubuntu1~18.04.2) ...
        Adding group `docker' (GID 113) ...
        Done.
        Created symlink /etc/systemd/system/multi-user.target.wants/docker.service → /li
        b/systemd/system/docker.service.
        Created symlink /etc/systemd/system/sockets.target.wants/docker.socket → /lib/sy
        stemd/system/docker.socket.
        Processing triggers for systemd (237-3ubuntu10.42) ...
        Processing triggers for man-db (2.8.3-2ubuntu0.1) ...
        Processing triggers for ureadahead (0.100.0-21) ...
[13:53:39] # Running command «docker -v» on 192.168.1.121
        wire@wire-client:~$ docker -v
        Docker version 20.10.2, build 20.10.2-0ubuntu1~18.04.2
[13:53:47] # Running command «sudo rm -rf /home/wire/wire*» on 192.168.1.121
        wire@wire-client:~$ sudo rm -rf /home/wire/wire*
[13:53:54] # Running command «ls -l /home/wire/» on 192.168.1.121
        wire@wire-client:~$ ls -l /home/wire/
        total 0
[13:54:02] # Running command «date» on 192.168.1.121
        wire@wire-client:~$ date
        Fri Jul  9 11:53:59 UTC 2021
[13:54:10] # Running command «uptime» on 192.168.1.121
        wire@wire-client:~$ uptime
         11:54:06 up 6 min,  0 users,  load average: 0.65, 0.63, 0.31
[13:54:21] # Running command «sudo apt update» on 192.168.1.121
        wire@wire-client:~$ sudo apt update
        Hit:1 http://fr.archive.ubuntu.com/ubuntu bionic InRelease
        Hit:2 http://fr.archive.ubuntu.com/ubuntu bionic-updates InRelease
        Hit:3 http://fr.archive.ubuntu.com/ubuntu bionic-backports InRelease
        Hit:4 http://fr.archive.ubuntu.com/ubuntu bionic-security InRelease
        Reading package lists... Done
        Building dependency tree
        Reading state information... Done
        71 packages can be upgraded. Run 'apt list --upgradable' to see them.
[13:54:29] # Running command «sudo dpkg --configure -a» on 192.168.1.121
        wire@wire-client:~$ sudo dpkg --configure -a
[13:54:36] # Running command «date» on 95.216.208.159
        wire@arthur-demo:~$ date
        Fri Jul  9 13:54:32 CEST 2021
[13:54:44] # Running command «uptime» on 95.216.208.159
        wire@arthur-demo:~$ uptime
         13:54:40 up 1 day, 18:45,  0 users,  load average: 1.09, 0.96, 1.02
[13:55:00] # Running command «sudo apt update» on 95.216.208.159
        wire@arthur-demo:~$ sudo apt update
        Hit:1 http://mirror.hetzner.de/ubuntu/packages bionic InRelease
        Hit:2 https://download.docker.com/linux/ubuntu bionic InRelease
        Get:3 http://mirror.hetzner.de/ubuntu/packages bionic-updates InRelease [88.7 kB
        ]
        Hit:4 https://mirror.hetzner.com/ubuntu/packages bionic InRelease
        Get:5 http://mirror.hetzner.de/ubuntu/packages bionic-backports InRelease [74.6
        kB]
        Get:6 http://mirror.hetzner.de/ubuntu/packages bionic-security InRelease [88.7 k
        B]
        Get:7 https://mirror.hetzner.com/ubuntu/packages bionic-updates InRelease [88.7
        kB]
        Get:8 https://mirror.hetzner.com/ubuntu/packages bionic-backports InRelease [74.
        6 kB]
        Get:9 https://mirror.hetzner.com/ubuntu/security bionic-security InRelease [88.7
         kB]
        Get:10 http://mirror.hetzner.de/ubuntu/packages bionic-updates/main amd64 Packag
        es [2,131 kB]
        Get:11 http://mirror.hetzner.de/ubuntu/packages bionic-updates/main i386 Package
        s [1,312 kB]
        Get:12 http://mirror.hetzner.de/ubuntu/packages bionic-updates/main Translation-
        en [422 kB]
        Get:13 https://mirror.hetzner.com/ubuntu/packages bionic-updates/main amd64 Pack
        ages [2,131 kB]
        Get:14 https://mirror.hetzner.com/ubuntu/packages bionic-updates/main i386 Packa
        ges [1,312 kB]
        Get:15 https://mirror.hetzner.com/ubuntu/packages bionic-updates/main Translatio
        n-en [422 kB]
        Fetched 8,234 kB in 3s (2,777 kB/s)
        Reading package lists... Done
        Building dependency tree
        Reading state information... Done
        3 packages can be upgraded. Run 'apt list --upgradable' to see them.
[13:55:07] # Running command «sudo dpkg --configure -a» on 95.216.208.159
        wire@arthur-demo:~$ sudo dpkg --configure -a
[13:55:15] # Running command «sudo apt install docker.io» on 192.168.1.121
        wire@wire-client:~$ sudo apt install docker.io
        Reading package lists... Done
        Building dependency tree
        Reading state information... Done
        docker.io is already the newest version (20.10.2-0ubuntu1~18.04.2).
        0 upgraded, 0 newly installed, 0 to remove and 71 not upgraded.
[13:55:23] # Running command «docker -v» on 192.168.1.121
        wire@wire-client:~$ docker -v
        Docker version 20.10.2, build 20.10.2-0ubuntu1~18.04.2
[13:55:33] # Running command «git clone --branch master https://github.com/wireapp/wire-server-deploy.git» on 192.168.1.121
        wire@wire-client:~$ git clone --branch master https://github.com/wireapp/wire-se
        rver-deploy.git
        Cloning into 'wire-server-deploy'...
        remote: Enumerating objects: 18498, done.
        remote: Counting objects: 100% (331/331), done.
        remote: Compressing objects: 100% (210/210), done.
        remote: Total 18498 (delta 163), reused 232 (delta 115), pack-reused 18167
        Receiving objects: 100% (18498/18498), 2.19 MiB | 6.63 MiB/s, done.
        Resolving deltas: 100% (13819/13819), done.
[13:55:41] # Running command «ls -l» on 192.168.1.121
        wire@wire-client:~$ ls -l
        total 4
        drwxrwxr-x 12 wire wire 4096 Jul  9 11:55 wire-server-deploy
[13:55:49] # Running command «cd ~/wire-server-deploy» on 192.168.1.121
        wire@wire-client:~$ cd ~/wire-server-deploy
[13:55:57] # Running command «pwd» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy$ pwd
        /home/wire/wire-server-deploy
[13:56:05] # Running command «ls -l» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy$ ls -l
        total 108
        -rw-rw-r--  1 wire wire 16072 Jul  9 11:55 CHANGELOG.md
        -rw-rw-r--  1 wire wire  1531 Jul  9 11:55 CONTRIBUTING.md
        -rw-rw-r--  1 wire wire   252 Jul  9 11:55 Dockerfile
        -rw-rw-r--  1 wire wire 34520 Jul  9 11:55 LICENSE
        -rw-rw-r--  1 wire wire  5893 Jul  9 11:55 Makefile
        -rw-rw-r--  1 wire wire  2251 Jul  9 11:55 README.md
        drwxrwxr-x  9 wire wire  4096 Jul  9 11:55 ansible
        drwxrwxr-x  3 wire wire  4096 Jul  9 11:55 bin
        -rw-rw-r--  1 wire wire  2338 Jul  9 11:55 default.nix
        drwxrwxr-x  5 wire wire  4096 Jul  9 11:55 examples
        drwxrwxr-x  2 wire wire  4096 Jul  9 11:55 helm
        drwxrwxr-x  4 wire wire  4096 Jul  9 11:55 nix
        drwxrwxr-x  2 wire wire  4096 Jul  9 11:55 offline
        drwxrwxr-x  5 wire wire  4096 Jul  9 11:55 terraform
        drwxrwxr-x 15 wire wire  4096 Jul  9 11:55 values
[13:56:34] # Running command «git submodule update --init --recursive» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy$ git submodule update --init --recursive
        Submodule 'ansible/roles-external/ANXS.apt' (https://github.com/ANXS/apt.git) re
        gistered for path 'ansible/roles-external/ANXS.apt'
        Submodule 'ansible/roles-external/admin_users' (https://github.com/cchurch/ansib
        le-role-admin-users.git) registered for path 'ansible/roles-external/admin_users
        '
        Submodule 'ansible/roles-external/andrewrothstein.unarchive-deps' (https://githu
        b.com/andrewrothstein/ansible-unarchive-deps) registered for path 'ansible/roles
        -external/andrewrothstein.unarchive-deps'
        Submodule 'ansible/roles-external/ansible-cassandra' (https://github.com/wireapp
        /ansible-cassandra.git) registered for path 'ansible/roles-external/ansible-cass
        andra'
        Submodule 'ansible/roles-external/ansible-minio' (https://github.com/wireapp/ans
        ible-minio.git) registered for path 'ansible/roles-external/ansible-minio'
        Submodule 'ansible/roles-external/ansible-ntp-verify' (https://github.com/wireap
        p/ansible-ntp-verify.git) registered for path 'ansible/roles-external/ansible-nt
        p-verify'
        Submodule 'ansible/roles-external/ansible-restund' (https://github.com/wireapp/a
        nsible-restund.git) registered for path 'ansible/roles-external/ansible-restund'
        Submodule 'ansible/roles-external/ansible-role-java' (https://github.com/geerlin
        gguy/ansible-role-java.git) registered for path 'ansible/roles-external/ansible-
        role-java'
        Submodule 'ansible/roles-external/ansible-role-ntp' (https://github.com/geerling
        guy/ansible-role-ntp.git) registered for path 'ansible/roles-external/ansible-ro
        le-ntp'
        Submodule 'ansible/roles-external/ansible-tinc' (https://github.com/wireapp/ansi
        ble-tinc.git) registered for path 'ansible/roles-external/ansible-tinc'
        Submodule 'ansible/roles-external/cloudalchemy.node-exporter' (https://github.co
        m/cloudalchemy/ansible-node-exporter) registered for path 'ansible/roles-externa
        l/cloudalchemy.node-exporter'
        Submodule 'ansible/roles-external/elasticsearch' (https://github.com/elastic/ans
        ible-elasticsearch.git) registered for path 'ansible/roles-external/elasticsearc
        h'
        Submodule 'ansible/roles-external/hostname' (https://github.com/ANXS/hostname.gi
        t) registered for path 'ansible/roles-external/hostname'
        Submodule 'ansible/roles-external/kubespray' (https://github.com/kubernetes-sigs
        /kubespray.git) registered for path 'ansible/roles-external/kubespray'
        Submodule 'ansible/roles-external/logrotate' (https://github.com/nickhammond/ans
        ible-logrotate.git) registered for path 'ansible/roles-external/logrotate'
        Submodule 'ansible/roles-external/sft' (https://github.com/wireapp/ansible-sft.g
        it) registered for path 'ansible/roles-external/sft'
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/ANXS.apt'...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/admin_users'.
        ..
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/andrewrothste
        in.unarchive-deps'...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/ansible-cassa
        ndra'...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/ansible-minio
        '...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/ansible-ntp-v
        erify'...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/ansible-restu
        nd'...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/ansible-role-
        java'...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/ansible-role-
        ntp'...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/ansible-tinc'
        ...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/cloudalchemy.
        node-exporter'...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/elasticsearch
        '...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/hostname'...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/kubespray'...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/logrotate'...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/sft'...
        Submodule path 'ansible/roles-external/ANXS.apt': checked out 'f602ba7e88abfbb3a
        f6679a8ca47207dc3e9d9c4'
        Submodule path 'ansible/roles-external/admin_users': checked out 'd5bcef7e925ee1
        acf4e42359f0a95ed788eea58f'
        Submodule path 'ansible/roles-external/andrewrothstein.unarchive-deps': checked
        out '4485543262cfe04170d1ec02c8ccb95c44a7a222'
        Submodule path 'ansible/roles-external/ansible-cassandra': checked out '8a0f029f
        533856e8270d7fd74d75c099a677b2e3'
        Submodule path 'ansible/roles-external/ansible-minio': checked out '22ab28f75c00
        7a0c48dc47db574773773ef19d22'
        Submodule path 'ansible/roles-external/ansible-ntp-verify': checked out '4c3d0c6
        7d32d2d74444f4db45b2a4d2efdc7d590'
        Submodule path 'ansible/roles-external/ansible-restund': checked out '9807313a7c
        72ffa40e74f69d239404fd87db65ab'
        Submodule path 'ansible/roles-external/ansible-role-java': checked out '13b42705
        5702d9e91558cad7dcccab7db91e5663'
        Submodule path 'ansible/roles-external/ansible-role-ntp': checked out 'af1ec6238
        5c899a3e3f24407d8417adcdc9eea60'
        Submodule path 'ansible/roles-external/ansible-tinc': checked out '42951a951f638
        1e387174178bf3bff228b6a5dc5'
        Submodule path 'ansible/roles-external/cloudalchemy.node-exporter': checked out
        '8dc13ae077e3da1a71c268b114cd4fb8103ced80'
        Submodule path 'ansible/roles-external/elasticsearch': checked out '389a3ff45f8f
        51de95313ca0354cedcdc92b16f4'
        Submodule path 'ansible/roles-external/hostname': checked out 'da6f329b2984e84d2
        248d4251e0c679c53dfbb30'
        Submodule path 'ansible/roles-external/kubespray': checked out 'c7658c0256bfeb50
        913ac73f638a760680e4dd6d'
        Submodule path 'ansible/roles-external/logrotate': checked out '91d570f68c44261d
        2051a99a2b3c7d736306bf0d'
        Submodule path 'ansible/roles-external/sft': checked out '126fde847dfc9deffeef8b
        a133c541c16628a63a'
[13:56:42] # Running command «mkdir -p wire-server» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy$ mkdir -p wire-server
[13:56:50] # Running command «cd wire-server» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy$ cd wire-server
[13:56:57] # Running command «pwd» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ pwd
        /home/wire/wire-server-deploy/wire-server
[13:57:05] # Running command «curl -sSL https://raw.githubusercontent.com/wireapp/wire-server-deploy/master/values/wire-server/demo-secrets.example.yaml > secrets.yaml» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ curl -sSL https://raw.githubu
        sercontent.com/wireapp/wire-server-deploy/master/values/wire-server/demo-secrets
        .example.yaml > secrets.yaml
[13:57:13] # Running command «curl -sSL https://raw.githubusercontent.com/wireapp/wire-server-deploy/master/values/wire-server/demo-values.example.yaml > values.yaml» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ curl -sSL https://raw.githubu
        sercontent.com/wireapp/wire-server-deploy/master/values/wire-server/demo-values.
        example.yaml > values.yaml
[13:57:21] # Running command «openssl rand -base64 64 | env LC_CTYPE=C tr -dc a-zA-Z0-9 | head -c 42 > restund.txt» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ openssl rand -base64 64 | env
         LC_CTYPE=C tr -dc a-zA-Z0-9 | head -c 42 > restund.txt
[13:58:40] # Running command «sudo docker run --rm quay.io/wire/alpine-intermediate /dist/zauth -m gen-keypair -i 1 > zauth.txt» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ sudo docker run --rm quay.io/
        wire/alpine-intermediate /dist/zauth -m gen-keypair -i 1 > zauth.txt
        Unable to find image 'quay.io/wire/alpine-intermediate:latest' locally
        latest: Pulling from wire/alpine-intermediate
        188c0c94c7c5: Pulling fs layer
        edebdc4a6437: Pulling fs layer
        d9b6d5749552: Pulling fs layer
        0dd342b35df2: Pulling fs layer
        ea7c73d2e35b: Pulling fs layer
        0dd342b35df2: Waiting
        ea7c73d2e35b: Waiting
        edebdc4a6437: Download complete
        188c0c94c7c5: Verifying Checksum
        188c0c94c7c5: Download complete
        ea7c73d2e35b: Verifying Checksum
        ea7c73d2e35b: Download complete
        d9b6d5749552: Download complete
        188c0c94c7c5: Pull complete
        edebdc4a6437: Pull complete
        d9b6d5749552: Pull complete
        0dd342b35df2: Verifying Checksum
        0dd342b35df2: Download complete
        0dd342b35df2: Pull complete
        ea7c73d2e35b: Pull complete
        Digest: sha256:bfb7dfd21e389ec699527f3cdad32d158ecab12577404c4578001af7c899d044
        Status: Downloaded newer image for quay.io/wire/alpine-intermediate:latest
[13:58:47] # Running command «ls -l» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ ls -l
        total 20
        -rw-rw-r-- 1 wire wire   42 Jul  9 11:57 restund.txt
        -rw-rw-r-- 1 wire wire 1949 Jul  9 11:57 secrets.yaml
        -rw-rw-r-- 1 wire wire 7642 Jul  9 11:57 values.yaml
        -rw-rw-r-- 1 wire wire  150 Jul  9 11:58 zauth.txt
[13:58:58] # Running command «curl -sSL https://raw.githubusercontent.com/wireapp/wire-server-deploy/master/values/wire-server/prod-values.example.yaml > prod-values.yaml» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ curl -sSL https://raw.githubu
        sercontent.com/wireapp/wire-server-deploy/master/values/wire-server/prod-values.
        example.yaml > prod-values.yaml
[13:59:06] # Running command «grep 'spar:' prod-values.yaml -A 24 > spar.yaml» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ grep 'spar:' prod-values.yaml
         -A 24 > spar.yaml
[13:59:14] # Running command «sed -i 's/cassandra-external/cassandra-ephemeral/' spar.yaml» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ sed -i 's/cassandra-external/
        cassandra-ephemeral/' spar.yaml
[13:59:22] # Running command «ls -l» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ ls -l
        total 40
        -rw-rw-r-- 1 wire wire 12606 Jul  9 11:58 prod-values.yaml
        -rw-rw-r-- 1 wire wire    42 Jul  9 11:57 restund.txt
        -rw-rw-r-- 1 wire wire  1949 Jul  9 11:57 secrets.yaml
        -rw-rw-r-- 1 wire wire   744 Jul  9 11:59 spar.yaml
        -rw-rw-r-- 1 wire wire  7642 Jul  9 11:57 values.yaml
        -rw-rw-r-- 1 wire wire   150 Jul  9 11:58 zauth.txt
[13:59:30] # Running command «cat restund.txt && echo» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ cat restund.txt && echo
        ex7Pdkg7WA77uvjTh21xOUr56xq6Hkw22tkgOfe2YF
[13:59:38] # Running command «cat zauth.txt» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ cat zauth.txt
        public: fGcTQpkN2Qv_ZNUyw41siIcs3-_PpyxOz4kUSkGLYJM=
        secret: epi4uAw5JdklKPvnai_WsTzMBe-_svX37edvCnbSHah8ZxNCmQ3ZC_9k1TLDjWyIhyzf78-n
        LE7PiRRKQYtgkw==
[13:59:45] # Running command «sed -i 's/secret:$/secret: ex7Pdkg7WA77uvjTh21xOUr56xq6Hkw22tkgOfe2YF/' secrets.yaml» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ sed -i 's/secret:$/secret: ex
        7Pdkg7WA77uvjTh21xOUr56xq6Hkw22tkgOfe2YF/' secrets.yaml
[13:59:53] # Running command «sed -i 's/publicKeys: "<public key>"/publicKeys: "fGcTQpkN2Qv_ZNUyw41siIcs3-_PpyxOz4kUSkGLYJM="/' secrets.yaml» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ sed -i 's/publicKeys: "<publi
        c key>"/publicKeys: "fGcTQpkN2Qv_ZNUyw41siIcs3-_PpyxOz4kUSkGLYJM="/' secrets.yam
        l
[14:00:01] # Running command «sed -i 's/privateKeys: "<private key>"/privateKeys: "epi4uAw5JdklKPvnai_WsTzMBe-_svX37edvCnbSHah8ZxNCmQ3ZC_9k1TLDjWyIhyzf78-nLE7PiRRKQYtgkw=="/' secrets.yaml» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ sed -i 's/privateKeys: "<priv
        ate key>"/privateKeys: "epi4uAw5JdklKPvnai_WsTzMBe-_svX37edvCnbSHah8ZxNCmQ3ZC_9k
        1TLDjWyIhyzf78-nLE7PiRRKQYtgkw=="/' secrets.yaml
[14:00:09] # Running command «cat secrets.yaml | grep 'ex7Pdkg7WA77uvjTh21xOUr56xq6Hkw22tkgOfe2YF'» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ cat secrets.yaml | grep 'ex7P
        dkg7WA77uvjTh21xOUr56xq6Hkw22tkgOfe2YF'
              secret: ex7Pdkg7WA77uvjTh21xOUr56xq6Hkw22tkgOfe2YF
[14:00:17] # Running command «cat secrets.yaml | grep 'fGcTQpkN2Qv_ZNUyw41siIcs3-_PpyxOz4kUSkGLYJM='» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ cat secrets.yaml | grep 'fGcT
        QpkN2Qv_ZNUyw41siIcs3-_PpyxOz4kUSkGLYJM='
              publicKeys: "fGcTQpkN2Qv_ZNUyw41siIcs3-_PpyxOz4kUSkGLYJM="
              publicKeys: "fGcTQpkN2Qv_ZNUyw41siIcs3-_PpyxOz4kUSkGLYJM="
[14:00:24] # Running command «cat secrets.yaml | grep 'epi4uAw5JdklKPvnai_WsTzMBe-_svX37edvCnbSHah8ZxNCmQ3ZC_9k1TLDjWyIhyzf78-nLE7PiRRKQYtgkw=='» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ cat secrets.yaml | grep 'epi4
        uAw5JdklKPvnai_WsTzMBe-_svX37edvCnbSHah8ZxNCmQ3ZC_9k1TLDjWyIhyzf78-nLE7PiRRKQYtg
        kw=='
              privateKeys: "epi4uAw5JdklKPvnai_WsTzMBe-_svX37edvCnbSHah8ZxNCmQ3ZC_9k1TLD
        jWyIhyzf78-nLE7PiRRKQYtgkw=="
[14:00:32] # Running command «cat values.yaml | grep spar» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ cat values.yaml | grep spar
          spar: false # enable if you want/need Single-Sign-On (SSO)
[14:00:40] # Running command «sed -i 's/spar: false/spar: true/' values.yaml» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ sed -i 's/spar: false/spar: t
        rue/' values.yaml
[14:00:48] # Running command «cat spar.yaml >> values.yaml» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ cat spar.yaml >> values.yaml
[14:00:55] # Running command «cat values.yaml | grep spar» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ cat values.yaml | grep spar
          spar: true # enable if you want/need Single-Sign-On (SSO)
        spar:
[14:01:03] # Running command «cd ~/wire-server-deploy» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ cd ~/wire-server-deploy
[14:01:11] # Running command «pwd» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy$ pwd
        /home/wire/wire-server-deploy
[14:01:19] # Running command «ls -l» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy$ ls -l
        total 112
        -rw-rw-r--  1 wire wire 16072 Jul  9 11:55 CHANGELOG.md
        -rw-rw-r--  1 wire wire  1531 Jul  9 11:55 CONTRIBUTING.md
        -rw-rw-r--  1 wire wire   252 Jul  9 11:55 Dockerfile
        -rw-rw-r--  1 wire wire 34520 Jul  9 11:55 LICENSE
        -rw-rw-r--  1 wire wire  5893 Jul  9 11:55 Makefile
        -rw-rw-r--  1 wire wire  2251 Jul  9 11:55 README.md
        drwxrwxr-x  9 wire wire  4096 Jul  9 11:55 ansible
        drwxrwxr-x  3 wire wire  4096 Jul  9 11:55 bin
        -rw-rw-r--  1 wire wire  2338 Jul  9 11:55 default.nix
        drwxrwxr-x  5 wire wire  4096 Jul  9 11:55 examples
        drwxrwxr-x  2 wire wire  4096 Jul  9 11:55 helm
        drwxrwxr-x  4 wire wire  4096 Jul  9 11:55 nix
        drwxrwxr-x  2 wire wire  4096 Jul  9 11:55 offline
        drwxrwxr-x  5 wire wire  4096 Jul  9 11:55 terraform
        drwxrwxr-x 15 wire wire  4096 Jul  9 11:55 values
        drwxrwxr-x  2 wire wire  4096 Jul  9 12:00 wire-server
[14:01:27] # Running command «WSD_CONTAINER=quay.io/wire/wire-server-deploy:cdc1c84c1a10a4f5f1b77b51ee5655d0da7f9518» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy$ WSD_CONTAINER=quay.io/wire/wire-server-de
        ploy:cdc1c84c1a10a4f5f1b77b51ee5655d0da7f9518
[14:03:26] # Running command «sudo docker run -it --network=host -v ${SSH_AUTH_SOCK:-nonexistent}:/ssh-agent -v $HOME/.ssh:/root/.ssh -v $PWD:/wire-server-deploy -e SSH_AUTH_SOCK=/ssh-agent $WSD_CONTAINER bash» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy$ sudo docker run -it --network=host -v ${S
        SH_AUTH_SOCK:-nonexistent}:/ssh-agent -v $HOME/.ssh:/root/.ssh -v $PWD:/wire-ser
        ver-deploy -e SSH_AUTH_SOCK=/ssh-agent $WSD_CONTAINER bash
        Unable to find image 'quay.io/wire/wire-server-deploy:cdc1c84c1a10a4f5f1b77b51ee
        5655d0da7f9518' locally
        cdc1c84c1a10a4f5f1b77b51ee5655d0da7f9518: Pulling from wire/wire-server-deploy
        083086456ea5: Pull complete
        ae57e4902e57: Pull complete
        Digest: sha256:a18a5fae78d5ffb25e95e43ed899d840f5032890b76bdc4fb89a5669c1a60549
        Status: Downloaded newer image for quay.io/wire/wire-server-deploy:cdc1c84c1a10a
        4f5f1b77b51ee5655d0da7f9518
[14:03:33] # Running command «ansible --version» on 192.168.1.121
        bash-4.4# ansible --version
        ansible 2.9.12
          config file = /wire-server-deploy/ansible/ansible.cfg
          configured module search path = ['/root/.ansible/plugins/modules', '/usr/share
        /ansible/plugins/modules']
          ansible python module location = /nix/store/407c5v2x920kri79w0sb9gb1g82q28mg-p
        ython3.8-ansible-2.9.12/lib/python3.8/site-packages/ansible
          executable location = /nix/store/407c5v2x920kri79w0sb9gb1g82q28mg-python3.8-an
        sible-2.9.12/bin/ansible
          python version = 3.8.8 (default, Feb 19 2021, 11:04:50) [GCC 10.2.0]
[14:03:41] # Running command «cd ansible» on 192.168.1.121
        bash-4.4# cd ansible
[14:03:48] # Running command «pwd» on 192.168.1.121
        bash-4.4# pwd
        /wire-server-deploy/ansible
[14:03:56] # Running command «cp inventory/demo/hosts.example.ini inventory/demo/hosts.ini» on 192.168.1.121
        bash-4.4# cp inventory/demo/hosts.example.ini inventory/demo/hosts.ini
[14:04:04] # Running command «ls -l inventory/demo/hosts.ini» on 192.168.1.121
        bash-4.4# ls -l inventory/demo/hosts.ini
        -rw-r--r-- 1 root root 1066 Jul  9 12:03 inventory/demo/hosts.ini
[14:04:11] # Running command «sed -i 's/X.X.X.X/95.216.208.159/g' inventory/demo/hosts.ini» on 192.168.1.121
        bash-4.4# sed -i 's/X.X.X.X/95.216.208.159/g' inventory/demo/hosts.ini
[14:04:19] # Running command «cat inventory/demo/hosts.ini | grep ansible_host» on 192.168.1.121
        bash-4.4# cat inventory/demo/hosts.ini | grep ansible_host
        # * 'ansible_host' is the IP to ssh into
        kubenode01    ansible_host=95.216.208.159 etcd_member_name=etcd1
[14:04:27] # Running command «ssh-keygen -f /root/.ssh/id_rsa -t rsa -P ''» on 192.168.1.121
        bash-4.4# ssh-keygen -f /root/.ssh/id_rsa -t rsa -P ''
        Generating public/private rsa key pair.
        Your identification has been saved in /root/.ssh/id_rsa
        Your public key has been saved in /root/.ssh/id_rsa.pub
        The key fingerprint is:
        SHA256:9lcjmnXiGtGe2Taik2KvoN2gXl1TB9RRvIuUJbA0BFw root@wire-client
        The key's randomart image is:
        +---[RSA 3072]----+
        |        ..+E+..+.|
        |         .. oo...|
        |           .. = .|
        |           o + . |
        |        S + * = .|
        |       o o O X o |
        |      + . =.B +  |
        |     = +o o= o . |
        |   .+ ..o++.     |
        +----[SHA256]-----+
[14:04:42] # Running command «ssh-copy-id wire@95.216.208.159» on 192.168.1.121
        bash-4.4# ssh-copy-id wire@95.216.208.159
        /bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/root/.ssh/id_rsa.pub
        "
        The authenticity of host '95.216.208.159 (95.216.208.159)' can't be established.
        ECDSA key fingerprint is SHA256:YT8wUJWXwDNyf6vSozbixVlBGvWpFojuaYRJH7kX7bA.
        Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
        /bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out
        any that are already installed
        expr: warning: '^ERROR: ': using '^' as the first character
        of a basic regular expression is not portable; it is ignored
        /bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted n
        ow it is to install the new keys
        wire@95.216.208.159's password:
        
        Number of key(s) added: 1
        
        Now try logging into the machine, with:   "ssh 'wire@95.216.208.159'"
        and check to make sure that only the key(s) you wanted were added.
        
[14:04:50] # Running command «cat /etc/hostname» on 95.216.208.159
        wire@arthur-demo:~$ cat /etc/hostname
        kubenode01
[14:04:57] # Running command «sudo tail -n 2 /etc/sudoers» on 95.216.208.159
        wire@arthur-demo:~$ sudo tail -n 2 /etc/sudoers
        wire ALL=(ALL) NOPASSWD:ALL
        wire ALL=(ALL) NOPASSWD:ALL
[14:05:03] # Running command «echo 'wire ALL=(ALL) NOPASSWD:ALL' | sudo tee -a /etc/sudoers» on 95.216.208.159
        wire@arthur-demo:~$ echo 'wire ALL=(ALL) NOPASSWD:ALL' | sudo tee -a /etc/sudoer
        s
        wire ALL=(ALL) NOPASSWD:ALL
[14:05:11] # Running command «sudo tail -n 2 /etc/sudoers» on 95.216.208.159
        wire@arthur-demo:~$ sudo tail -n 2 /etc/sudoers
        wire ALL=(ALL) NOPASSWD:ALL
        wire ALL=(ALL) NOPASSWD:ALL
[14:05:19] # Running command «cat inventory/demo/hosts.ini | grep ansible_user» on 192.168.1.121
        bash-4.4# cat inventory/demo/hosts.ini | grep ansible_user
        # ansible_user = ...
[14:05:26] # Running command «sed -i 's/# ansible_user = .../ansible_user = wire/g' inventory/demo/hosts.ini» on 192.168.1.121
        bash-4.4# sed -i 's/# ansible_user = .../ansible_user = wire/g' inventory/demo/h
        osts.ini
[14:05:34] # Running command «cat inventory/demo/hosts.ini | grep ansible_user» on 192.168.1.121
        bash-4.4# cat inventory/demo/hosts.ini | grep ansible_user
        ansible_user = wire
[14:11:33] # Running command «ansible-playbook -i inventory/demo/hosts.ini kubernetes.yml -vv» on 192.168.1.121
        was False"}
        
        TASK [kubernetes-apps/persistent_volumes/gcp-pd-csi : Kubernetes Persistent Volu
        mes | Copy GCP PD CSI Storage Class template] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/persistent_volumes/gcp-pd-csi/tasks/main.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/persistent_volumes/gcp-pd-csi : Kubernetes Persistent Volu
        mes | Add GCP PD CSI Storage Class] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/persistent_volumes/gcp-pd-csi/tasks/main.yml:10
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/snapshots/snapshot-controller : Snapshot Controller | Gene
        rate Manifests] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/snapshots/snapshot-controller/tasks/main.yml:2
        skipping: [kubenode01] => (item={'name': 'rbac-snapshot-controller', 'file': 'rb
        ac-snapshot-controller.yml'})  => {"ansible_loop_var": "item", "changed": false,
         "item": {"file": "rbac-snapshot-controller.yml", "name": "rbac-snapshot-control
        ler"}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'name': 'snapshot-controller', 'file': 'snapsho
        t-controller.yml'})  => {"ansible_loop_var": "item", "changed": false, "item": {
        "file": "snapshot-controller.yml", "name": "snapshot-controller"}, "skip_reason"
        : "Conditional result was False"}
        
        TASK [kubernetes-apps/snapshots/snapshot-controller : Snapshot Controller | Appl
        y Manifests] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/snapshots/snapshot-controller/tasks/main.yml:13
        skipping: [kubenode01] => (item=rbac-snapshot-controller.yml)  => {"ansible_loop
        _var": "item", "changed": false, "item": {"ansible_loop_var": "item", "changed":
         false, "item": {"file": "rbac-snapshot-controller.yml", "name": "rbac-snapshot-
        controller"}, "skip_reason": "Conditional result was False", "skipped": true}, "
        skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item=snapshot-controller.yml)  => {"ansible_loop_var"
        : "item", "changed": false, "item": {"ansible_loop_var": "item", "changed": fals
        e, "item": {"file": "snapshot-controller.yml", "name": "snapshot-controller"}, "
        skip_reason": "Conditional result was False", "skipped": true}, "skip_reason": "
        Conditional result was False"}
        
        TASK [kubernetes-apps/snapshots/cinder-csi : Kubernetes Snapshots | Copy Cinder
        CSI Snapshot Class template] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/snapshots/cinder-csi/tasks/main.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/snapshots/cinder-csi : Kubernetes Snapshots | Add Cinder C
        SI Snapshot Class] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/snapshots/cinder-csi/tasks/main.yml:10
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/container_runtimes/kata_containers : Kata Containers | Cre
        ate addon dir] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/container_runtimes/kata_containers/tasks/main.yaml:3
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/container_runtimes/kata_containers : Kata Containers | Tem
        plates list] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/container_runtimes/kata_containers/tasks/main.yaml:11
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/container_runtimes/kata_containers : Kata Containers | Cre
        ate manifests] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/container_runtimes/kata_containers/tasks/main.yaml:16
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/container_runtimes/kata_containers : Kata Containers | App
        ly manifests] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/container_runtimes/kata_containers/tasks/main.yaml:25
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/container_runtimes/crun : crun | Copy runtime class manife
        st] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/container_runtimes/crun/tasks/main.yaml:3
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/container_runtimes/crun : crun | Apply manifests] ********
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/container_runtimes/crun/tasks/main.yaml:11
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/container_engine_accelerator/nvidia_gpu : Container Engine
         Acceleration Nvidia GPU| gather os specific variables] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/container_engine_accelerator/nvidia_gpu/tasks/main.yml:3
        skipping: [kubenode01] => (item=/wire-server-deploy/ansible/roles-external/kubes
        pray/roles/kubernetes-apps/container_engine_accelerator/nvidia_gpu/vars/ubuntu-1
        8.yml)  => {"ansible_loop_var": "item", "changed": false, "item": "/wire-server-
        deploy/ansible/roles-external/kubespray/roles/kubernetes-apps/container_engine_a
        ccelerator/nvidia_gpu/vars/ubuntu-18.yml", "skip_reason": "Conditional result wa
        s False"}
        
        TASK [kubernetes-apps/container_engine_accelerator/nvidia_gpu : Container Engine
         Acceleration Nvidia GPU | Set fact of download url Tesla] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/container_engine_accelerator/nvidia_gpu/tasks/main.yml:14
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/container_engine_accelerator/nvidia_gpu : Container Engine
         Acceleration Nvidia GPU | Set fact of download url GTX] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/container_engine_accelerator/nvidia_gpu/tasks/main.yml:19
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/container_engine_accelerator/nvidia_gpu : Container Engine
         Acceleration Nvidia GPU | Create addon dir] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/container_engine_accelerator/nvidia_gpu/tasks/main.yml:24
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/container_engine_accelerator/nvidia_gpu : Container Engine
         Acceleration Nvidia GPU | Create manifests for nvidia accelerators] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/container_engine_accelerator/nvidia_gpu/tasks/main.yml:32
        skipping: [kubenode01] => (item={'name': 'nvidia-driver-install-daemonset', 'fil
        e': 'nvidia-driver-install-daemonset.yml', 'type': 'daemonset'})  => {"ansible_l
        oop_var": "item", "changed": false, "item": {"file": "nvidia-driver-install-daem
        onset.yml", "name": "nvidia-driver-install-daemonset", "type": "daemonset"}, "sk
        ip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'name': 'k8s-device-plugin-nvidia-daemonset', '
        file': 'k8s-device-plugin-nvidia-daemonset.yml', 'type': 'daemonset'})  => {"ans
        ible_loop_var": "item", "changed": false, "item": {"file": "k8s-device-plugin-nv
        idia-daemonset.yml", "name": "k8s-device-plugin-nvidia-daemonset", "type": "daem
        onset"}, "skip_reason": "Conditional result was False"}
        
        TASK [kubernetes-apps/container_engine_accelerator/nvidia_gpu : Container Engine
         Acceleration Nvidia GPU | Apply manifests for nvidia accelerators] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/container_engine_accelerator/nvidia_gpu/tasks/main.yml:43
        skipping: [kubenode01] => (item={'changed': False, 'skipped': True, 'skip_reason
        ': 'Conditional result was False', 'item': {'name': 'nvidia-driver-install-daemo
        nset', 'file': 'nvidia-driver-install-daemonset.yml', 'type': 'daemonset'}, 'ans
        ible_loop_var': 'item'})  => {"ansible_loop_var": "item", "changed": false, "ite
        m": {"ansible_loop_var": "item", "changed": false, "item": {"file": "nvidia-driv
        er-install-daemonset.yml", "name": "nvidia-driver-install-daemonset", "type": "d
        aemonset"}, "skip_reason": "Conditional result was False", "skipped": true}, "sk
        ip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'changed': False, 'skipped': True, 'skip_reason
        ': 'Conditional result was False', 'item': {'name': 'k8s-device-plugin-nvidia-da
        emonset', 'file': 'k8s-device-plugin-nvidia-daemonset.yml', 'type': 'daemonset'}
        , 'ansible_loop_var': 'item'})  => {"ansible_loop_var": "item", "changed": false
        , "item": {"ansible_loop_var": "item", "changed": false, "item": {"file": "k8s-d
        evice-plugin-nvidia-daemonset.yml", "name": "k8s-device-plugin-nvidia-daemonset"
        , "type": "daemonset"}, "skip_reason": "Conditional result was False", "skipped"
        : true}, "skip_reason": "Conditional result was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Credentials
        Check | oci_private_key] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/cloud_controller/oci/tasks/credentials-check.yml:3
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Credentials
        Check | oci_region_id] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/cloud_controller/oci/tasks/credentials-check.yml:10
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Credentials
        Check | oci_tenancy_id] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/cloud_controller/oci/tasks/credentials-check.yml:17
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Credentials
        Check | oci_user_id] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/cloud_controller/oci/tasks/credentials-check.yml:24
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Credentials
        Check | oci_user_fingerprint] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/cloud_controller/oci/tasks/credentials-check.yml:31
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Credentials
        Check | oci_compartment_id] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/cloud_controller/oci/tasks/credentials-check.yml:38
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Credentials
        Check | oci_vnc_id] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/cloud_controller/oci/tasks/credentials-check.yml:44
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Credentials
        Check | oci_subnet1_id] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/cloud_controller/oci/tasks/credentials-check.yml:50
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Credentials
        Check | oci_subnet2_id] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/cloud_controller/oci/tasks/credentials-check.yml:56
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Credentials
        Check | oci_security_list_management] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/cloud_controller/oci/tasks/credentials-check.yml:63
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Generate Clo
        ud Provider Configuration] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/cloud_controller/oci/tasks/main.yml:6
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Slurp Config
        uration] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/cloud_controller/oci/tasks/main.yml:13
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Encode Confi
        guration] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/cloud_controller/oci/tasks/main.yml:18
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Generate Man
        ifests] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/cloud_controller/oci/tasks/main.yml:24
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Apply Manife
        sts] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/cloud_controller/oci/tasks/main.yml:31
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/metallb : Kubernetes Apps | Check cluster settings for Met
        alLB] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/metallb/tasks/main.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/metallb : Kubernetes Apps | Check cluster settings for Met
        alLB] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/metallb/tasks/main.yml:8
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/metallb : Kubernetes Apps | Check BGP peers for MetalLB] *
        **
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/metallb/tasks/main.yml:14
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/metallb : Kubernetes Apps | Check AppArmor status] *******
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/metallb/tasks/main.yml:20
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/metallb : Kubernetes Apps | Set apparmor_enabled] ********
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/metallb/tasks/main.yml:28
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/metallb : Kubernetes Apps | Lay Down MetalLB] ************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/metallb/tasks/main.yml:35
        skipping: [kubenode01] => (item=metallb.yml)  => {"ansible_loop_var": "item", "c
        hanged": false, "item": "metallb.yml", "skip_reason": "Conditional result was Fa
        lse"}
        skipping: [kubenode01] => (item=metallb-config.yml)  => {"ansible_loop_var": "it
        em", "changed": false, "item": "metallb-config.yml", "skip_reason": "Conditional
         result was False"}
        
        TASK [kubernetes-apps/metallb : Kubernetes Apps | Install and configure MetalLB]
         ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/metallb/tasks/main.yml:43
        skipping: [kubenode01] => (item={'changed': False, 'skipped': True, 'skip_reason
        ': 'Conditional result was False', 'item': 'metallb.yml', 'ansible_loop_var': 'i
        tem'})  => {"ansible_loop_var": "item", "changed": false, "item": {"ansible_loop
        _var": "item", "changed": false, "item": "metallb.yml", "skip_reason": "Conditio
        nal result was False", "skipped": true}, "skip_reason": "Conditional result was
        False"}
        skipping: [kubenode01] => (item={'changed': False, 'skipped': True, 'skip_reason
        ': 'Conditional result was False', 'item': 'metallb-config.yml', 'ansible_loop_v
        ar': 'item'})  => {"ansible_loop_var": "item", "changed": false, "item": {"ansib
        le_loop_var": "item", "changed": false, "item": "metallb-config.yml", "skip_reas
        on": "Conditional result was False", "skipped": true}, "skip_reason": "Condition
        al result was False"}
        
        TASK [kubernetes-apps/metallb : Kubernetes Apps | Check existing secret of Metal
        LB] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/metallb/tasks/main.yml:54
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/metallb : Kubernetes Apps | Create random bytes for MetalL
        B] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/metallb/tasks/main.yml:62
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/metallb : Kubernetes Apps | Install secret of MetalLB if n
        ot existing] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        -apps/metallb/tasks/main.yml:69
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        META: ran handlers
        META: ran handlers
        
        PLAY [k8s-cluster] *************************************************************
        META: ran handlers
        
        TASK [prep_download | Set a few facts] *****************************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/download/t
        asks/prep_download.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [prep_download | Set image info command for containerd and crio] **********
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/download/t
        asks/prep_download.yml:8
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [prep_download | Set image info command for containerd and crio on localhos
        t] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/download/t
        asks/prep_download.yml:14
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [prep_download | On localhost, check if passwordless root is possible] ****
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/download/t
        asks/prep_download.yml:20
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [prep_download | On localhost, check if user has access to docker without u
        sing sudo] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/download/t
        asks/prep_download.yml:35
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [prep_download | Parse the outputs of the previous commands] **************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/download/t
        asks/prep_download.yml:50
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [prep_download | Check that local user is in group or can become root] ****
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/download/t
        asks/prep_download.yml:60
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [prep_download | Register docker images info] *****************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/download/t
        asks/prep_download.yml:71
        skipping: [kubenode01] => {"censored": "the output has been hidden due to the fa
        ct that 'no_log: true' was specified for this result", "changed": false}
        
        TASK [prep_download | Create staging directory on remote node] *****************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/download/t
        asks/prep_download.yml:80
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [prep_download | Create local cache for files and images on control node] *
        **
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/download/t
        asks/prep_download.yml:90
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [container-engine/crictl : install crictĺ] ********************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/container-
        engine/crictl/tasks/main.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [download | Get kubeadm binary and list of required images] ***************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/download/t
        asks/main.yml:17
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [download | Download files / images] **************************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/download/t
        asks/main.yml:26
        skipping: [kubenode01] => (item={'key': 'netcheck_server', 'value': {'enabled':
        False, 'container': True, 'repo': 'quay.io/l23network/k8s-netchecker-server', 't
        ag': 'v1.0', 'sha256': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var":
         "item", "changed": false, "item": {"key": "netcheck_server", "value": {"contain
        er": true, "enabled": false, "groups": ["k8s-cluster"], "repo": "quay.io/l23netw
        ork/k8s-netchecker-server", "sha256": "", "tag": "v1.0"}}, "skip_reason": "Condi
        tional result was False"}
        skipping: [kubenode01] => (item={'key': 'netcheck_agent', 'value': {'enabled': F
        alse, 'container': True, 'repo': 'quay.io/l23network/k8s-netchecker-agent', 'tag
        ': 'v1.0', 'sha256': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "
        item", "changed": false, "item": {"key": "netcheck_agent", "value": {"container"
        : true, "enabled": false, "groups": ["k8s-cluster"], "repo": "quay.io/l23network
        /k8s-netchecker-agent", "sha256": "", "tag": "v1.0"}}, "skip_reason": "Condition
        al result was False"}
        skipping: [kubenode01] => (item={'key': 'etcd', 'value': {'container': True, 'fi
        le': False, 'enabled': True, 'version': 'v3.4.13', 'dest': '/tmp/releases/etcd-v
        3.4.13-linux-amd64.tar.gz', 'repo': 'quay.io/coreos/etcd', 'tag': 'v3.4.13', 'sh
        a256': '', 'url': 'https://github.com/coreos/etcd/releases/download/v3.4.13/etcd
        -v3.4.13-linux-amd64.tar.gz', 'unarchive': False, 'owner': 'root', 'mode': '0755
        ', 'groups': ['etcd']}})  => {"ansible_loop_var": "item", "changed": false, "ite
        m": {"key": "etcd", "value": {"container": true, "dest": "/tmp/releases/etcd-v3.
        4.13-linux-amd64.tar.gz", "enabled": true, "file": false, "groups": ["etcd"], "m
        ode": "0755", "owner": "root", "repo": "quay.io/coreos/etcd", "sha256": "", "tag
        ": "v3.4.13", "unarchive": false, "url": "https://github.com/coreos/etcd/release
        s/download/v3.4.13/etcd-v3.4.13-linux-amd64.tar.gz", "version": "v3.4.13"}}, "sk
        ip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'cni', 'value': {'enabled': True, 'file'
        : True, 'version': 'v0.9.0', 'dest': '/tmp/releases/cni-plugins-linux-amd64-v0.9
        .0.tgz', 'sha256': '58a58d389895ba9f9bbd3ef330f186c0bb7484136d0bfb9b50152eed55d9
        ec24', 'url': 'https://github.com/containernetworking/plugins/releases/download/
        v0.9.0/cni-plugins-linux-amd64-v0.9.0.tgz', 'unarchive': False, 'owner': 'root',
         'mode': '0755', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "
        changed": false, "item": {"key": "cni", "value": {"dest": "/tmp/releases/cni-plu
        gins-linux-amd64-v0.9.0.tgz", "enabled": true, "file": true, "groups": ["k8s-clu
        ster"], "mode": "0755", "owner": "root", "sha256": "58a58d389895ba9f9bbd3ef330f1
        86c0bb7484136d0bfb9b50152eed55d9ec24", "unarchive": false, "url": "https://githu
        b.com/containernetworking/plugins/releases/download/v0.9.0/cni-plugins-linux-amd
        64-v0.9.0.tgz", "version": "v0.9.0"}}, "skip_reason": "Conditional result was Fa
        lse"}
        skipping: [kubenode01] => (item={'key': 'kubeadm', 'value': {'enabled': True, 'f
        ile': True, 'version': 'v1.19.7', 'dest': '/tmp/releases/kubeadm-v1.19.7-amd64',
         'sha256': 'c63ef1842533cd7888c7452cab9f320dcf45fc1c173e9d40abb712d45992db24', '
        url': 'https://storage.googleapis.com/kubernetes-release/release/v1.19.7/bin/lin
        ux/amd64/kubeadm', 'unarchive': False, 'owner': 'root', 'mode': '0755', 'groups'
        : ['k8s-cluster']}})  => {"ansible_loop_var": "item", "changed": false, "item":
        {"key": "kubeadm", "value": {"dest": "/tmp/releases/kubeadm-v1.19.7-amd64", "ena
        bled": true, "file": true, "groups": ["k8s-cluster"], "mode": "0755", "owner": "
        root", "sha256": "c63ef1842533cd7888c7452cab9f320dcf45fc1c173e9d40abb712d45992db
        24", "unarchive": false, "url": "https://storage.googleapis.com/kubernetes-relea
        se/release/v1.19.7/bin/linux/amd64/kubeadm", "version": "v1.19.7"}}, "skip_reaso
        n": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'kubelet', 'value': {'enabled': True, 'f
        ile': True, 'version': 'v1.19.7', 'dest': '/tmp/releases/kubelet-v1.19.7-amd64',
         'sha256': 'd8b296825f6dd7a17287b73cd6604d32210abbba86c88fb68c1b1c5016906c54', '
        url': 'https://storage.googleapis.com/kubernetes-release/release/v1.19.7/bin/lin
        ux/amd64/kubelet', 'unarchive': False, 'owner': 'root', 'mode': '0755', 'groups'
        : ['k8s-cluster']}})  => {"ansible_loop_var": "item", "changed": false, "item":
        {"key": "kubelet", "value": {"dest": "/tmp/releases/kubelet-v1.19.7-amd64", "ena
        bled": true, "file": true, "groups": ["k8s-cluster"], "mode": "0755", "owner": "
        root", "sha256": "d8b296825f6dd7a17287b73cd6604d32210abbba86c88fb68c1b1c5016906c
        54", "unarchive": false, "url": "https://storage.googleapis.com/kubernetes-relea
        se/release/v1.19.7/bin/linux/amd64/kubelet", "version": "v1.19.7"}}, "skip_reaso
        n": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'kubectl', 'value': {'enabled': True, 'f
        ile': True, 'version': 'v1.19.7', 'dest': '/tmp/releases/kubectl-v1.19.7-amd64',
         'sha256': 'd46eb3bbe2575e5b6bedbc6d3519424b4f2f57929d7da1ef7e11c09068f37297', '
        url': 'https://storage.googleapis.com/kubernetes-release/release/v1.19.7/bin/lin
        ux/amd64/kubectl', 'unarchive': False, 'owner': 'root', 'mode': '0755', 'groups'
        : ['kube-master']}})  => {"ansible_loop_var": "item", "changed": false, "item":
        {"key": "kubectl", "value": {"dest": "/tmp/releases/kubectl-v1.19.7-amd64", "ena
        bled": true, "file": true, "groups": ["kube-master"], "mode": "0755", "owner": "
        root", "sha256": "d46eb3bbe2575e5b6bedbc6d3519424b4f2f57929d7da1ef7e11c09068f372
        97", "unarchive": false, "url": "https://storage.googleapis.com/kubernetes-relea
        se/release/v1.19.7/bin/linux/amd64/kubectl", "version": "v1.19.7"}}, "skip_reaso
        n": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'crictl', 'value': {'file': True, 'enabl
        ed': False, 'version': 'v1.19.0', 'dest': '/tmp/releases/crictl-v1.19.0-linux-am
        d64.tar.gz', 'sha256': '87d8ef70b61f2fe3d8b4a48f6f712fd798c6e293ed3723c1e4bbb505
        2098f0ae', 'url': 'https://github.com/kubernetes-sigs/cri-tools/releases/downloa
        d/v1.19.0/crictl-v1.19.0-linux-amd64.tar.gz', 'unarchive': True, 'owner': 'root'
        , 'mode': '0755', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item",
        "changed": false, "item": {"key": "crictl", "value": {"dest": "/tmp/releases/cri
        ctl-v1.19.0-linux-amd64.tar.gz", "enabled": false, "file": true, "groups": ["k8s
        -cluster"], "mode": "0755", "owner": "root", "sha256": "87d8ef70b61f2fe3d8b4a48f
        6f712fd798c6e293ed3723c1e4bbb5052098f0ae", "unarchive": true, "url": "https://gi
        thub.com/kubernetes-sigs/cri-tools/releases/download/v1.19.0/crictl-v1.19.0-linu
        x-amd64.tar.gz", "version": "v1.19.0"}}, "skip_reason": "Conditional result was
        False"}
        skipping: [kubenode01] => (item={'key': 'cilium', 'value': {'enabled': False, 'c
        ontainer': True, 'repo': 'quay.io/cilium/cilium', 'tag': 'v1.8.6', 'sha256': '',
         'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "changed": false,
         "item": {"key": "cilium", "value": {"container": true, "enabled": false, "group
        s": ["k8s-cluster"], "repo": "quay.io/cilium/cilium", "sha256": "", "tag": "v1.8
        .6"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'cilium_init', 'value': {'enabled': Fals
        e, 'container': True, 'repo': 'quay.io/cilium/cilium-init', 'tag': '2019-04-05',
         'sha256': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "ch
        anged": false, "item": {"key": "cilium_init", "value": {"container": true, "enab
        led": false, "groups": ["k8s-cluster"], "repo": "quay.io/cilium/cilium-init", "s
        ha256": "", "tag": "2019-04-05"}}, "skip_reason": "Conditional result was False"
        }
        skipping: [kubenode01] => (item={'key': 'cilium_operator', 'value': {'enabled':
        False, 'container': True, 'repo': 'quay.io/cilium/operator', 'tag': 'v1.8.6', 's
        ha256': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "chang
        ed": false, "item": {"key": "cilium_operator", "value": {"container": true, "ena
        bled": false, "groups": ["k8s-cluster"], "repo": "quay.io/cilium/operator", "sha
        256": "", "tag": "v1.8.6"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'multus', 'value': {'enabled': False, 'c
        ontainer': True, 'repo': 'docker.io/nfvpe/multus', 'tag': 'v3.6', 'sha256': '',
        'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "changed": false,
        "item": {"key": "multus", "value": {"container": true, "enabled": false, "groups
        ": ["k8s-cluster"], "repo": "docker.io/nfvpe/multus", "sha256": "", "tag": "v3.6
        "}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'flannel', 'value': {'enabled': True, 'c
        ontainer': True, 'repo': 'quay.io/coreos/flannel', 'tag': 'v0.13.0', 'sha256': '
        ', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "changed": fals
        e, "item": {"key": "flannel", "value": {"container": true, "enabled": true, "gro
        ups": ["k8s-cluster"], "repo": "quay.io/coreos/flannel", "sha256": "", "tag": "v
        0.13.0"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'calicoctl', 'value': {'enabled': False,
         'file': True, 'version': 'v3.16.6', 'dest': '/tmp/releases/calicoctl', 'sha256'
        : '9b82230446d4749a1043dddd8d466d275a460e570a412e6ced003368ab9c72d8', 'url': 'ht
        tps://github.com/projectcalico/calicoctl/releases/download/v3.16.6/calicoctl-lin
        ux-amd64', 'unarchive': False, 'owner': 'root', 'mode': '0755', 'groups': ['k8s-
        cluster']}})  => {"ansible_loop_var": "item", "changed": false, "item": {"key":
        "calicoctl", "value": {"dest": "/tmp/releases/calicoctl", "enabled": false, "fil
        e": true, "groups": ["k8s-cluster"], "mode": "0755", "owner": "root", "sha256":
        "9b82230446d4749a1043dddd8d466d275a460e570a412e6ced003368ab9c72d8", "unarchive":
         false, "url": "https://github.com/projectcalico/calicoctl/releases/download/v3.
        16.6/calicoctl-linux-amd64", "version": "v3.16.6"}}, "skip_reason": "Conditional
         result was False"}
        skipping: [kubenode01] => (item={'key': 'calico_node', 'value': {'enabled': Fals
        e, 'container': True, 'repo': 'quay.io/calico/node', 'tag': 'v3.16.6', 'sha256':
         '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "changed": fa
        lse, "item": {"key": "calico_node", "value": {"container": true, "enabled": fals
        e, "groups": ["k8s-cluster"], "repo": "quay.io/calico/node", "sha256": "", "tag"
        : "v3.16.6"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'calico_cni', 'value': {'enabled': False
        , 'container': True, 'repo': 'quay.io/calico/cni', 'tag': 'v3.16.6', 'sha256': '
        ', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "changed": fals
        e, "item": {"key": "calico_cni", "value": {"container": true, "enabled": false,
        "groups": ["k8s-cluster"], "repo": "quay.io/calico/cni", "sha256": "", "tag": "v
        3.16.6"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'calico_policy', 'value': {'enabled': Fa
        lse, 'container': True, 'repo': 'quay.io/calico/kube-controllers', 'tag': 'v3.16
        .6', 'sha256': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item",
         "changed": false, "item": {"key": "calico_policy", "value": {"container": true,
         "enabled": false, "groups": ["k8s-cluster"], "repo": "quay.io/calico/kube-contr
        ollers", "sha256": "", "tag": "v3.16.6"}}, "skip_reason": "Conditional result wa
        s False"}
        skipping: [kubenode01] => (item={'key': 'calico_typha', 'value': {'enabled': Fal
        se, 'container': True, 'repo': 'quay.io/calico/typha', 'tag': 'v3.16.6', 'sha256
        ': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "changed":
        false, "item": {"key": "calico_typha", "value": {"container": true, "enabled": f
        alse, "groups": ["k8s-cluster"], "repo": "quay.io/calico/typha", "sha256": "", "
        tag": "v3.16.6"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'weave_kube', 'value': {'enabled': False
        , 'container': True, 'repo': 'docker.io/weaveworks/weave-kube', 'tag': '2.7.0',
        'sha256': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "cha
        nged": false, "item": {"key": "weave_kube", "value": {"container": true, "enable
        d": false, "groups": ["k8s-cluster"], "repo": "docker.io/weaveworks/weave-kube",
         "sha256": "", "tag": "2.7.0"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'weave_npc', 'value': {'enabled': False,
         'container': True, 'repo': 'docker.io/weaveworks/weave-npc', 'tag': '2.7.0', 's
        ha256': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "chang
        ed": false, "item": {"key": "weave_npc", "value": {"container": true, "enabled":
         false, "groups": ["k8s-cluster"], "repo": "docker.io/weaveworks/weave-npc", "sh
        a256": "", "tag": "2.7.0"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'ovn4nfv', 'value': {'enabled': False, '
        container': True, 'repo': 'docker.io/integratedcloudnative/ovn4nfv-k8s-plugin',
        'tag': 'v1.1.0', 'sha256': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_v
        ar": "item", "changed": false, "item": {"key": "ovn4nfv", "value": {"container":
         true, "enabled": false, "groups": ["k8s-cluster"], "repo": "docker.io/integrate
        dcloudnative/ovn4nfv-k8s-plugin", "sha256": "", "tag": "v1.1.0"}}, "skip_reason"
        : "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'kube_ovn', 'value': {'enabled': False,
        'container': True, 'repo': 'docker.io/kubeovn/kube-ovn', 'tag': 'v1.5.2', 'sha25
        6': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "changed":
         false, "item": {"key": "kube_ovn", "value": {"container": true, "enabled": fals
        e, "groups": ["k8s-cluster"], "repo": "docker.io/kubeovn/kube-ovn", "sha256": ""
        , "tag": "v1.5.2"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'kube_router', 'value': {'enabled': Fals
        e, 'container': True, 'repo': 'docker.io/cloudnativelabs/kube-router', 'tag': 'v
        1.1.1', 'sha256': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "ite
        m", "changed": false, "item": {"key": "kube_router", "value": {"container": true
        , "enabled": false, "groups": ["k8s-cluster"], "repo": "docker.io/cloudnativelab
        s/kube-router", "sha256": "", "tag": "v1.1.1"}}, "skip_reason": "Conditional res
        ult was False"}
        skipping: [kubenode01] => (item={'key': 'pod_infra', 'value': {'enabled': True,
        'container': True, 'repo': 'k8s.gcr.io/pause', 'tag': '3.3', 'sha256': '', 'grou
        ps': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "changed": false, "item
        ": {"key": "pod_infra", "value": {"container": true, "enabled": true, "groups":
        ["k8s-cluster"], "repo": "k8s.gcr.io/pause", "sha256": "", "tag": "3.3"}}, "skip
        _reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'install_socat', 'value': {'enabled': Fa
        lse, 'container': True, 'repo': 'docker.io/xueshanf/install-socat', 'tag': 'late
        st', 'sha256': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item",
         "changed": false, "item": {"key": "install_socat", "value": {"container": true,
         "enabled": false, "groups": ["k8s-cluster"], "repo": "docker.io/xueshanf/instal
        l-socat", "sha256": "", "tag": "latest"}}, "skip_reason": "Conditional result wa
        s False"}
        skipping: [kubenode01] => (item={'key': 'nginx', 'value': {'enabled': True, 'con
        tainer': True, 'repo': 'docker.io/library/nginx', 'tag': 1.19, 'sha256': '', 'gr
        oups': ['kube-node']}})  => {"ansible_loop_var": "item", "changed": false, "item
        ": {"key": "nginx", "value": {"container": true, "enabled": true, "groups": ["ku
        be-node"], "repo": "docker.io/library/nginx", "sha256": "", "tag": 1.19}}, "skip
        _reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'haproxy', 'value': {'enabled': False, '
        container': True, 'repo': 'docker.io/library/haproxy', 'tag': 2.3, 'sha256': '',
         'groups': ['kube-node']}})  => {"ansible_loop_var": "item", "changed": false, "
        item": {"key": "haproxy", "value": {"container": true, "enabled": false, "groups
        ": ["kube-node"], "repo": "docker.io/library/haproxy", "sha256": "", "tag": 2.3}
        }, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'coredns', 'value': {'enabled': True, 'c
        ontainer': True, 'repo': 'k8s.gcr.io/coredns', 'tag': '1.7.0', 'sha256': '', 'gr
        oups': ['kube-master']}})  => {"ansible_loop_var": "item", "changed": false, "it
        em": {"key": "coredns", "value": {"container": true, "enabled": true, "groups":
        ["kube-master"], "repo": "k8s.gcr.io/coredns", "sha256": "", "tag": "1.7.0"}}, "
        skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'nodelocaldns', 'value': {'enabled': Tru
        e, 'container': True, 'repo': 'k8s.gcr.io/dns/k8s-dns-node-cache', 'tag': '1.16.
        0', 'sha256': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item",
        "changed": false, "item": {"key": "nodelocaldns", "value": {"container": true, "
        enabled": true, "groups": ["k8s-cluster"], "repo": "k8s.gcr.io/dns/k8s-dns-node-
        cache", "sha256": "", "tag": "1.16.0"}}, "skip_reason": "Conditional result was
        False"}
        skipping: [kubenode01] => (item={'key': 'dnsautoscaler', 'value': {'enabled': Tr
        ue, 'container': True, 'repo': 'k8s.gcr.io/cpa/cluster-proportional-autoscaler-a
        md64', 'tag': '1.8.3', 'sha256': '', 'groups': ['kube-master']}})  => {"ansible_
        loop_var": "item", "changed": false, "item": {"key": "dnsautoscaler", "value": {
        "container": true, "enabled": true, "groups": ["kube-master"], "repo": "k8s.gcr.
        io/cpa/cluster-proportional-autoscaler-amd64", "sha256": "", "tag": "1.8.3"}}, "
        skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'testbox', 'value': {'enabled': False, '
        container': True, 'repo': 'k8s.gcr.io/busybox', 'tag': 'latest', 'sha256': ''}})
          => {"ansible_loop_var": "item", "changed": false, "item": {"key": "testbox", "
        value": {"container": true, "enabled": false, "repo": "k8s.gcr.io/busybox", "sha
        256": "", "tag": "latest"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'helm', 'value': {'enabled': False, 'fil
        e': True, 'version': 'v3.5.2', 'dest': '/tmp/releases/helm-v3.5.2/helm-v3.5.2-li
        nux-amd64.tar.gz', 'sha256': '01b317c506f8b6ad60b11b1dc3f093276bb703281cb1ae0113
        2752253ec706a2', 'url': 'https://get.helm.sh/helm-v3.5.2-linux-amd64.tar.gz', 'u
        narchive': True, 'owner': 'root', 'mode': '0755', 'groups': ['kube-master']}})
        => {"ansible_loop_var": "item", "changed": false, "item": {"key": "helm", "value
        ": {"dest": "/tmp/releases/helm-v3.5.2/helm-v3.5.2-linux-amd64.tar.gz", "enabled
        ": false, "file": true, "groups": ["kube-master"], "mode": "0755", "owner": "roo
        t", "sha256": "01b317c506f8b6ad60b11b1dc3f093276bb703281cb1ae01132752253ec706a2"
        , "unarchive": true, "url": "https://get.helm.sh/helm-v3.5.2-linux-amd64.tar.gz"
        , "version": "v3.5.2"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'registry', 'value': {'enabled': False,
        'container': True, 'repo': 'docker.io/library/registry', 'tag': '2.7.1', 'sha256
        ': '', 'groups': ['kube-node']}})  => {"ansible_loop_var": "item", "changed": fa
        lse, "item": {"key": "registry", "value": {"container": true, "enabled": false,
        "groups": ["kube-node"], "repo": "docker.io/library/registry", "sha256": "", "ta
        g": "2.7.1"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'registry_proxy', 'value': {'enabled': F
        alse, 'container': True, 'repo': 'k8s.gcr.io/kube-registry-proxy', 'tag': '0.4',
         'sha256': '', 'groups': ['kube-node']}})  => {"ansible_loop_var": "item", "chan
        ged": false, "item": {"key": "registry_proxy", "value": {"container": true, "ena
        bled": false, "groups": ["kube-node"], "repo": "k8s.gcr.io/kube-registry-proxy",
         "sha256": "", "tag": "0.4"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'metrics_server', 'value': {'enabled': F
        alse, 'container': True, 'repo': 'k8s.gcr.io/metrics-server/metrics-server', 'ta
        g': 'v0.3.7', 'sha256': '', 'groups': ['kube-master']}})  => {"ansible_loop_var"
        : "item", "changed": false, "item": {"key": "metrics_server", "value": {"contain
        er": true, "enabled": false, "groups": ["kube-master"], "repo": "k8s.gcr.io/metr
        ics-server/metrics-server", "sha256": "", "tag": "v0.3.7"}}, "skip_reason": "Con
        ditional result was False"}
        skipping: [kubenode01] => (item={'key': 'addon_resizer', 'value': {'enabled': Fa
        lse, 'container': True, 'repo': 'k8s.gcr.io/addon-resizer', 'tag': '1.8.11', 'sh
        a256': '', 'groups': ['kube-master']}})  => {"ansible_loop_var": "item", "change
        d": false, "item": {"key": "addon_resizer", "value": {"container": true, "enable
        d": false, "groups": ["kube-master"], "repo": "k8s.gcr.io/addon-resizer", "sha25
        6": "", "tag": "1.8.11"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'local_volume_provisioner', 'value': {'e
        nabled': False, 'container': True, 'repo': 'quay.io/external_storage/local-volum
        e-provisioner', 'tag': 'v2.3.4', 'sha256': '', 'groups': ['kube-node']}})  => {"
        ansible_loop_var": "item", "changed": false, "item": {"key": "local_volume_provi
        sioner", "value": {"container": true, "enabled": false, "groups": ["kube-node"],
         "repo": "quay.io/external_storage/local-volume-provisioner", "sha256": "", "tag
        ": "v2.3.4"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'cephfs_provisioner', 'value': {'enabled
        ': False, 'container': True, 'repo': 'quay.io/external_storage/cephfs-provisione
        r', 'tag': 'v2.1.0-k8s1.11', 'sha256': '', 'groups': ['kube-node']}})  => {"ansi
        ble_loop_var": "item", "changed": false, "item": {"key": "cephfs_provisioner", "
        value": {"container": true, "enabled": false, "groups": ["kube-node"], "repo": "
        quay.io/external_storage/cephfs-provisioner", "sha256": "", "tag": "v2.1.0-k8s1.
        11"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'rbd_provisioner', 'value': {'enabled':
        False, 'container': True, 'repo': 'quay.io/external_storage/rbd-provisioner', 't
        ag': 'v2.1.1-k8s1.11', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_lo
        op_var": "item", "changed": false, "item": {"key": "rbd_provisioner", "value": {
        "container": true, "enabled": false, "groups": ["kube-node"], "repo": "quay.io/e
        xternal_storage/rbd-provisioner", "sha256": "", "tag": "v2.1.1-k8s1.11"}}, "skip
        _reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'local_path_provisioner', 'value': {'ena
        bled': False, 'container': True, 'repo': 'docker.io/rancher/local-path-provision
        er', 'tag': 'v0.0.19', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_lo
        op_var": "item", "changed": false, "item": {"key": "local_path_provisioner", "va
        lue": {"container": true, "enabled": false, "groups": ["kube-node"], "repo": "do
        cker.io/rancher/local-path-provisioner", "sha256": "", "tag": "v0.0.19"}}, "skip
        _reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'ingress_nginx_controller', 'value': {'e
        nabled': False, 'container': True, 'repo': 'k8s.gcr.io/ingress-nginx/controller'
        , 'tag': 'v0.41.2', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_loop_
        var": "item", "changed": false, "item": {"key": "ingress_nginx_controller", "val
        ue": {"container": true, "enabled": false, "groups": ["kube-node"], "repo": "k8s
        .gcr.io/ingress-nginx/controller", "sha256": "", "tag": "v0.41.2"}}, "skip_reaso
        n": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'ingress_ambassador_controller', 'value'
        : {'enabled': False, 'container': True, 'repo': 'quay.io/datawire/ambassador-ope
        rator', 'tag': 'v1.2.9', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_
        loop_var": "item", "changed": false, "item": {"key": "ingress_ambassador_control
        ler", "value": {"container": true, "enabled": false, "groups": ["kube-node"], "r
        epo": "quay.io/datawire/ambassador-operator", "sha256": "", "tag": "v1.2.9"}}, "
        skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'ingress_alb_controller', 'value': {'ena
        bled': False, 'container': True, 'repo': 'docker.io/amazon/aws-alb-ingress-contr
        oller', 'tag': 'v1.1.9', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_
        loop_var": "item", "changed": false, "item": {"key": "ingress_alb_controller", "
        value": {"container": true, "enabled": false, "groups": ["kube-node"], "repo": "
        docker.io/amazon/aws-alb-ingress-controller", "sha256": "", "tag": "v1.1.9"}}, "
        skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'cert_manager_controller', 'value': {'en
        abled': False, 'container': True, 'repo': 'quay.io/jetstack/cert-manager-control
        ler', 'tag': 'v1.0.4', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_lo
        op_var": "item", "changed": false, "item": {"key": "cert_manager_controller", "v
        alue": {"container": true, "enabled": false, "groups": ["kube-node"], "repo": "q
        uay.io/jetstack/cert-manager-controller", "sha256": "", "tag": "v1.0.4"}}, "skip
        _reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'cert_manager_cainjector', 'value': {'en
        abled': False, 'container': True, 'repo': 'quay.io/jetstack/cert-manager-cainjec
        tor', 'tag': 'v1.0.4', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_lo
        op_var": "item", "changed": false, "item": {"key": "cert_manager_cainjector", "v
        alue": {"container": true, "enabled": false, "groups": ["kube-node"], "repo": "q
        uay.io/jetstack/cert-manager-cainjector", "sha256": "", "tag": "v1.0.4"}}, "skip
        _reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'cert_manager_webhook', 'value': {'enabl
        ed': False, 'container': True, 'repo': 'quay.io/jetstack/cert-manager-webhook',
        'tag': 'v1.0.4', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_loop_var
        ": "item", "changed": false, "item": {"key": "cert_manager_webhook", "value": {"
        container": true, "enabled": false, "groups": ["kube-node"], "repo": "quay.io/je
        tstack/cert-manager-webhook", "sha256": "", "tag": "v1.0.4"}}, "skip_reason": "C
        onditional result was False"}
        skipping: [kubenode01] => (item={'key': 'csi_attacher', 'value': {'enabled': Fal
        se, 'container': True, 'repo': 'quay.io/k8scsi/csi-attacher', 'tag': 'v2.2.0', '
        sha256': '', 'groups': ['kube-node']}})  => {"ansible_loop_var": "item", "change
        d": false, "item": {"key": "csi_attacher", "value": {"container": true, "enabled
        ": false, "groups": ["kube-node"], "repo": "quay.io/k8scsi/csi-attacher", "sha25
        6": "", "tag": "v2.2.0"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'csi_provisioner', 'value': {'enabled':
        False, 'container': True, 'repo': 'quay.io/k8scsi/csi-provisioner', 'tag': 'v1.6
        .0', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_loop_var": "item", "
        changed": false, "item": {"key": "csi_provisioner", "value": {"container": true,
         "enabled": false, "groups": ["kube-node"], "repo": "quay.io/k8scsi/csi-provisio
        ner", "sha256": "", "tag": "v1.6.0"}}, "skip_reason": "Conditional result was Fa
        lse"}
        skipping: [kubenode01] => (item={'key': 'csi_snapshotter', 'value': {'enabled':
        False, 'container': True, 'repo': 'quay.io/k8scsi/csi-snapshotter', 'tag': 'v2.1
        .1', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_loop_var": "item", "
        changed": false, "item": {"key": "csi_snapshotter", "value": {"container": true,
         "enabled": false, "groups": ["kube-node"], "repo": "quay.io/k8scsi/csi-snapshot
        ter", "sha256": "", "tag": "v2.1.1"}}, "skip_reason": "Conditional result was Fa
        lse"}
        skipping: [kubenode01] => (item={'key': 'snapshot_controller', 'value': {'enable
        d': False, 'container': True, 'repo': 'quay.io/k8scsi/snapshot-controller', 'tag
        ': 'v2.0.1', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_loop_var": "
        item", "changed": false, "item": {"key": "snapshot_controller", "value": {"conta
        iner": true, "enabled": false, "groups": ["kube-node"], "repo": "quay.io/k8scsi/
        snapshot-controller", "sha256": "", "tag": "v2.0.1"}}, "skip_reason": "Condition
        al result was False"}
        skipping: [kubenode01] => (item={'key': 'csi_resizer', 'value': {'enabled': Fals
        e, 'container': True, 'repo': 'quay.io/k8scsi/csi-resizer', 'tag': 'v0.5.0', 'sh
        a256': '', 'groups': ['kube-node']}})  => {"ansible_loop_var": "item", "changed"
        : false, "item": {"key": "csi_resizer", "value": {"container": true, "enabled":
        false, "groups": ["kube-node"], "repo": "quay.io/k8scsi/csi-resizer", "sha256":
        "", "tag": "v0.5.0"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'csi_node_driver_registrar', 'value': {'
        enabled': False, 'container': True, 'repo': 'quay.io/k8scsi/csi-node-driver-regi
        strar', 'tag': 'v1.3.0', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_
        loop_var": "item", "changed": false, "item": {"key": "csi_node_driver_registrar"
        , "value": {"container": true, "enabled": false, "groups": ["kube-node"], "repo"
        : "quay.io/k8scsi/csi-node-driver-registrar", "sha256": "", "tag": "v1.3.0"}}, "
        skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'cinder_csi_plugin', 'value': {'enabled'
        : False, 'container': True, 'repo': 'docker.io/k8scloudprovider/cinder-csi-plugi
        n', 'tag': 'v1.18.0', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_loo
        p_var": "item", "changed": false, "item": {"key": "cinder_csi_plugin", "value":
        {"container": true, "enabled": false, "groups": ["kube-node"], "repo": "docker.i
        o/k8scloudprovider/cinder-csi-plugin", "sha256": "", "tag": "v1.18.0"}}, "skip_r
        eason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'aws_ebs_csi_plugin', 'value': {'enabled
        ': False, 'container': True, 'repo': 'docker.io/amazon/aws-ebs-csi-driver', 'tag
        ': 'v0.5.0', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_loop_var": "
        item", "changed": false, "item": {"key": "aws_ebs_csi_plugin", "value": {"contai
        ner": true, "enabled": false, "groups": ["kube-node"], "repo": "docker.io/amazon
        /aws-ebs-csi-driver", "sha256": "", "tag": "v0.5.0"}}, "skip_reason": "Condition
        al result was False"}
        skipping: [kubenode01] => (item={'key': 'dashboard', 'value': {'enabled': False,
         'container': True, 'repo': 'docker.io/kubernetesui/dashboard-amd64', 'tag': 'v2
        .1.0', 'sha256': '', 'groups': ['kube-master']}})  => {"ansible_loop_var": "item
        ", "changed": false, "item": {"key": "dashboard", "value": {"container": true, "
        enabled": false, "groups": ["kube-master"], "repo": "docker.io/kubernetesui/dash
        board-amd64", "sha256": "", "tag": "v2.1.0"}}, "skip_reason": "Conditional resul
        t was False"}
        skipping: [kubenode01] => (item={'key': 'dashboard_metrics_scrapper', 'value': {
        'enabled': False, 'container': True, 'repo': 'docker.io/kubernetesui/metrics-scr
        aper', 'tag': 'v1.0.6', 'sha256': '', 'groups': ['kube-master']}})  => {"ansible
        _loop_var": "item", "changed": false, "item": {"key": "dashboard_metrics_scrappe
        r", "value": {"container": true, "enabled": false, "groups": ["kube-master"], "r
        epo": "docker.io/kubernetesui/metrics-scraper", "sha256": "", "tag": "v1.0.6"}},
         "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'kubeadm_kube-apiserver', 'value': {'ena
        bled': True, 'container': True, 'repo': 'k8s.gcr.io/kube-apiserver', 'tag': 'v1.
        19.7', 'groups': 'k8s-cluster'}})  => {"ansible_loop_var": "item", "changed": fa
        lse, "item": {"key": "kubeadm_kube-apiserver", "value": {"container": true, "ena
        bled": true, "groups": "k8s-cluster", "repo": "k8s.gcr.io/kube-apiserver", "tag"
        : "v1.19.7"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'kubeadm_kube-controller-manager', 'valu
        e': {'enabled': True, 'container': True, 'repo': 'k8s.gcr.io/kube-controller-man
        ager', 'tag': 'v1.19.7', 'groups': 'k8s-cluster'}})  => {"ansible_loop_var": "it
        em", "changed": false, "item": {"key": "kubeadm_kube-controller-manager", "value
        ": {"container": true, "enabled": true, "groups": "k8s-cluster", "repo": "k8s.gc
        r.io/kube-controller-manager", "tag": "v1.19.7"}}, "skip_reason": "Conditional r
        esult was False"}
        skipping: [kubenode01] => (item={'key': 'kubeadm_kube-scheduler', 'value': {'ena
        bled': True, 'container': True, 'repo': 'k8s.gcr.io/kube-scheduler', 'tag': 'v1.
        19.7', 'groups': 'k8s-cluster'}})  => {"ansible_loop_var": "item", "changed": fa
        lse, "item": {"key": "kubeadm_kube-scheduler", "value": {"container": true, "ena
        bled": true, "groups": "k8s-cluster", "repo": "k8s.gcr.io/kube-scheduler", "tag"
        : "v1.19.7"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'kubeadm_kube-proxy', 'value': {'enabled
        ': True, 'container': True, 'repo': 'k8s.gcr.io/kube-proxy', 'tag': 'v1.19.7', '
        groups': 'k8s-cluster'}})  => {"ansible_loop_var": "item", "changed": false, "it
        em": {"key": "kubeadm_kube-proxy", "value": {"container": true, "enabled": true,
         "groups": "k8s-cluster", "repo": "k8s.gcr.io/kube-proxy", "tag": "v1.19.7"}}, "
        skip_reason": "Conditional result was False"}
        
        TASK [kubespray-defaults : Configure defaults] *********************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubespray-
        defaults/tasks/main.yaml:2
        ok: [kubenode01] => {
            "msg": "Check roles/kubespray-defaults/defaults/main.yml"
        }
        
        TASK [kubespray-defaults : Gather ansible_default_ipv4 from all hosts] *********
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubespray-
        defaults/tasks/fallback_ips.yml:6
        skipping: [kubenode01] => (item=kubenode01)  => {"ansible_loop_var": "delegate_h
        ost_to_gather_facts", "changed": false, "delegate_host_to_gather_facts": "kubeno
        de01", "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item=kubenode01)  => {"ansible_loop_var": "delegate_h
        ost_to_gather_facts", "changed": false, "delegate_host_to_gather_facts": "kubeno
        de01", "skip_reason": "Conditional result was False"}
        
        TASK [kubespray-defaults : create fallback_ips_base] ***************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubespray-
        defaults/tasks/fallback_ips.yml:15
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubespray-defaults : set fallback_ips] ***********************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubespray-
        defaults/tasks/fallback_ips.yml:29
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubespray-defaults : Set no_proxy to all assigned cluster IPs and hostname
        s] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubespray-
        defaults/tasks/no_proxy.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubespray-defaults : Populates no_proxy to all hosts] ********************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubespray-
        defaults/tasks/no_proxy.yml:32
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [adduser : User | Create User Group] **************************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/adduser/ta
        sks/main.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [adduser : User | Create User] ********************************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/adduser/ta
        sks/main.yml:7
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Remove swapfile from /etc/fstab] *****************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0010-swapoff.yml:2
        skipping: [kubenode01] => (item=swap)  => {"ansible_loop_var": "item", "changed"
        : false, "item": "swap", "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item=none)  => {"ansible_loop_var": "item", "changed"
        : false, "item": "none", "skip_reason": "Conditional result was False"}
        
        TASK [kubernetes/preinstall : check swap] **************************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0010-swapoff.yml:12
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Disable swap] ************************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0010-swapoff.yml:16
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if either kube-master or kube-node group is e
        mpty] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:2
        skipping: [kubenode01] => (item=kube-master)  => {"ansible_loop_var": "item", "c
        hanged": false, "item": "kube-master", "skip_reason": "Conditional result was Fa
        lse"}
        skipping: [kubenode01] => (item=kube-node)  => {"ansible_loop_var": "item", "cha
        nged": false, "item": "kube-node", "skip_reason": "Conditional result was False"
        }
        
        TASK [kubernetes/preinstall : Stop if etcd group is empty in external etcd mode]
         ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:11
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if non systemd OS type] *********************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:20
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if unknown OS] ******************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:25
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if unknown network plugin] ******************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:31
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if incompatible network plugin and cloudprovi
        der] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:39
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if unsupported version of Kubernetes] *******
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:47
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if known booleans are set as strings (Use JSO
        N format on CLI: -e "{'key': true }")] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:54
        skipping: [kubenode01] => (item={'name': 'download_run_once', 'value': False})
        => {"ansible_loop_var": "item", "changed": false, "item": {"name": "download_run
        _once", "value": false}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'name': 'deploy_netchecker', 'value': False})
        => {"ansible_loop_var": "item", "changed": false, "item": {"name": "deploy_netch
        ecker", "value": false}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'name': 'download_always_pull', 'value': False}
        )  => {"ansible_loop_var": "item", "changed": false, "item": {"name": "download_
        always_pull", "value": false}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'name': 'helm_enabled', 'value': False})  => {"
        ansible_loop_var": "item", "changed": false, "item": {"name": "helm_enabled", "v
        alue": false}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'name': 'openstack_lbaas_enabled', 'value': Fal
        se})  => {"ansible_loop_var": "item", "changed": false, "item": {"name": "openst
        ack_lbaas_enabled", "value": false}, "skip_reason": "Conditional result was Fals
        e"}
        
        TASK [kubernetes/preinstall : Stop if even number of etcd hosts] ***************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:67
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if memory is too small for masters] *********
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:74
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if memory is too small for nodes] ***********
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:81
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Guarantee that enough network address space is ava
        ilable for all pods] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:93
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if ip var does not match local ips] *********
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:103
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if access_ip is not pingable] ***************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:111
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if RBAC is not enabled when dashboard is enab
        led] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:118
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if RBAC is not enabled when OCI cloud control
        ler is enabled] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:125
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if RBAC and anonymous-auth are not enabled wh
        en insecure port is disabled] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:132
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if kernel version is too low] ***************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:139
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if bad hostname] ****************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:146
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : check cloud_provider value] **********************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:152
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Ensure minimum calico version] *******************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:163
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Get current calico cluster version] **************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:171
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Check that current calico version is enough for up
        grade] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:184
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Check that cluster_id is set if calico_rr enabled]
         ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:196
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Check that calico_rr nodes are in k8s-cluster grou
        p] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:207
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Check that kube_service_addresses is a network ran
        ge] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:216
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Check that kube_pods_subnet is a network range] **
        *
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:223
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Check that kube_pods_subnet does not collide with
        kube_service_addresses] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:230
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if unknown dns mode] ************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:237
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if unknown kube proxy mode] *****************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:244
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if vault is chose] **************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:251
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if unknown cert_management] *****************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:258
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if unknown resolvconf_mode] *****************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:264
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if etcd deployment type is not host or docker
        ] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:271
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if etcd deployment type is not host when cont
        ainer_manager != docker] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:279
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if download_localhost is enabled but download
        _run_once is not] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:288
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if kata_containers_enabled is enabled when co
        ntainer_manager is docker] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:294
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if download_localhost is enabled for Flatcar
        Container Linux] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0020-verify-settings.yml:300
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Force binaries directory for Flatcar Container Lin
        ux by Kinvolk] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0040-set_facts.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : check if booted with ostree] *********************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0040-set_facts.yml:9
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : set is_fedora_coreos] ****************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0040-set_facts.yml:14
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : set is_fedora_coreos] ****************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0040-set_facts.yml:23
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : check resolvconf] ********************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0040-set_facts.yml:27
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : check systemd-resolved] **************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0040-set_facts.yml:34
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : set dns facts] ***********************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0040-set_facts.yml:42
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : check if kubelet is configured] ******************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0040-set_facts.yml:59
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : check if early DNS configuration stage] **********
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0040-set_facts.yml:65
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : target resolv.conf files] ************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0040-set_facts.yml:70
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : target temporary resolvconf cloud init file (Flatc
        ar Container Linux by Kinvolk / Fedora CoreOS)] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0040-set_facts.yml:79
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : check if /etc/dhclient.conf exists] **************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0040-set_facts.yml:84
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : target dhclient conf file for /etc/dhclient.conf]
        ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0040-set_facts.yml:89
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : check if /etc/dhcp/dhclient.conf exists] *********
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0040-set_facts.yml:94
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : target dhclient conf file for /etc/dhcp/dhclient.c
        onf] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0040-set_facts.yml:99
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : target dhclient hook file for Red Hat family] ****
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0040-set_facts.yml:104
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : target dhclient hook file for Debian family] *****
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0040-set_facts.yml:109
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : generate search domains to resolvconf] ***********
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0040-set_facts.yml:114
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : pick coredns cluster IP or default resolver] *****
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0040-set_facts.yml:125
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : generate nameservers to resolvconf] **************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0040-set_facts.yml:140
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : gather os specific variables] ********************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0040-set_facts.yml:147
        skipping: [kubenode01] => (item=/wire-server-deploy/ansible/roles-external/kubes
        pray/roles/kubernetes/preinstall/vars/../vars/ubuntu.yml)  => {"ansible_loop_var
        ": "item", "changed": false, "item": "/wire-server-deploy/ansible/roles-external
        /kubespray/roles/kubernetes/preinstall/vars/../vars/ubuntu.yml", "skip_reason":
        "Conditional result was False"}
        
        TASK [kubernetes/preinstall : set etcd vars if using kubeadm mode] *************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0040-set_facts.yml:161
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : check /usr readonly] *****************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0040-set_facts.yml:170
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : set alternate flexvolume path] *******************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0040-set_facts.yml:175
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Create kubernetes directories] *******************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0050-create_directories.yml:2
        skipping: [kubenode01] => (item=/etc/kubernetes)  => {"ansible_loop_var": "item"
        , "changed": false, "item": "/etc/kubernetes", "skip_reason": "Conditional resul
        t was False"}
        skipping: [kubenode01] => (item=/etc/kubernetes/ssl)  => {"ansible_loop_var": "i
        tem", "changed": false, "item": "/etc/kubernetes/ssl", "skip_reason": "Condition
        al result was False"}
        skipping: [kubenode01] => (item=/etc/kubernetes/manifests)  => {"ansible_loop_va
        r": "item", "changed": false, "item": "/etc/kubernetes/manifests", "skip_reason"
        : "Conditional result was False"}
        skipping: [kubenode01] => (item=/usr/local/bin/kubernetes-scripts)  => {"ansible
        _loop_var": "item", "changed": false, "item": "/usr/local/bin/kubernetes-scripts
        ", "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item=/usr/libexec/kubernetes/kubelet-plugins/volume/e
        xec)  => {"ansible_loop_var": "item", "changed": false, "item": "/usr/libexec/ku
        bernetes/kubelet-plugins/volume/exec", "skip_reason": "Conditional result was Fa
        lse"}
        
        TASK [kubernetes/preinstall : Create other directories] ************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0050-create_directories.yml:26
        skipping: [kubenode01] => (item=/usr/local/bin)  => {"ansible_loop_var": "item",
         "changed": false, "item": "/usr/local/bin", "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Check if kubernetes kubeadm compat cert dir exists
        ] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0050-create_directories.yml:46
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Create kubernetes kubeadm compat cert dir (kuberne
        tes/kubeadm issue 1498)] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0050-create_directories.yml:54
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Create cni directories] **************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0050-create_directories.yml:64
        skipping: [kubenode01] => (item=/etc/cni/net.d)  => {"ansible_loop_var": "item",
         "changed": false, "item": "/etc/cni/net.d", "skip_reason": "Conditional result
        was False"}
        skipping: [kubenode01] => (item=/opt/cni/bin)  => {"ansible_loop_var": "item", "
        changed": false, "item": "/opt/cni/bin", "skip_reason": "Conditional result was
        False"}
        skipping: [kubenode01] => (item=/var/lib/calico)  => {"ansible_loop_var": "item"
        , "changed": false, "item": "/var/lib/calico", "skip_reason": "Conditional resul
        t was False"}
        
        TASK [kubernetes/preinstall : Create local volume provisioner directories] *****
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0050-create_directories.yml:87
        skipping: [kubenode01] => (item=local-storage)  => {"ansible_loop_var": "item",
        "changed": false, "item": "local-storage", "skip_reason": "Conditional result wa
        s False"}
        
        TASK [kubernetes/preinstall : create temporary resolveconf cloud init file] ****
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0060-resolvconf.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Add domain/search/nameservers/options to resolv.co
        nf] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0060-resolvconf.yml:6
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Remove search/domain/nameserver options before blo
        ck] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0060-resolvconf.yml:23
        skipping: [kubenode01] => (item=['/etc/resolv.conf', 'search '])  => {"ansible_l
        oop_var": "item", "changed": false, "item": ["/etc/resolv.conf", "search "], "sk
        ip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item=['/etc/resolv.conf', 'nameserver '])  => {"ansib
        le_loop_var": "item", "changed": false, "item": ["/etc/resolv.conf", "nameserver
         "], "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item=['/etc/resolv.conf', 'domain '])  => {"ansible_l
        oop_var": "item", "changed": false, "item": ["/etc/resolv.conf", "domain "], "sk
        ip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item=['/etc/resolv.conf', 'options '])  => {"ansible_
        loop_var": "item", "changed": false, "item": ["/etc/resolv.conf", "options "], "
        skip_reason": "Conditional result was False"}
        
        TASK [kubernetes/preinstall : Remove search/domain/nameserver options after bloc
        k] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0060-resolvconf.yml:34
        skipping: [kubenode01] => (item=['/etc/resolv.conf', 'search '])  => {"ansible_l
        oop_var": "item", "changed": false, "item": ["/etc/resolv.conf", "search "], "sk
        ip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item=['/etc/resolv.conf', 'nameserver '])  => {"ansib
        le_loop_var": "item", "changed": false, "item": ["/etc/resolv.conf", "nameserver
         "], "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item=['/etc/resolv.conf', 'domain '])  => {"ansible_l
        oop_var": "item", "changed": false, "item": ["/etc/resolv.conf", "domain "], "sk
        ip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item=['/etc/resolv.conf', 'options '])  => {"ansible_
        loop_var": "item", "changed": false, "item": ["/etc/resolv.conf", "options "], "
        skip_reason": "Conditional result was False"}
        
        TASK [kubernetes/preinstall : get temporary resolveconf cloud init file content]
         ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0060-resolvconf.yml:47
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : persist resolvconf cloud init file] **************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0060-resolvconf.yml:52
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Write resolved.conf] *****************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0061-systemd-resolved.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : NetworkManager | Add nameservers to NM configurati
        on] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0062-networkmanager.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : NetworkManager | Add DNS search to NM configuratio
        n] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0062-networkmanager.yml:12
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : NetworkManager | Add DNS options to NM configurati
        on] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0062-networkmanager.yml:22
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Update package management cache (zypper) - SUSE] *
        **
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0070-system-packages.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Update package management cache (APT)] ***********
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0070-system-packages.yml:12
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Remove legacy docker repo file] ******************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0070-system-packages.yml:20
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Install python3-dnf for latest RedHat versions] **
        *
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0070-system-packages.yml:28
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Install epel-release on RedHat/CentOS] ***********
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0070-system-packages.yml:42
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Update common_required_pkgs with ipvsadm when kube
        _proxy_mode is ipvs] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0070-system-packages.yml:53
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Install packages requirements] *******************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0070-system-packages.yml:58
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Install ipvsadm for ClearLinux] ******************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0070-system-packages.yml:70
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Confirm selinux deployed] ************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0080-system-configurations.yml:3
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Set selinux policy] ******************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0080-system-configurations.yml:11
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Disable IPv6 DNS lookup] *************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0080-system-configurations.yml:23
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stat sysctl file configuration] ******************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0080-system-configurations.yml:36
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Change sysctl file path to link source if linked]
        ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0080-system-configurations.yml:43
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Make sure sysctl file path folder exists] ********
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0080-system-configurations.yml:52
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Enable ip forwarding] ****************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0080-system-configurations.yml:57
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Ensure kube-bench parameters are set] ************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0080-system-configurations.yml:65
        skipping: [kubenode01] => (item={'name': 'vm.overcommit_memory', 'value': 1})  =
        > {"ansible_loop_var": "item", "changed": false, "item": {"name": "vm.overcommit
        _memory", "value": 1}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'name': 'kernel.panic', 'value': 10})  => {"ans
        ible_loop_var": "item", "changed": false, "item": {"name": "kernel.panic", "valu
        e": 10}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'name': 'kernel.panic_on_oops', 'value': 1})  =
        > {"ansible_loop_var": "item", "changed": false, "item": {"name": "kernel.panic_
        on_oops", "value": 1}, "skip_reason": "Conditional result was False"}
        
        TASK [kubernetes/preinstall : Hosts | create list from inventory] **************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0090-etchosts.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Hosts | populate inventory into hosts file] ******
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0090-etchosts.yml:16
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Hosts | populate kubernetes loadbalancer address i
        nto hosts file] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0090-etchosts.yml:27
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Hosts | Retrieve hosts file content] *************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0090-etchosts.yml:39
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Hosts | Extract existing entries for localhost fro
        m hosts file] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0090-etchosts.yml:44
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Hosts | Update target hosts file entries dict with
         required entries] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0090-etchosts.yml:54
        skipping: [kubenode01] => (item={'key': '127.0.0.1', 'value': {'expected': ['loc
        alhost', 'localhost.localdomain']}})  => {"ansible_loop_var": "item", "changed":
         false, "item": {"key": "127.0.0.1", "value": {"expected": ["localhost", "localh
        ost.localdomain"]}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': '::1', 'value': {'expected': ['localhost
        6', 'localhost6.localdomain'], 'unexpected': ['localhost', 'localhost.localdomai
        n']}})  => {"ansible_loop_var": "item", "changed": false, "item": {"key": "::1",
         "value": {"expected": ["localhost6", "localhost6.localdomain"], "unexpected": [
        "localhost", "localhost.localdomain"]}}, "skip_reason": "Conditional result was
        False"}
        
        TASK [kubernetes/preinstall : Hosts | Update (if necessary) hosts file] ********
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0090-etchosts.yml:61
        skipping: [kubenode01] => (item={'key': '127.0.0.1', 'value': ['localhost', 'loc
        alhost.localdomain']})  => {"ansible_loop_var": "item", "changed": false, "item"
        : {"key": "127.0.0.1", "value": ["localhost", "localhost.localdomain"]}, "skip_r
        eason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': '::1', 'value': ['ip6-localhost', 'ip6-l
        oopback', 'localhost6', 'localhost6.localdomain']})  => {"ansible_loop_var": "it
        em", "changed": false, "item": {"key": "::1", "value": ["ip6-localhost", "ip6-lo
        opback", "localhost6", "localhost6.localdomain"]}, "skip_reason": "Conditional r
        esult was False"}
        
        TASK [kubernetes/preinstall : Update facts] ************************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0090-etchosts.yml:72
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Configure dhclient to supersede search/domain/name
        servers] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0100-dhclient-hooks.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Configure dhclient hooks for resolv.conf (non-RH)]
         ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0100-dhclient-hooks.yml:17
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Configure dhclient hooks for resolv.conf (RH-only)
        ] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0100-dhclient-hooks.yml:26
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Remove kubespray specific config from dhclient con
        fig] ***
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0110-dhclient-hooks-undo.yml:6
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Remove kubespray specific dhclient hook] *********
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0110-dhclient-hooks-undo.yml:15
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        META: ran handlers
        
        TASK [kubernetes/preinstall : Check if we are running inside a Azure VM] *******
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/main.yml:92
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : install growpart] ********************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0120-growpart-azure-centos-7.yml:5
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : check if growpart needs to be run] ***************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0120-growpart-azure-centos-7.yml:10
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : check fs type] ***********************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0120-growpart-azure-centos-7.yml:18
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : run growpart] ************************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0120-growpart-azure-centos-7.yml:23
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : run xfs_growfs] **********************************
        task path: /wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes
        /preinstall/tasks/0120-growpart-azure-centos-7.yml:29
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        META: ran handlers
        META: ran handlers
        
        PLAY [k8s-cluster] *************************************************************
        META: ran handlers
        
        TASK [Annotate nodes] **********************************************************
        task path: /wire-server-deploy/ansible/kubernetes.yml:12
        META: ran handlers
        META: ran handlers
        
        PLAY [k8s-cluster] *************************************************************
        META: ran handlers
        
        TASK [nickhammond.logrotate | Install logrotate] *******************************
        task path: /wire-server-deploy/ansible/roles-external/logrotate/tasks/main.yml:2
        ok: [kubenode01] => {"cache_update_time": 1625831691, "cache_updated": false, "c
        hanged": false}
        
        TASK [nickhammond.logrotate | Setup logrotate.d scripts] ***********************
        task path: /wire-server-deploy/ansible/roles-external/logrotate/tasks/main.yml:8
        ok: [kubenode01] => (item={'name': 'podlogs', 'path': '/var/lib/docker/container
        s/*/*.log', 'options': ['daily', 'missingok', 'rotate 2', 'maxage 1', 'copytrunc
        ate', 'nocreate', 'nocompress']}) => {"ansible_loop_var": "item", "changed": fal
        se, "checksum": "cbb2a32ef27e84ded753a3d749c86e0aed963f33", "dest": "/etc/logrot
        ate.d/podlogs", "gid": 0, "group": "root", "item": {"name": "podlogs", "options"
        : ["daily", "missingok", "rotate 2", "maxage 1", "copytruncate", "nocreate", "no
        compress"], "path": "/var/lib/docker/containers/*/*.log"}, "mode": "0600", "owne
        r": "root", "path": "/etc/logrotate.d/podlogs", "size": 143, "state": "file", "u
        id": 0}
        META: ran handlers
        META: ran handlers
        
        PLAY [Bringing kubeconfig in place] ********************************************
        META: ran handlers
        
        TASK [Checking if 'kubeconfig' file already exists] ****************************
        task path: /wire-server-deploy/ansible/kubernetes.yml:24
        ok: [kubenode01] => {"changed": false, "stat": {"exists": false}}
        
        TASK [Renaming kubeconfig file provided by Kubespray] **************************
        task path: /wire-server-deploy/ansible/kubernetes.yml:30
        [WARNING]: File '/wire-server-deploy/ansible/inventory/demo/../kubeconfig.dec'
        created with default permissions '600'. The previous default was '666'. Specify
        'mode' to avoid this warning.
        changed: [kubenode01] => {"changed": true, "checksum": "e6cd6613668afec2a22d5afc
        790417ba75f63054", "dest": "/wire-server-deploy/ansible/inventory/demo/../kubeco
        nfig.dec", "gid": 0, "group": "root", "md5sum": "041bb958ff156cbc47775553b6b077c
        8", "mode": "0600", "owner": "root", "size": 5638, "src": "/root/.ansible/tmp/an
        sible-tmp-1625832684.9764237-3644-169228438435371/source", "state": "file", "uid
        ": 0}
        
        TASK [debug] *******************************************************************
        task path: /wire-server-deploy/ansible/kubernetes.yml:34
        ok: [kubenode01] => {
            "msg": "TODO: Encrypt /wire-server-deploy/ansible/inventory/demo/../kubeconf
        ig.dec with sops"
        }
        META: ran handlers
        META: ran handlers
        
        PLAY [etcd] ********************************************************************
        META: ran handlers
        
        TASK [etcd-helpers : Add etcd helper scripts] **********************************
        task path: /wire-server-deploy/ansible/roles/etcd-helpers/tasks/main.yml:3
        ok: [kubenode01] => (item=etcd-health.sh) => {"ansible_loop_var": "item", "chang
        ed": false, "checksum": "9626e733cbc0c5ab3f512aba5031291a45c8ad43", "dest": "/us
        r/local/bin/etcd-health.sh", "gid": 0, "group": "root", "item": "etcd-health.sh"
        , "mode": "0755", "owner": "root", "path": "/usr/local/bin/etcd-health.sh", "siz
        e": 236, "state": "file", "uid": 0}
        ok: [kubenode01] => (item=etcdctl3.sh) => {"ansible_loop_var": "item", "changed"
        : false, "checksum": "cd5c88dfe6cd230dfdd01cd673280a2b5a3b66d0", "dest": "/usr/l
        ocal/bin/etcdctl3.sh", "gid": 0, "group": "root", "item": "etcdctl3.sh", "mode":
         "0755", "owner": "root", "path": "/usr/local/bin/etcdctl3.sh", "size": 249, "st
        ate": "file", "uid": 0}
        META: ran handlers
        META: ran handlers
        
        PLAY RECAP *********************************************************************
        kubenode01                 : ok=480  changed=31   unreachable=0    failed=0    s
        kipped=1036 rescued=0    ignored=0
        localhost                  : ok=1    changed=0    unreachable=0    failed=0    s
        kipped=0    rescued=0    ignored=0
        
[14:11:41] # Running command «find . -name 'admin.conf'» on 192.168.1.121
        bash-4.4# find . -name 'admin.conf'
        ./inventory/demo/artifacts/admin.conf
[14:11:48] # Running command «mkdir -p ~/.kube/» on 192.168.1.121
        bash-4.4# mkdir -p ~/.kube/
[14:11:56] # Running command «cp ./inventory/demo/artifacts/admin.conf ~/.kube/config» on 192.168.1.121
        bash-4.4# cp ./inventory/demo/artifacts/admin.conf ~/.kube/config
[14:12:04] # Running command «KUBECONFIG=~/.kube/config» on 192.168.1.121
        bash-4.4# KUBECONFIG=~/.kube/config
[14:12:11] # Running command «which kubectl» on 192.168.1.121
        bash-4.4# which kubectl
        /bin/kubectl
[14:12:19] # Running command «kubectl version» on 192.168.1.121
        bash-4.4# kubectl version
        Client Version: version.Info{Major:"1", Minor:"19", GitVersion:"v1.19.7", GitCom
        mit:"1dd5338295409edcfff11505e7bb246f0d325d15", GitTreeState:"clean", BuildDate:
        "2021-01-13T13:23:52Z", GoVersion:"go1.15.5", Compiler:"gc", Platform:"linux/amd
        64"}
        Server Version: version.Info{Major:"1", Minor:"19", GitVersion:"v1.19.7", GitCom
        mit:"1dd5338295409edcfff11505e7bb246f0d325d15", GitTreeState:"clean", BuildDate:
        "2021-01-13T13:15:20Z", GoVersion:"go1.15.5", Compiler:"gc", Platform:"linux/amd
        64"}
[14:12:37] # Running command «kubectl get pods -A » on 192.168.1.121
        bash-4.4# kubectl get pods -A
        NAMESPACE     NAME                                       READY   STATUS
            RESTARTS   AGE
        default       brig-59f54c5b65-snw95                      1/1     Running
            0          17h
        default       cannon-0                                   1/1     Running
            0          42h
        default       cargohold-546664f7df-ls47t                 1/1     Running
            0          42h
        default       cassandra-ephemeral-0                      1/1     Running
            0          42h
        default       cassandra-migrations-s2z75                 0/1     Completed
            0          17h
        default       demo-smtp-85557f6877-2rkrw                 1/1     Running
            0          42h
        default       elasticsearch-ephemeral-86f4b8ff6f-9z65d   1/1     Running
            0          42h
        default       elasticsearch-index-create-5xfnb           0/1     Completed
            0          17h
        default       fake-aws-s3-77d9447b8f-pxrkv               1/1     Running
            0          42h
        default       fake-aws-s3-reaper-78d9f58dd4-w4k46        1/1     Running
            0          42h
        default       fake-aws-sns-6c7c4b7479-htf9s              2/2     Running
            0          42h
        default       fake-aws-sqs-59fbfbcbd4-74586              2/2     Running
            0          42h
        default       galley-5b7b69cd75-plpg7                    1/1     Running
            0          42h
        default       gundeck-84cdfbc564-46k9j                   1/1     Running
            0          42h
        default       nginz-5485fbf785-8vmj2                     2/2     Running
            0          17h
        default       redis-ephemeral-master-0                   1/1     Running
            0          42h
        default       spar-5859c8f9d-6jkjk                       0/1     CrashLoopBackOf
        f   217        17h
        default       spar-5859c8f9d-ksgbl                       0/1     CrashLoopBackOf
        f   217        17h
        default       spar-5859c8f9d-t9nr4                       0/1     CrashLoopBackOf
        f   218        17h
        default       spar-5b7588fc4c-58ngk                      0/1     CrashLoopBackOf
        f   508        42h
        default       spar-5b7588fc4c-jxxsr                      0/1     CrashLoopBackOf
        f   508        42h
        default       spar-5b7588fc4c-k8fd4                      0/1     CrashLoopBackOf
        f   508        42h
        default       webapp-7678f9457-tt7vm                     1/1     Running
            0          42h
        kube-system   coredns-7677f9bb54-2m4gq                   0/1     Pending
            0          42h
        kube-system   coredns-7677f9bb54-h6vtx                   1/1     Running
            0          42h
        kube-system   dns-autoscaler-5b7b5c9b6f-4dqq7            1/1     Running
            0          42h
        kube-system   kube-apiserver-kubenode01                  1/1     Running
            0          42h
        kube-system   kube-controller-manager-kubenode01         1/1     Running
            0          42h
        kube-system   kube-flannel-vtqsv                         1/1     Running
            0          42h
        kube-system   kube-proxy-xvsjd                           1/1     Running
            0          2m9s
        kube-system   kube-scheduler-kubenode01                  1/1     Running
            0          42h
        kube-system   nodelocaldns-v6m7d                         1/1     Running
            0          42h
[14:12:45] # Running command «free -m && uptime» on 192.168.1.121
        bash-4.4# free -m && uptime
                      total        used        free      shared  buff/cache   available
        Mem:           2928         258         570           0        2098        2521
        Swap:          3943           0        3943
         12:12:41 up 25 min,  load average: 0.97, 0.86, 0.72
[14:12:52] # Running command «free -m && uptime» on 95.216.208.159
        wire@arthur-demo:~$ free -m && uptime
                      total        used        free      shared  buff/cache   available
        Mem:          15661        5558         438           4        9663       10147
        Swap:             0           0           0
         14:12:49 up 1 day, 19:03,  0 users,  load average: 1.09, 1.31, 1.26
[14:13:00] # Running command «helm version» on 192.168.1.121
        bash-4.4# helm version
        version.BuildInfo{Version:"v3.5.2", GitCommit:"", GitTreeState:"", GoVersion:"go
        1.16"}
[14:13:10] # Running command «helm repo add wire https://s3-eu-west-1.amazonaws.com/public.wire.com/charts» on 192.168.1.121
        bash-4.4# helm repo add wire https://s3-eu-west-1.amazonaws.com/public.wire.com/
        charts
        "wire" has been added to your repositories
[14:13:17] # Running command «helm search repo wire/» on 192.168.1.121
        bash-4.4# helm search repo wire/
        NAME                            CHART VERSION   APP VERSION     DESCRIPTION
        
        wire/account-pages              0.130.0                         A Helm chart for
         the Wire account pages in Kube...
        wire/aws-ingress                2.109.0                         A Helm chart for
         ingresses (AWS specific) on Ku...
        wire/aws-storage                0.130.0                         AWS storage clas
        ses
        wire/backoffice                 2.109.0                         Backoffice tool
        
        wire/brig                       0.130.0                         Brig (part of Wi
        re Server) - User management
        wire/calling-test               2.109.0         1.0.14          Network testing
        tool for audio/video/signalling...
        wire/cannon                     0.130.0                         A Helm chart for
         cannon in Kubernetes
        wire/cargohold                  0.130.0                         Cargohold (part
        of Wire Server) - Asset storage
        wire/cassandra-ephemeral        0.130.0                         Wrapper chart fo
        r incubator/cassandra with cust...
        wire/cassandra-external         2.109.0                         Refer to cassand
        ra IPs located outside kubernet...
        wire/cassandra-migrations       0.130.0                         cassandra databa
        se schema migration for gundeck...
        wire/databases-ephemeral        2.109.0                         A Helm chart in-
        memory, ephemeral databases for...
        wire/demo-smtp                  2.109.0         1.0             A demo helm char
        t to send emails. Not productio...
        wire/elasticsearch-curator      2.109.0                         Wrapper chart fo
        r stable/elasticsearch-curator
        wire/elasticsearch-ephemeral    2.109.0                         Dummy ephemeral
        elasticsearch
        wire/elasticsearch-external     2.109.0                         Refer to elastic
        search IPs located outside kube...
        wire/elasticsearch-index        0.130.0                         Elasticsearch in
        dex for brig
        wire/fake-aws                   2.109.0                         A Helm chart for
         fake-aws services (replacing r...
        wire/fake-aws-dynamodb          0.130.0                         Dummy ephemeral
        DynamoDB service
        wire/fake-aws-s3                2.109.0                         Wrapper chart fo
        r stable/minio
        wire/fake-aws-ses               0.130.0                         Dummy ephemeral
        SES service (based on localstack)
        wire/fake-aws-sns               0.130.0                         Dummy ephemeral
        SNS service (based on localstack)
        wire/fake-aws-sqs               2.109.0                         Dummy ephemeral
        SQS service
        wire/fluent-bit                 2.109.0                         Wrapper chart fo
        r stable/fluent-bit
        wire/galley                     0.130.0                         Galley (part of
        Wire Server) - Conversations
        wire/gundeck                    0.130.0                         Gundeck (part of
         Wire Server) - Push Notificati...
        wire/kibana                     2.109.0                         Wrapper chart fo
        r stable/kibana
        wire/legalhold                  0.130.0                         A Helm chart for
         legalhold
        wire/metallb                    0.130.0                         A Helm chart for
         metallb on Kubernetes
        wire/minio-external             2.109.0                         Refer to minio I
        Ps located outside kubernetes b...
        wire/nginx-ingress-controller   2.109.0                         A Helm chart for
         an ingress controller (using n...
        wire/nginx-ingress-services     2.109.0                         A Helm chart for
         ingresses and services on Kube...
        wire/nginx-lb-ingress           0.1.3                           A Helm chart for
         ingresses (using nginx) on Kub...
        wire/nginz                      0.130.0                         A Helm chart for
         nginz in Kubernetes
        wire/proxy                      0.130.0                         Proxy (part of W
        ire Server) - 3rd party proxy s...
        wire/reaper                     2.109.0         0.1.0           A helm charts to
         restart cannons if redis-ephem...
        wire/redis-ephemeral            2.109.0                         Wrapper chart fo
        r stable/redis
        wire/sftd                       2.109.0         1.0.88          SFTD is a compon
        ent for engaging in conference ...
        wire/spar                       0.130.0                         Spar (part of Wi
        re Server) - SSO Service
        wire/team-settings              0.130.0                         A Helm chart for
         the Wire team-settings in Kube...
        wire/webapp                     0.130.0                         A Helm chart for
         the Wire webapp in Kubernetes
        wire/wire-server                2.109.0                         A Helm chart for
         wire-server https://github.com...
        wire/wire-server-metrics        2.109.0         1.0             Adds monitoring
        for the kubernetes cluster and ...
[14:13:25] # Running command «kubectl get pods -A» on 192.168.1.121
        bash-4.4# kubectl get pods -A
        NAMESPACE     NAME                                       READY   STATUS
            RESTARTS   AGE
        default       brig-59f54c5b65-snw95                      1/1     Running
            0          17h
        default       cannon-0                                   1/1     Running
            0          42h
        default       cargohold-546664f7df-ls47t                 1/1     Running
            0          42h
        default       cassandra-ephemeral-0                      1/1     Running
            0          42h
        default       cassandra-migrations-s2z75                 0/1     Completed
            0          17h
        default       demo-smtp-85557f6877-2rkrw                 1/1     Running
            0          42h
        default       elasticsearch-ephemeral-86f4b8ff6f-9z65d   1/1     Running
            0          42h
        default       elasticsearch-index-create-5xfnb           0/1     Completed
            0          17h
        default       fake-aws-s3-77d9447b8f-pxrkv               1/1     Running
            0          42h
        default       fake-aws-s3-reaper-78d9f58dd4-w4k46        1/1     Running
            0          42h
        default       fake-aws-sns-6c7c4b7479-htf9s              2/2     Running
            0          42h
        default       fake-aws-sqs-59fbfbcbd4-74586              2/2     Running
            0          42h
        default       galley-5b7b69cd75-plpg7                    1/1     Running
            0          42h
        default       gundeck-84cdfbc564-46k9j                   1/1     Running
            0          42h
        default       nginz-5485fbf785-8vmj2                     2/2     Running
            0          17h
        default       redis-ephemeral-master-0                   1/1     Running
            0          42h
        default       spar-5859c8f9d-6jkjk                       0/1     CrashLoopBackOf
        f   218        17h
        default       spar-5859c8f9d-ksgbl                       0/1     CrashLoopBackOf
        f   218        17h
        default       spar-5859c8f9d-t9nr4                       0/1     CrashLoopBackOf
        f   219        17h
        default       spar-5b7588fc4c-58ngk                      0/1     CrashLoopBackOf
        f   509        42h
        default       spar-5b7588fc4c-jxxsr                      0/1     CrashLoopBackOf
        f   509        42h
        default       spar-5b7588fc4c-k8fd4                      0/1     CrashLoopBackOf
        f   509        42h
        default       webapp-7678f9457-tt7vm                     1/1     Running
            0          42h
        kube-system   coredns-7677f9bb54-2m4gq                   0/1     Pending
            0          42h
        kube-system   coredns-7677f9bb54-h6vtx                   1/1     Running
            0          42h
        kube-system   dns-autoscaler-5b7b5c9b6f-4dqq7            1/1     Running
            0          42h
        kube-system   kube-apiserver-kubenode01                  1/1     Running
            0          42h
        kube-system   kube-controller-manager-kubenode01         1/1     Running
            0          42h
        kube-system   kube-flannel-vtqsv                         1/1     Running
            0          42h
        kube-system   kube-proxy-xvsjd                           1/1     Running
            0          2m57s
        kube-system   kube-scheduler-kubenode01                  1/1     Running
            0          42h
        kube-system   nodelocaldns-v6m7d                         1/1     Running
            0          42h
[14:13:49] # Running command «helm upgrade --install databases-ephemeral wire/databases-ephemeral --wait --debug» on 192.168.1.121
        bash-4.4# helm upgrade --install databases-ephemeral wire/databases-ephemeral --
        wait --debug
        history.go:56: [debug] getting history for release databases-ephemeral
        upgrade.go:123: [debug] preparing upgrade for databases-ephemeral
        upgrade.go:131: [debug] performing update for databases-ephemeral
        upgrade.go:303: [debug] creating upgraded release for databases-ephemeral
        client.go:201: [debug] checking 10 resources for changes
        client.go:464: [debug] Looks like there are no changes for ConfigMap "redis-ephe
        meral-scripts"
        client.go:464: [debug] Looks like there are no changes for ConfigMap "redis-ephe
        meral"
        client.go:464: [debug] Looks like there are no changes for ConfigMap "redis-ephe
        meral-health"
        client.go:464: [debug] Looks like there are no changes for Service "cassandra-ep
        hemeral"
        client.go:464: [debug] Looks like there are no changes for Service "elasticsearc
        h-ephemeral"
        client.go:464: [debug] Looks like there are no changes for Service "redis-epheme
        ral-headless"
        client.go:464: [debug] Looks like there are no changes for Service "redis-epheme
        ral-master"
        wait.go:53: [debug] beginning wait for 10 resources with timeout of 5m0s
        upgrade.go:138: [debug] updating status for upgraded release for databases-ephem
        eral
        Release "databases-ephemeral" has been upgraded. Happy Helming!
        NAME: databases-ephemeral
        LAST DEPLOYED: Fri Jul  9 12:13:41 2021
        NAMESPACE: default
        STATUS: deployed
        REVISION: 3
        TEST SUITE: None
        USER-SUPPLIED VALUES:
        {}
        
        COMPUTED VALUES:
        cassandra-ephemeral:
          cassandra-ephemeral:
            affinity: {}
            argsOverrides: []
            backup:
              annotations:
                iam.amazonaws.com/role: cain
              destination: s3://bucket/cassandra
              enabled: false
              env:
              - name: AWS_REGION
                value: us-east-1
              extraArgs: []
              image:
                repository: maorfr/cain
                tag: 0.6.0
              resources:
                limits:
                  cpu: 1
                  memory: 1Gi
                requests:
                  cpu: 1
                  memory: 1Gi
              schedule:
              - cron: 0 7 * * *
                keyspace: keyspace1
              - cron: 30 7 * * *
                keyspace: keyspace2
            commandOverrides: []
            config:
              cluster_domain: cluster.local
              cluster_name: cassandra
              cluster_size: 1
              dc_name: DC1
              endpoint_snitch: SimpleSnitch
              heap_new_size: 1024M
              max_heap_size: 2048M
              num_tokens: 256
              ports:
                cql: 9042
                thrift: 9160
              rack_name: RAC1
              seed_size: 1
              start_rpc: false
            configOverrides: {}
            env: {}
            exporter:
              enabled: false
              image:
                repo: criteord/cassandra_exporter
                tag: 2.0.2
              jvmOpts: ""
              port: 5556
              resources: {}
              servicemonitor: true
            global: {}
            hostNetwork: false
            image:
              pullPolicy: IfNotPresent
              repo: cassandra
              tag: 3.11.3
            livenessProbe:
              failureThreshold: 3
              initialDelaySeconds: 90
              periodSeconds: 30
              successThreshold: 1
              timeoutSeconds: 5
            persistence:
              accessMode: ReadWriteOnce
              enabled: false
              size: 10Gi
            podAnnotations: {}
            podDisruptionBudget: {}
            podLabels: {}
            podManagementPolicy: OrderedReady
            podSettings:
              terminationGracePeriodSeconds: 30
            rbac:
              create: true
            readinessProbe:
              address: ${POD_IP}
              failureThreshold: 3
              initialDelaySeconds: 90
              periodSeconds: 30
              successThreshold: 1
              timeoutSeconds: 5
            resources:
              limits:
                cpu: "4"
                memory: 4.0Gi
              requests:
                cpu: "1"
                memory: 2.0Gi
            securityContext:
              enabled: false
              fsGroup: 999
              runAsUser: 999
            service:
              type: ClusterIP
            serviceAccount:
              create: true
            tolerations: []
            updateStrategy:
              type: RollingUpdate
          global: {}
        elasticsearch-ephemeral:
          global: {}
          image:
            repository: elasticsearch
            tag: 6.7.1
          resources:
            limits:
              cpu: 2000m
              memory: 4Gi
            requests:
              cpu: 250m
              memory: 500Mi
          service:
            httpPort: 9200
            transportPort: 9300
        redis-ephemeral:
          global: {}
          redis-ephemeral:
            cluster:
              enabled: false
              slaveCount: 2
            clusterDomain: cluster.local
            configmap: |-
              # Enable AOF https://redis.io/topics/persistence#append-only-file
              appendonly yes
              # Disable RDB persistence, AOF persistence already enabled.
              save ""
            containerSecurityContext:
              enabled: true
              runAsUser: 1001
            global:
              redis: {}
            image:
              pullPolicy: IfNotPresent
              registry: docker.io
              repository: bitnami/redis
              tag: 6.0.9-debian-10-r0
            master:
              affinity: {}
              command: /run.sh
              configmap: null
              customLivenessProbe: {}
              customReadinessProbe: {}
              disableCommands:
              - FLUSHDB
              - FLUSHALL
              extraEnvVars: []
              extraEnvVarsCM: []
              extraEnvVarsSecret: []
              extraFlags: []
              livenessProbe:
                enabled: true
                failureThreshold: 5
                initialDelaySeconds: 5
                periodSeconds: 5
                successThreshold: 1
                timeoutSeconds: 5
              persistence:
                accessModes:
                - ReadWriteOnce
                enabled: false
                matchExpressions: {}
                matchLabels: {}
                path: /data
                size: 8Gi
                subPath: ""
              podAnnotations: {}
              podLabels: {}
              preExecCmds: ""
              priorityClassName: ""
              readinessProbe:
                enabled: true
                failureThreshold: 5
                initialDelaySeconds: 5
                periodSeconds: 5
                successThreshold: 1
                timeoutSeconds: 1
              resources:
                limits:
                  cpu: 1000m
                  memory: 1024Mi
                requests:
                  cpu: 500m
                  memory: 512Mi
              service:
                annotations: {}
                labels: {}
                port: 6379
                type: ClusterIP
              shareProcessNamespace: false
              statefulset:
                labels: {}
                updateStrategy: RollingUpdate
            metrics:
              enabled: false
              image:
                pullPolicy: IfNotPresent
                registry: docker.io
                repository: bitnami/redis-exporter
                tag: 1.12.1-debian-10-r11
              podAnnotations:
                prometheus.io/port: "9121"
                prometheus.io/scrape: "true"
              prometheusRule:
                additionalLabels: {}
                enabled: false
                namespace: ""
                rules: []
              service:
                annotations: {}
                labels: {}
                type: ClusterIP
              serviceMonitor:
                enabled: false
                selector:
                  prometheus: kube-prometheus
            networkPolicy:
              enabled: false
              ingressNSMatchLabels: {}
              ingressNSPodMatchLabels: {}
            password: ""
            persistence: {}
            podDisruptionBudget:
              enabled: false
              minAvailable: 1
            podSecurityPolicy:
              create: false
            rbac:
              create: false
              role:
                rules: []
            redisPort: 6379
            securityContext:
              enabled: true
              fsGroup: 1001
            sentinel:
              customLivenessProbe: {}
              customReadinessProbe: {}
              downAfterMilliseconds: 60000
              enabled: false
              failoverTimeout: 18000
              image:
                pullPolicy: IfNotPresent
                registry: docker.io
                repository: bitnami/redis-sentinel
                tag: 6.0.8-debian-10-r55
              initialCheckTimeout: 5
              livenessProbe:
                enabled: true
                failureThreshold: 5
                initialDelaySeconds: 5
                periodSeconds: 5
                successThreshold: 1
                timeoutSeconds: 5
              masterSet: mymaster
              parallelSyncs: 1
              port: 26379
              quorum: 2
              readinessProbe:
                enabled: true
                failureThreshold: 5
                initialDelaySeconds: 5
                periodSeconds: 5
                successThreshold: 1
                timeoutSeconds: 1
              service:
                annotations: {}
                labels: {}
                redisPort: 6379
                sentinelPort: 26379
                type: ClusterIP
              staticID: false
              usePassword: true
            serviceAccount:
              create: false
            slave:
              affinity: {}
              command: /run.sh
              customLivenessProbe: {}
              customReadinessProbe: {}
              disableCommands:
              - FLUSHDB
              - FLUSHALL
              extraEnvVars: []
              extraEnvVarsCM: []
              extraEnvVarsSecret: []
              extraFlags: []
              livenessProbe:
                enabled: true
                failureThreshold: 5
                initialDelaySeconds: 30
                periodSeconds: 10
                successThreshold: 1
                timeoutSeconds: 5
              persistence:
                accessModes:
                - ReadWriteOnce
                enabled: true
                matchExpressions: {}
                matchLabels: {}
                path: /data
                size: 8Gi
                subPath: ""
              podAnnotations: {}
              podLabels: {}
              port: 6379
              preExecCmds: ""
              readinessProbe:
                enabled: true
                failureThreshold: 5
                initialDelaySeconds: 5
                periodSeconds: 10
                successThreshold: 1
                timeoutSeconds: 10
              service:
                annotations: {}
                labels: {}
                port: 6379
                type: ClusterIP
              shareProcessNamespace: false
              spreadConstraints: {}
              statefulset:
                labels: {}
                updateStrategy: RollingUpdate
            sysctlImage:
              command: []
              enabled: false
              mountHostSys: false
              pullPolicy: Always
              registry: docker.io
              repository: bitnami/minideb
              resources: {}
              tag: buster
            tls:
              authClients: true
              enabled: false
            usePassword: false
            usePasswordFile: false
            volumePermissions:
              enabled: false
              image:
                pullPolicy: Always
                registry: docker.io
                repository: bitnami/minideb
                tag: buster
              resources: {}
              securityContext:
                runAsUser: 0
        
        HOOKS:
        MANIFEST:
        ---
        # Source: databases-ephemeral/charts/redis-ephemeral/charts/redis-ephemeral/temp
        lates/configmap-scripts.yaml
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: redis-ephemeral-scripts
          namespace: "default"
          labels:
            app: redis-ephemeral
            chart: redis-ephemeral-11.3.4
            heritage: Helm
            release: databases-ephemeral
        data:
          start-master.sh: |
            #!/bin/bash
            useradd redis
            chown -R redis /data
            if [[ -n $REDIS_PASSWORD_FILE ]]; then
              password_aux=`cat ${REDIS_PASSWORD_FILE}`
              export REDIS_PASSWORD=$password_aux
            fi
            if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then
              cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/maste
        r.conf
            fi
            if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then
              cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.
        conf
            fi
            ARGS=("--port" "${REDIS_PORT}")
            ARGS+=("--protected-mode" "no")
            ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
            ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
            exec /run.sh "${ARGS[@]}"
        ---
        # Source: databases-ephemeral/charts/redis-ephemeral/charts/redis-ephemeral/temp
        lates/configmap.yaml
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: redis-ephemeral
          namespace: "default"
          labels:
            app: redis-ephemeral
            chart: redis-ephemeral-11.3.4
            heritage: Helm
            release: databases-ephemeral
        data:
          redis.conf: |-
            # User-supplied configuration:
            # Enable AOF https://redis.io/topics/persistence#append-only-file
            appendonly yes
            # Disable RDB persistence, AOF persistence already enabled.
            save ""
          master.conf: |-
            dir /data
            rename-command FLUSHDB ""
            rename-command FLUSHALL ""
          replica.conf: |-
            dir /data
            slave-read-only yes
            rename-command FLUSHDB ""
            rename-command FLUSHALL ""
        ---
        # Source: databases-ephemeral/charts/redis-ephemeral/charts/redis-ephemeral/temp
        lates/health-configmap.yaml
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: redis-ephemeral-health
          namespace: "default"
          labels:
            app: redis-ephemeral
            chart: redis-ephemeral-11.3.4
            heritage: Helm
            release: databases-ephemeral
        data:
          ping_readiness_local.sh: |-
            #!/bin/bash
            response=$(
              timeout -s 3 $1 \
              redis-cli \
                -h localhost \
                -p $REDIS_PORT \
                ping
            )
            if [ "$response" != "PONG" ]; then
              echo "$response"
              exit 1
            fi
          ping_liveness_local.sh: |-
            #!/bin/bash
            response=$(
              timeout -s 3 $1 \
              redis-cli \
                -h localhost \
                -p $REDIS_PORT \
                ping
            )
            if [ "$response" != "PONG" ] && [ "$response" != "LOADING Redis is loading t
        he dataset in memory" ]; then
              echo "$response"
              exit 1
            fi
          ping_readiness_master.sh: |-
            #!/bin/bash
             response=$(
              timeout -s 3 $1 \
              redis-cli \
                -h $REDIS_MASTER_HOST \
                -p $REDIS_MASTER_PORT_NUMBER \
                ping
            )
            if [ "$response" != "PONG" ]; then
              echo "$response"
              exit 1
            fi
          ping_liveness_master.sh: |-
            #!/bin/bash
            response=$(
              timeout -s 3 $1 \
              redis-cli \
                -h $REDIS_MASTER_HOST \
                -p $REDIS_MASTER_PORT_NUMBER \
                ping
            )
            if [ "$response" != "PONG" ] && [ "$response" != "LOADING Redis is loading t
        he dataset in memory" ]; then
              echo "$response"
              exit 1
            fi
          ping_readiness_local_and_master.sh: |-
            script_dir="$(dirname "$0")"
            exit_status=0
            "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
            "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
            exit $exit_status
          ping_liveness_local_and_master.sh: |-
            script_dir="$(dirname "$0")"
            exit_status=0
            "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
            "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
            exit $exit_status
        ---
        # Source: databases-ephemeral/charts/cassandra-ephemeral/charts/cassandra-epheme
        ral/templates/service.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: cassandra-ephemeral
          labels:
            app: cassandra-ephemeral
            chart: cassandra-ephemeral-0.13.3
            release: databases-ephemeral
            heritage: Helm
        spec:
          clusterIP: None
          type: ClusterIP
          ports:
          - name: intra
            port: 7000
            targetPort: 7000
          - name: tls
            port: 7001
            targetPort: 7001
          - name: jmx
            port: 7199
            targetPort: 7199
          - name: cql
            port: 9042
            targetPort: 9042
          - name: thrift
            port: 9160
            targetPort: 9160
          selector:
            app: cassandra-ephemeral
            release: databases-ephemeral
        ---
        # Source: databases-ephemeral/charts/elasticsearch-ephemeral/templates/es-svc.ya
        ml
        apiVersion: v1
        kind: Service
        metadata:
          name: elasticsearch-ephemeral
          labels:
            wireService: elasticsearch-ephemeral
            app: elasticsearch-ephemeral
            chart: "elasticsearch-ephemeral-2.109.0"
            release: "databases-ephemeral"
            heritage: "Helm"
            component: elasticsearch-ephemeral
        spec:
          type: ClusterIP
          selector:
            component: elasticsearch-ephemeral
          ports:
          - name: http
            port: 9200
            targetPort: 9200
            protocol: TCP
          - name: transport
            port: 9300
            targetPort: 9300
            protocol: TCP
        ---
        # Source: databases-ephemeral/charts/redis-ephemeral/charts/redis-ephemeral/temp
        lates/headless-svc.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: redis-ephemeral-headless
          namespace: "default"
          labels:
            app: redis-ephemeral
            chart: redis-ephemeral-11.3.4
            release: databases-ephemeral
            heritage: Helm
        spec:
          type: ClusterIP
          clusterIP: None
          ports:
            - name: redis
              port: 6379
              targetPort: redis
          selector:
            app: redis-ephemeral
            release: databases-ephemeral
        ---
        # Source: databases-ephemeral/charts/redis-ephemeral/charts/redis-ephemeral/temp
        lates/redis-master-svc.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: redis-ephemeral-master
          namespace: "default"
          labels:
            app: redis-ephemeral
            chart: redis-ephemeral-11.3.4
            release: databases-ephemeral
            heritage: Helm
        spec:
          type: ClusterIP
          ports:
            - name: redis
              port: 6379
              targetPort: redis
          selector:
            app: redis-ephemeral
            release: databases-ephemeral
            role: master
        ---
        # Source: databases-ephemeral/charts/elasticsearch-ephemeral/templates/es.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: elasticsearch-ephemeral
          labels:
            wireService: elasticsearch-ephemeral
            app: elasticsearch-ephemeral
            chart: "elasticsearch-ephemeral-2.109.0"
            release: "databases-ephemeral"
            heritage: "Helm"
            component: elasticsearch-ephemeral
        spec:
          replicas: 1
          selector:
            matchLabels:
              component: elasticsearch-ephemeral
          template:
            metadata:
              labels:
                component: elasticsearch-ephemeral
            spec:
              containers:
              - name: es
                image: "elasticsearch:6.7.1"
                env:
                - name: MAX_HEAP_SIZE
                  value: "2048"
                - name: HEAP_NEWSIZE
                  value: "800M"
                - name: "bootstrap.system_call_filter"
                  value: "false"
                - name: "discovery.type"
                  value: "single-node"
                ports:
                - containerPort: 9200
                  name: http
                  protocol: TCP
                - containerPort: 9300
                  name: transport
                  protocol: TCP
                volumeMounts:
                - name: storage
                  mountPath: /data
                resources:
                    limits:
                      cpu: 2000m
                      memory: 4Gi
                    requests:
                      cpu: 250m
                      memory: 500Mi
              volumes:
                - emptyDir:
                    medium: ""
                  name: "storage"
        ---
        # Source: databases-ephemeral/charts/cassandra-ephemeral/charts/cassandra-epheme
        ral/templates/statefulset.yaml
        apiVersion: apps/v1
        kind: StatefulSet
        metadata:
          name: cassandra-ephemeral
          labels:
            app: cassandra-ephemeral
            chart: cassandra-ephemeral-0.13.3
            release: databases-ephemeral
            heritage: Helm
        spec:
          selector:
            matchLabels:
              app: cassandra-ephemeral
              release: databases-ephemeral
          serviceName: cassandra-ephemeral
          replicas: 1
          podManagementPolicy: OrderedReady
          updateStrategy:
            type: RollingUpdate
          template:
            metadata:
              labels:
                app: cassandra-ephemeral
                release: databases-ephemeral
            spec:
              hostNetwork: false
              containers:
              - name: cassandra-ephemeral
                image: "cassandra:3.11.3"
                imagePullPolicy: "IfNotPresent"
                resources:
                  limits:
                    cpu: "4"
                    memory: 4.0Gi
                  requests:
                    cpu: "1"
                    memory: 2.0Gi
                env:
                - name: CASSANDRA_SEEDS
                  value: "cassandra-ephemeral-0.cassandra-ephemeral.default.svc.cluster.
        local"
                - name: MAX_HEAP_SIZE
                  value: "2048M"
                - name: HEAP_NEWSIZE
                  value: "1024M"
                - name: CASSANDRA_ENDPOINT_SNITCH
                  value: "SimpleSnitch"
                - name: CASSANDRA_CLUSTER_NAME
                  value: "cassandra"
                - name: CASSANDRA_DC
                  value: "DC1"
                - name: CASSANDRA_RACK
                  value: "RAC1"
                - name: CASSANDRA_START_RPC
                  value: "false"
                - name: POD_IP
                  valueFrom:
                    fieldRef:
                      fieldPath: status.podIP
                livenessProbe:
                  exec:
                    command: [ "/bin/sh", "-c", "nodetool status" ]
                  initialDelaySeconds: 90
                  periodSeconds: 30
                  timeoutSeconds: 5
                  successThreshold: 1
                  failureThreshold: 3
                readinessProbe:
                  exec:
                    command: [ "/bin/sh", "-c", "nodetool status | grep -E \"^UN\\s+${PO
        D_IP}\"" ]
                  initialDelaySeconds: 90
                  periodSeconds: 30
                  timeoutSeconds: 5
                  successThreshold: 1
                  failureThreshold: 3
                ports:
                - name: intra
                  containerPort: 7000
                - name: tls
                  containerPort: 7001
                - name: jmx
                  containerPort: 7199
                - name: cql
                  containerPort: 9042
                - name: thrift
                  containerPort: 9160
                volumeMounts:
                - name: data
                  mountPath: /var/lib/cassandra
                lifecycle:
                  preStop:
                    exec:
                      command: ["/bin/sh", "-c", "exec nodetool decommission"]
              terminationGracePeriodSeconds: 30
              volumes:
              - name: data
                emptyDir: {}
        ---
        # Source: databases-ephemeral/charts/redis-ephemeral/charts/redis-ephemeral/temp
        lates/redis-master-statefulset.yaml
        apiVersion: apps/v1
        kind: StatefulSet
        metadata:
          name: redis-ephemeral-master
          namespace: "default"
          labels:
            app: redis-ephemeral
            chart: redis-ephemeral-11.3.4
            release: databases-ephemeral
            heritage: Helm
        spec:
          selector:
            matchLabels:
              app: redis-ephemeral
              release: databases-ephemeral
              role: master
          serviceName: redis-ephemeral-headless
          template:
            metadata:
              labels:
                app: redis-ephemeral
                chart: redis-ephemeral-11.3.4
                release: databases-ephemeral
                role: master
              annotations:
                checksum/health: e8c90bae4ee68e3ea33f86992512385397556e9993eb7bbd6130358
        80121814b
                checksum/configmap: f3cb092ae316f13c0503c3bcf837885791c9f5365d6168ea83b3
        c45f40743f5a
                checksum/secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991
        b7852b855
            spec:
        
              securityContext:
                fsGroup: 1001
              serviceAccountName: default
              containers:
                - name: redis-ephemeral
                  image: docker.io/bitnami/redis:6.0.9-debian-10-r0
                  imagePullPolicy: "IfNotPresent"
                  securityContext:
                    runAsUser: 1001
                  command:
                    - /bin/bash
                    - -c
                    - /opt/bitnami/scripts/start-scripts/start-master.sh
                  env:
                    - name: REDIS_REPLICATION_MODE
                      value: master
                    - name: ALLOW_EMPTY_PASSWORD
                      value: "yes"
                    - name: REDIS_TLS_ENABLED
                      value: "no"
                    - name: REDIS_PORT
                      value: "6379"
                  ports:
                    - name: redis
                      containerPort: 6379
                  livenessProbe:
                    initialDelaySeconds: 5
                    periodSeconds: 5
                    # One second longer than command timeout should prevent generation o
        f zombie processes.
                    timeoutSeconds: 6
                    successThreshold: 1
                    failureThreshold: 5
                    exec:
                      command:
                        - sh
                        - -c
                        - /health/ping_liveness_local.sh 5
                  readinessProbe:
                    initialDelaySeconds: 5
                    periodSeconds: 5
                    timeoutSeconds: 2
                    successThreshold: 1
                    failureThreshold: 5
                    exec:
                      command:
                        - sh
                        - -c
                        - /health/ping_readiness_local.sh 1
                  resources:
                    limits:
                      cpu: 1000m
                      memory: 1024Mi
                    requests:
                      cpu: 500m
                      memory: 512Mi
                  volumeMounts:
                    - name: start-scripts
                      mountPath: /opt/bitnami/scripts/start-scripts
                    - name: health
                      mountPath: /health
                    - name: redis-data
                      mountPath: /data
                      subPath:
                    - name: config
                      mountPath: /opt/bitnami/redis/mounted-etc
                    - name: redis-tmp-conf
                      mountPath: /opt/bitnami/redis/etc/
              volumes:
                - name: start-scripts
                  configMap:
                    name: redis-ephemeral-scripts
                    defaultMode: 0755
                - name: health
                  configMap:
                    name: redis-ephemeral-health
                    defaultMode: 0755
                - name: config
                  configMap:
                    name: redis-ephemeral
                - name: "redis-data"
                  emptyDir: {}
                - name: redis-tmp-conf
                  emptyDir: {}
          updateStrategy:
            type: RollingUpdate
        
        NOTES:
        You now have an in-memory, non-persistent, non-highly-available set of databases
        :
        
        * cassandra-ephemeral
        * elasticsearch-ephemeral
        * redis-ephemeral
        
        !! WARNING WARNING !!
        This is fine for testing and demo purposes, but NOT for a production use case.
        !! WARNING WARNING !!
        
        Note that before use of these databases for wire-server components, an index (in
         the case of elasticsearch) and a set of cassandra-migrations (in the case of ca
        ssandra) have to be applied. This comes bundled with the wire-server chart (see
        cassandra-migrations and elasticsearch-index charts for details)
[14:14:19] # Running command «helm upgrade --install fake-aws wire/fake-aws --wait --debug» on 192.168.1.121
        bash-4.4# helm upgrade --install fake-aws wire/fake-aws --wait --debug
        history.go:56: [debug] getting history for release fake-aws
        upgrade.go:123: [debug] preparing upgrade for fake-aws
        upgrade.go:131: [debug] performing update for fake-aws
        upgrade.go:303: [debug] creating upgraded release for fake-aws
        client.go:201: [debug] checking 13 resources for changes
        client.go:464: [debug] Looks like there are no changes for ServiceAccount "fake-
        aws-s3-update-prometheus-secret"
        client.go:464: [debug] Looks like there are no changes for ServiceAccount "fake-
        aws-s3"
        client.go:464: [debug] Looks like there are no changes for Secret "fake-aws-s3"
        client.go:464: [debug] Looks like there are no changes for ConfigMap "fake-aws-s
        3"
        client.go:464: [debug] Looks like there are no changes for Role "fake-aws-s3-upd
        ate-prometheus-secret"
        client.go:464: [debug] Looks like there are no changes for RoleBinding "fake-aws
        -s3-update-prometheus-secret"
        client.go:464: [debug] Looks like there are no changes for Service "fake-aws-s3"
        client.go:464: [debug] Looks like there are no changes for Service "fake-aws-sns
        "
        client.go:464: [debug] Looks like there are no changes for Service "fake-aws-sqs
        "
        client.go:464: [debug] Looks like there are no changes for Deployment "fake-aws-
        s3-reaper"
        client.go:464: [debug] Looks like there are no changes for Deployment "fake-aws-
        sns"
        wait.go:53: [debug] beginning wait for 13 resources with timeout of 5m0s
        client.go:282: [debug] Starting delete for "fake-aws-s3-make-bucket-job" Job
        client.go:311: [debug] jobs.batch "fake-aws-s3-make-bucket-job" not found
        client.go:122: [debug] creating 1 resource(s)
        client.go:491: [debug] Watching for changes to Job fake-aws-s3-make-bucket-job w
        ith timeout of 5m0s
        client.go:519: [debug] Add/Modify event for fake-aws-s3-make-bucket-job: ADDED
        client.go:558: [debug] fake-aws-s3-make-bucket-job: Jobs active: 1, jobs failed:
         0, jobs succeeded: 0
        client.go:519: [debug] Add/Modify event for fake-aws-s3-make-bucket-job: MODIFIE
        D
        client.go:282: [debug] Starting delete for "fake-aws-s3-make-bucket-job" Job
        upgrade.go:138: [debug] updating status for upgraded release for fake-aws
        Release "fake-aws" has been upgraded. Happy Helming!
        NAME: fake-aws
        LAST DEPLOYED: Fri Jul  9 12:14:05 2021
        NAMESPACE: default
        STATUS: deployed
        REVISION: 3
        TEST SUITE: None
        USER-SUPPLIED VALUES:
        {}
        
        COMPUTED VALUES:
        fake-aws-s3:
          enabled: true
          global: {}
          minio:
            DeploymentUpdate:
              maxSurge: 100%
              maxUnavailable: 0
              type: RollingUpdate
            StatefulSetUpdate:
              updateStrategy: RollingUpdate
            accessKey: dummykey
            affinity: {}
            azuregateway:
              enabled: false
              replicas: 4
            bucketRoot: ""
            buckets:
            - name: dummy-bucket
              policy: none
              purge: true
            - name: assets
              policy: none
              purge: false
            - name: public
              policy: public
              purge: false
            certsPath: /etc/minio/certs/
            clusterDomain: cluster.local
            configPathmc: /etc/minio/mc/
            defaultBucket:
              enabled: false
              name: bucket
              policy: none
              purge: false
            drivesPerNode: 1
            environment:
              MINIO_BROWSER: "off"
            etcd:
              clientCert: ""
              clientCertKey: ""
              corednsPathPrefix: ""
              endpoints: []
              pathPrefix: ""
            existingSecret: ""
            extraArgs: []
            fullnameOverride: fake-aws-s3
            gcsgateway:
              enabled: false
              gcsKeyJson: ""
              projectId: ""
              replicas: 4
            global: {}
            helmKubectlJqImage:
              pullPolicy: IfNotPresent
              repository: bskim45/helm-kubectl-jq
              tag: 3.1.0
            image:
              pullPolicy: IfNotPresent
              repository: minio/minio
              tag: RELEASE.2020-10-18T21-54-12Z
            imagePullSecrets: []
            ingress:
              annotations: {}
              enabled: false
              hosts:
              - chart-example.local
              labels: {}
              path: /
              tls: []
            makeBucketJob:
              resources:
                requests:
                  memory: 128Mi
              securityContext:
                enabled: false
                fsGroup: 1000
                runAsGroup: 1000
                runAsUser: 1000
            mcImage:
              pullPolicy: IfNotPresent
              repository: minio/mc
              tag: RELEASE.2020-10-03T02-54-56Z
            metrics:
              serviceMonitor:
                additionalLabels: {}
                enabled: false
            mode: standalone
            mountPath: /export
            nameOverride: ""
            nasgateway:
              enabled: false
              replicas: 4
            networkPolicy:
              allowExternal: true
              enabled: false
            nodeSelector: {}
            persistence:
              VolumeName: ""
              accessMode: ReadWriteOnce
              enabled: false
              existingClaim: ""
              size: 500Gi
              storageClass: ""
              subPath: ""
            podAnnotations: {}
            podDisruptionBudget:
              enabled: false
              maxUnavailable: 1
            podLabels: {}
            priorityClassName: ""
            replicas: 4
            resources:
              requests:
                memory: 4Gi
            s3gateway:
              accessKey: ""
              enabled: false
              replicas: 4
              secretKey: ""
              serviceEndpoint: ""
            secretKey: dummysecret
            securityContext:
              enabled: true
              fsGroup: 1000
              runAsGroup: 1000
              runAsUser: 1000
            service:
              annotations: {}
              externalIPs: []
              nodePort: 32000
              port: 9000
              type: ClusterIP
            serviceAccount:
              create: true
            tls:
              certSecret: ""
              enabled: false
              privateKey: private.key
              publicCrt: public.crt
            tolerations: []
            trustedCertsSecret: ""
            updatePrometheusJob:
              securityContext:
                enabled: false
                fsGroup: 1000
                runAsGroup: 1000
                runAsUser: 1000
            zones: 1
        fake-aws-ses:
          enabled: false
          global: {}
          image:
            repository: localstack/localstack
            tag: 0.8.7
          resources:
            limits:
              cpu: 200m
              memory: 500Mi
            requests:
              cpu: 100m
              memory: 100Mi
          service:
            externalPort: 4569
            internalPort: 4579
        fake-aws-sns:
          applications:
          - credential: testkey
            name: integration-test
            platform: GCM
          - credential: testprivatekey
            name: integration-test
            platform: APNS_SANDBOX
          - credential: testprivatekey
            name: integration-com.wire.ent
            platform: APNS_SANDBOX
          enabled: true
          global: {}
          image:
            repository: localstack/localstack
            tag: 0.8.7
          resources:
            limits:
              cpu: 200m
              memory: 500Mi
            requests:
              cpu: 100m
              memory: 100Mi
          service:
            externalPort: 4575
            internalPort: 4575
        fake-aws-sqs:
          enabled: true
          global: {}
          image:
            repository: airdock/fake-sqs
            tag: 0.3.1
          queueNames:
          - integration-team-events.fifo
          - integration-brig-events
          - integration-brig-events-internal
          - integration-gundeck-events
          resources:
            limits:
              cpu: 1000m
              memory: 1000Mi
            requests:
              cpu: 100m
              memory: 256Mi
          service:
            httpPort: 4568
        
        HOOKS:
        ---
        # Source: fake-aws/charts/fake-aws-s3/charts/minio/templates/post-install-create
        -bucket-job.yaml
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: fake-aws-s3-make-bucket-job
          labels:
            app: minio-make-bucket-job
            chart: minio-8.0.3
            release: fake-aws
            heritage: Helm
          annotations:
            "helm.sh/hook": post-install,post-upgrade
            "helm.sh/hook-delete-policy": hook-succeeded,before-hook-creation
        spec:
          template:
            metadata:
              labels:
                app: minio-job
                release: fake-aws
            spec:
              restartPolicy: OnFailure
              volumes:
                - name: minio-configuration
                  projected:
                    sources:
                    - configMap:
                        name: fake-aws-s3
                    - secret:
                        name: fake-aws-s3
              serviceAccountName: "fake-aws-s3"
              containers:
              - name: minio-mc
                image: "minio/mc:RELEASE.2020-10-03T02-54-56Z"
                imagePullPolicy: IfNotPresent
                command: ["/bin/sh", "/config/initialize"]
                env:
                  - name: MINIO_ENDPOINT
                    value: fake-aws-s3
                  - name: MINIO_PORT
                    value: "9000"
                volumeMounts:
                  - name: minio-configuration
                    mountPath: /config
                resources:
                  requests:
                    memory: 128Mi
        MANIFEST:
        ---
        # Source: fake-aws/charts/fake-aws-s3/charts/minio/templates/post-install-promet
        heus-metrics-serviceaccount.yaml
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: fake-aws-s3-update-prometheus-secret
          labels:
            app: minio-update-prometheus-secret
            chart: minio-8.0.3
            release: fake-aws
            heritage: Helm
        ---
        # Source: fake-aws/charts/fake-aws-s3/charts/minio/templates/serviceaccount.yaml
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: "fake-aws-s3"
          namespace: "default"
          labels:
            app: minio
            chart: minio-8.0.3
            release: "fake-aws"
        ---
        # Source: fake-aws/charts/fake-aws-s3/charts/minio/templates/secrets.yaml
        apiVersion: v1
        kind: Secret
        metadata:
          name: fake-aws-s3
          labels:
            app: minio
            chart: minio-8.0.3
            release: fake-aws
            heritage: Helm
        type: Opaque
        data:
          accesskey: "ZHVtbXlrZXk="
          secretkey: "ZHVtbXlzZWNyZXQ="
        ---
        # Source: fake-aws/charts/fake-aws-s3/charts/minio/templates/configmap.yaml
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: fake-aws-s3
          labels:
            app: minio
            chart: minio-8.0.3
            release: fake-aws
            heritage: Helm
        data:
          initialize: |-
            #!/bin/sh
            set -e ; # Have script exit in the event of a failed command.
            MC_CONFIG_DIR="/etc/minio/mc/"
            MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"
        
            # connectToMinio
            # Use a check-sleep-check loop to wait for Minio service to be available
            connectToMinio() {
              SCHEME=$1
              ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
              set -e ; # fail if we can't read the keys.
              ACCESS=$(cat /config/accesskey) ; SECRET=$(cat /config/secretkey) ;
              set +e ; # The connections to minio are allowed to fail.
              echo "Connecting to Minio server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
              MC_COMMAND="${MC} config host add myminio $SCHEME://$MINIO_ENDPOINT:$MINIO
        _PORT $ACCESS $SECRET" ;
              $MC_COMMAND ;
              STATUS=$? ;
              until [ $STATUS = 0 ]
              do
                ATTEMPTS=`expr $ATTEMPTS + 1` ;
                echo \"Failed attempts: $ATTEMPTS\" ;
                if [ $ATTEMPTS -gt $LIMIT ]; then
                  exit 1 ;
                fi ;
                sleep 2 ; # 1 second intervals between attempts
                $MC_COMMAND ;
                STATUS=$? ;
              done ;
              set -e ; # reset `e` as active
              return 0
            }
        
            # checkBucketExists ($bucket)
            # Check if the bucket exists, by using the exit code of `mc ls`
            checkBucketExists() {
              BUCKET=$1
              CMD=$(${MC} ls myminio/$BUCKET > /dev/null 2>&1)
              return $?
            }
        
            # createBucket ($bucket, $policy, $purge)
            # Ensure bucket exists, purging if asked to
            createBucket() {
              BUCKET=$1
              POLICY=$2
              PURGE=$3
              VERSIONING=$4
        
              # Purge the bucket, if set & exists
              # Since PURGE is user input, check explicitly for `true`
              if [ $PURGE = true ]; then
                if checkBucketExists $BUCKET ; then
                  echo "Purging bucket '$BUCKET'."
                  set +e ; # don't exit if this fails
                  ${MC} rm -r --force myminio/$BUCKET
                  set -e ; # reset `e` as active
                else
                  echo "Bucket '$BUCKET' does not exist, skipping purge."
                fi
              fi
        
              # Create the bucket if it does not exist
              if ! checkBucketExists $BUCKET ; then
                echo "Creating bucket '$BUCKET'"
                ${MC} mb myminio/$BUCKET
              else
                echo "Bucket '$BUCKET' already exists."
              fi
        
        
              # set versioning for bucket
              if [ ! -z $VERSIONING ] ; then
                if [ $VERSIONING = true ] ; then
                    echo "Enabling versioning for '$BUCKET'"
                    ${MC} version enable myminio/$BUCKET
                elif [ $VERSIONING = false ] ; then
                    echo "Suspending versioning for '$BUCKET'"
                    ${MC} version suspend myminio/$BUCKET
                fi
              else
                  echo "Bucket '$BUCKET' versioning unchanged."
              fi
        
              # At this point, the bucket should exist, skip checking for existence
              # Set policy on the bucket
              echo "Setting policy of bucket '$BUCKET' to '$POLICY'."
              ${MC} policy set $POLICY myminio/$BUCKET
            }
        
            # Try connecting to Minio instance
            scheme=http
            connectToMinio $scheme
            # Create the buckets
            createBucket dummy-bucket none true
            createBucket assets none false
            createBucket public public false
        ---
        # Source: fake-aws/charts/fake-aws-s3/charts/minio/templates/post-install-promet
        heus-metrics-role.yaml
        apiVersion: rbac.authorization.k8s.io/v1
        kind: Role
        metadata:
          name: fake-aws-s3-update-prometheus-secret
          labels:
            app: minio-update-prometheus-secret
            chart: minio-8.0.3
            release: fake-aws
            heritage: Helm
        rules:
          - apiGroups:
              - ""
            resources:
              - secrets
            verbs:
              - get
              - create
              - update
              - patch
            resourceNames:
              - fake-aws-s3-prometheus
          - apiGroups:
              - ""
            resources:
              - secrets
            verbs:
              - create
          - apiGroups:
              - monitoring.coreos.com
            resources:
              - servicemonitors
            verbs:
              - get
            resourceNames:
              - fake-aws-s3
        ---
        # Source: fake-aws/charts/fake-aws-s3/charts/minio/templates/post-install-promet
        heus-metrics-rolebinding.yaml
        apiVersion: rbac.authorization.k8s.io/v1
        kind: RoleBinding
        metadata:
          name: fake-aws-s3-update-prometheus-secret
          labels:
            app: minio-update-prometheus-secret
            chart: minio-8.0.3
            release: fake-aws
            heritage: Helm
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: Role
          name: fake-aws-s3-update-prometheus-secret
        subjects:
          - kind: ServiceAccount
            name: fake-aws-s3-update-prometheus-secret
            namespace: "default"
        ---
        # Source: fake-aws/charts/fake-aws-s3/charts/minio/templates/service.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: fake-aws-s3
          labels:
            app: minio
            chart: minio-8.0.3
            release: fake-aws
            heritage: Helm
        spec:
          type: ClusterIP
          ports:
            - name: http
              port: 9000
              protocol: TCP
              targetPort: 9000
          selector:
            app: minio
            release: fake-aws
        ---
        # Source: fake-aws/charts/fake-aws-sns/templates/service.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: fake-aws-sns
          labels:
            app: fake-aws-sns
            chart: "fake-aws-sns-2.109.0"
            release: "fake-aws"
            heritage: "Helm"
        spec:
          type: ClusterIP
          selector:
            app: fake-aws-sns
          ports:
            - name: http
              port: 4575
              targetPort: 4575
              protocol: TCP
        ---
        # Source: fake-aws/charts/fake-aws-sqs/templates/service.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: fake-aws-sqs
          labels:
            app: fake-aws-sqs
            chart: "fake-aws-sqs-2.109.0"
            release: "fake-aws"
            heritage: "Helm"
        spec:
          type: ClusterIP
          selector:
            app: fake-aws-sqs
          ports:
            - name: http
              port: 4568
              targetPort: 4568
              protocol: TCP
        ---
        # Source: fake-aws/charts/fake-aws-s3/charts/minio/templates/deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: fake-aws-s3
          labels:
            app: minio
            chart: minio-8.0.3
            release: fake-aws
            heritage: Helm
        spec:
          strategy:
            type: RollingUpdate
            rollingUpdate:
              maxSurge: 100%
              maxUnavailable: 0
          selector:
            matchLabels:
              app: minio
              release: fake-aws
          template:
            metadata:
              name: fake-aws-s3
              labels:
                app: minio
                release: fake-aws
              annotations:
                checksum/secrets: e309d074ad8b38e7ca5627c8f7d956390f5af83e0d5a16c45aa561
        1fb2519f52
                checksum/config: cc64be52d6c6c9e047232e55d24877c2227a6b5a0c1f5fded18ef03
        75936a50b
            spec:
              serviceAccountName: "fake-aws-s3"
              containers:
                - name: minio
                  image: "minio/minio:RELEASE.2020-10-18T21-54-12Z"
                  imagePullPolicy: IfNotPresent
                  command:
                    - "/bin/sh"
                    - "-ce"
                    - "/usr/bin/docker-entrypoint.sh minio -S /etc/minio/certs/ server /
        export"
                  volumeMounts:
                  ports:
                    - name: http
                      containerPort: 9000
                  env:
                    - name: MINIO_ACCESS_KEY
                      valueFrom:
                        secretKeyRef:
                          name: fake-aws-s3
                          key: accesskey
                    - name: MINIO_SECRET_KEY
                      valueFrom:
                        secretKeyRef:
                          name: fake-aws-s3
                          key: secretkey
                    - name: MINIO_BROWSER
                      value: "off"
                  resources:
                    requests:
                      memory: 4Gi
              volumes:
                - name: export
                  emptyDir: {}
                - name: minio-user
                  secret:
                    secretName: fake-aws-s3
        ---
        # Source: fake-aws/charts/fake-aws-s3/templates/reaper.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: fake-aws-s3-reaper
          labels:
            app: fake-aws-s3-reaper
            chart: "fake-aws-s3-2.109.0"
            release: "fake-aws"
            heritage: "Helm"
        spec:
          replicas: 1
          selector:
            matchLabels:
              app: fake-aws-s3-reaper
          template:
            metadata:
              labels:
                app: fake-aws-s3-reaper
            spec:
              containers:
              - name: initiate-fake-aws-s3
                image: mesosphere/aws-cli:1.14.5
                command: [/bin/sh]
                args:
                - -c
                - |
                  echo 'Creating AWS resources'
                  while true
                  do
                      AWS_SECRET_ACCESS_KEY=dummysecret AWS_ACCESS_KEY_ID=dummykey aws s
        3 --endpoint http://fake-aws-s3:9000 mb s3://bucket | grep -ev "BucketAlreadyOwn
        edByYou"
                      sleep 10
                  done
        ---
        # Source: fake-aws/charts/fake-aws-sns/templates/deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: fake-aws-sns
          labels:
            app: fake-aws-sns
            chart: "fake-aws-sns-2.109.0"
            release: "fake-aws"
            heritage: "Helm"
        spec:
          replicas: 1
          selector:
            matchLabels:
              app: fake-aws-sns
          template:
            metadata:
              labels:
                app: fake-aws-sns
            spec:
              containers:
              - name: fake-aws-sns
                image: "localstack/localstack:0.8.7"
                env:
                  - name: DEBUG
                    value: "1"
                  - name: DEFAULT_REGION
                    value: "eu-west-1"
                  - name: SERVICES
                    value: "sns"
                ports:
                - containerPort: 4575
                  name: http
                  protocol: TCP
                volumeMounts:
                - name: storage
                  mountPath: /data
                resources:
                    limits:
                      cpu: 200m
                      memory: 500Mi
                    requests:
                      cpu: 100m
                      memory: 100Mi
              - name: initiate-fake-aws-sns
                image: mesosphere/aws-cli:1.14.5
                command: [/bin/sh]
                args:
                - -c
                - |
                  exec_until_ready() {
                      until $1; do echo 'service not ready yet'; sleep 1; done
                  }
                  application_exists() {
                      OUTPUT=$(aws --endpoint-url=http://localhost:4575 sns list-platfor
        m-applications | grep $1 | wc -l)
                      echo $OUTPUT
                  }
                  echo 'Creating AWS resources'
                  aws configure set aws_access_key_id dummy
                  aws configure set aws_secret_access_key dummy
                  aws configure set region eu-west-1
        
                  while true
                  do
        
                          APPLICATION=$(application_exists "GCM/integration-test")
                          if [ "$APPLICATION" == "1" ]
                            then echo "Application integration-test exists, no need to r
        e-create"
                            else exec_until_ready "aws --endpoint-url=http://localhost:4
        575 sns create-platform-application --name integration-test --platform GCM --att
        ributes PlatformCredential=testkey"
                          fi
        
                          APPLICATION=$(application_exists "APNS_SANDBOX/integration-tes
        t")
                          if [ "$APPLICATION" == "1" ]
                            then echo "Application integration-test exists, no need to r
        e-create"
                            else exec_until_ready "aws --endpoint-url=http://localhost:4
        575 sns create-platform-application --name integration-test --platform APNS_SAND
        BOX --attributes PlatformCredential=testprivatekey"
                          fi
        
                          APPLICATION=$(application_exists "APNS_SANDBOX/integration-com
        .wire.ent")
                          if [ "$APPLICATION" == "1" ]
                            then echo "Application integration-com.wire.ent exists, no n
        eed to re-create"
                            else exec_until_ready "aws --endpoint-url=http://localhost:4
        575 sns create-platform-application --name integration-com.wire.ent --platform A
        PNS_SANDBOX --attributes PlatformCredential=testprivatekey"
                          fi
        
                      echo "Resources created, sleeping for 10, to keep this container (
        and thus the pod) alive"
                      sleep 10
                  done
              volumes:
                - emptyDir: {}
                  name: "storage"
        ---
        # Source: fake-aws/charts/fake-aws-sqs/templates/deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: fake-aws-sqs
          labels:
            app: fake-aws-sqs
            chart: "fake-aws-sqs-2.109.0"
            release: "fake-aws"
            heritage: "Helm"
        spec:
          replicas: 1
          selector:
            matchLabels:
              app: fake-aws-sqs
          template:
            metadata:
              labels:
                app: fake-aws-sqs
            spec:
              containers:
              - name: fake-aws-sqs
                image: "airdock/fake-sqs:0.3.1"
                ports:
                - containerPort: 4568
                  name: http
                  protocol: TCP
                volumeMounts:
                - name: storage
                  mountPath: /data
                resources:
                    limits:
                      cpu: 1000m
                      memory: 1000Mi
                    requests:
                      cpu: 100m
                      memory: 256Mi
              - name: initiate-fake-aws-sqs
                image: mesosphere/aws-cli:1.14.5
                command: [/bin/sh]
                args:
                - -c
                - |
                  exec_until_ready() {
                      until $1; do echo 'service not ready yet'; sleep 1; done
                  }
                  queue_exists() {
                      # NOTE: we use the '"' to match the queue name more exactly (other
        wise there is some overlap)
                      OUTPUT=$(aws --endpoint-url=http://localhost:4568 sqs list-queues
        | grep $1'"' | wc -l)
                      echo $OUTPUT
                  }
        
                  echo 'Creating AWS resources'
                  aws configure set aws_access_key_id dummy
                  aws configure set aws_secret_access_key dummy
                  aws configure set region eu-west-1
        
                  while true
                  do
                      # Recreate resources if needed
        
                          QUEUE=$(queue_exists "integration-team-events.fifo")
                          if [ "$QUEUE" == "1" ]
                            then echo "Queue integration-team-events.fifo exists, no nee
        d to re-create"
                            else exec_until_ready "aws --endpoint-url=http://localhost:4
        568 sqs create-queue --queue-name integration-team-events.fifo"
                          fi
        
                          QUEUE=$(queue_exists "integration-brig-events")
                          if [ "$QUEUE" == "1" ]
                            then echo "Queue integration-brig-events exists, no need to
        re-create"
                            else exec_until_ready "aws --endpoint-url=http://localhost:4
        568 sqs create-queue --queue-name integration-brig-events"
                          fi
        
                          QUEUE=$(queue_exists "integration-brig-events-internal")
                          if [ "$QUEUE" == "1" ]
                            then echo "Queue integration-brig-events-internal exists, no
         need to re-create"
                            else exec_until_ready "aws --endpoint-url=http://localhost:4
        568 sqs create-queue --queue-name integration-brig-events-internal"
                          fi
        
                          QUEUE=$(queue_exists "integration-gundeck-events")
                          if [ "$QUEUE" == "1" ]
                            then echo "Queue integration-gundeck-events exists, no need
        to re-create"
                            else exec_until_ready "aws --endpoint-url=http://localhost:4
        568 sqs create-queue --queue-name integration-gundeck-events"
                          fi
        
        
                      echo 'Sleeping 10'
                      sleep 10
                  done
              volumes:
                - emptyDir: {}
                  name: "storage"
        
        NOTES:
        You can reach the fake AWS services at:
        SNS      : http://fake-aws-sns:4575
        SQS      : http://fake-aws-sqs:4568
          queues:
            - integration-team-events.fifo
            - integration-brig-events
            - integration-brig-events-internal
            - integration-gundeck-events
        S3       : http://fake-aws-s3:9000
          bucket: bucket
[14:14:26] # Running command «kubectl get pods -A» on 192.168.1.121
        bash-4.4# kubectl get pods -A
        NAMESPACE     NAME                                       READY   STATUS
            RESTARTS   AGE
        default       brig-59f54c5b65-snw95                      1/1     Running
            0          17h
        default       cannon-0                                   1/1     Running
            0          42h
        default       cargohold-546664f7df-ls47t                 1/1     Running
            0          42h
        default       cassandra-ephemeral-0                      1/1     Running
            0          42h
        default       cassandra-migrations-s2z75                 0/1     Completed
            0          17h
        default       demo-smtp-85557f6877-2rkrw                 1/1     Running
            0          42h
        default       elasticsearch-ephemeral-86f4b8ff6f-9z65d   1/1     Running
            0          42h
        default       elasticsearch-index-create-5xfnb           0/1     Completed
            0          17h
        default       fake-aws-s3-77d9447b8f-pxrkv               1/1     Running
            0          42h
        default       fake-aws-s3-reaper-78d9f58dd4-w4k46        1/1     Running
            0          42h
        default       fake-aws-sns-6c7c4b7479-htf9s              2/2     Running
            0          42h
        default       fake-aws-sqs-59fbfbcbd4-74586              2/2     Running
            0          42h
        default       galley-5b7b69cd75-plpg7                    1/1     Running
            0          42h
        default       gundeck-84cdfbc564-46k9j                   1/1     Running
            0          42h
        default       nginz-5485fbf785-8vmj2                     2/2     Running
            0          17h
        default       redis-ephemeral-master-0                   1/1     Running
            0          42h
        default       spar-5859c8f9d-6jkjk                       0/1     CrashLoopBackOf
        f   218        17h
        default       spar-5859c8f9d-ksgbl                       0/1     CrashLoopBackOf
        f   218        17h
        default       spar-5859c8f9d-t9nr4                       0/1     CrashLoopBackOf
        f   219        17h
        default       spar-5b7588fc4c-58ngk                      0/1     CrashLoopBackOf
        f   509        42h
        default       spar-5b7588fc4c-jxxsr                      0/1     CrashLoopBackOf
        f   509        42h
        default       spar-5b7588fc4c-k8fd4                      0/1     CrashLoopBackOf
        f   509        42h
        default       webapp-7678f9457-tt7vm                     1/1     Running
            0          42h
        kube-system   coredns-7677f9bb54-2m4gq                   0/1     Pending
            0          42h
        kube-system   coredns-7677f9bb54-h6vtx                   1/1     Running
            0          42h
        kube-system   dns-autoscaler-5b7b5c9b6f-4dqq7            1/1     Running
            0          42h
        kube-system   kube-apiserver-kubenode01                  1/1     Running
            0          42h
        kube-system   kube-controller-manager-kubenode01         1/1     Running
            0          42h
        kube-system   kube-flannel-vtqsv                         1/1     Running
            0          42h
        kube-system   kube-proxy-xvsjd                           1/1     Running
            0          3m59s
        kube-system   kube-scheduler-kubenode01                  1/1     Running
            0          42h
        kube-system   nodelocaldns-v6m7d                         1/1     Running
            0          42h
[14:14:51] # Running command «helm upgrade --install smtp wire/demo-smtp --wait --debug» on 192.168.1.121
        bash-4.4# helm upgrade --install smtp wire/demo-smtp --wait --debug
        history.go:56: [debug] getting history for release smtp
        upgrade.go:123: [debug] preparing upgrade for smtp
        upgrade.go:131: [debug] performing update for smtp
        upgrade.go:303: [debug] creating upgraded release for smtp
        client.go:201: [debug] checking 2 resources for changes
        wait.go:53: [debug] beginning wait for 2 resources with timeout of 5m0s
        upgrade.go:138: [debug] updating status for upgraded release for smtp
        Release "smtp" has been upgraded. Happy Helming!
        NAME: smtp
        LAST DEPLOYED: Fri Jul  9 12:14:42 2021
        NAMESPACE: default
        STATUS: deployed
        REVISION: 3
        TEST SUITE: None
        USER-SUPPLIED VALUES:
        {}
        
        COMPUTED VALUES:
        envVars: {}
        fullnameOverride: demo-smtp
        image: quay.io/wire/namshi-smtp:aa63b8
        replicaCount: 1
        resources:
          limits:
            cpu: 500m
            memory: 500Mi
          requests:
            cpu: 100m
            memory: 128Mi
        service:
          port: 25
        
        HOOKS:
        MANIFEST:
        ---
        # Source: demo-smtp/templates/service.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: demo-smtp
          labels:
            app: demo-smtp
            chart: demo-smtp-2.109.0
            release: smtp
            heritage: Helm
        spec:
          type:
          ports:
            - port: 25
              targetPort: smtp
              protocol: TCP
              name: smtp
          selector:
            app: demo-smtp
            release: smtp
        ---
        # Source: demo-smtp/templates/deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: demo-smtp
          labels:
            app: demo-smtp
            chart: demo-smtp-2.109.0
            release: smtp
            heritage: Helm
        spec:
          replicas: 1
          selector:
            matchLabels:
              app: demo-smtp
              release: smtp
          template:
            metadata:
              labels:
                app: demo-smtp
                release: smtp
            spec:
              containers:
                - name: demo-smtp
                  image: "quay.io/wire/namshi-smtp:aa63b8"
                  env:
                  ports:
                    - name: smtp
                      containerPort: 25
                      protocol: TCP
                  resources:
                    limits:
                      cpu: 500m
                      memory: 500Mi
                    requests:
                      cpu: 100m
                      memory: 128Mi
        
[14:14:59] # Running command «kubectl get pods -A» on 192.168.1.121
        bash-4.4# kubectl get pods -A
        NAMESPACE     NAME                                       READY   STATUS
            RESTARTS   AGE
        default       brig-59f54c5b65-snw95                      1/1     Running
            0          17h
        default       cannon-0                                   1/1     Running
            0          42h
        default       cargohold-546664f7df-ls47t                 1/1     Running
            0          42h
        default       cassandra-ephemeral-0                      1/1     Running
            0          42h
        default       cassandra-migrations-s2z75                 0/1     Completed
            0          17h
        default       demo-smtp-85557f6877-2rkrw                 1/1     Running
            0          42h
        default       elasticsearch-ephemeral-86f4b8ff6f-9z65d   1/1     Running
            0          42h
        default       elasticsearch-index-create-5xfnb           0/1     Completed
            0          17h
        default       fake-aws-s3-77d9447b8f-pxrkv               1/1     Running
            0          42h
        default       fake-aws-s3-reaper-78d9f58dd4-w4k46        1/1     Running
            0          42h
        default       fake-aws-sns-6c7c4b7479-htf9s              2/2     Running
            0          42h
        default       fake-aws-sqs-59fbfbcbd4-74586              2/2     Running
            0          42h
        default       galley-5b7b69cd75-plpg7                    1/1     Running
            0          42h
        default       gundeck-84cdfbc564-46k9j                   1/1     Running
            0          42h
        default       nginz-5485fbf785-8vmj2                     2/2     Running
            0          17h
        default       redis-ephemeral-master-0                   1/1     Running
            0          42h
        default       spar-5859c8f9d-6jkjk                       0/1     CrashLoopBackOf
        f   218        17h
        default       spar-5859c8f9d-ksgbl                       0/1     CrashLoopBackOf
        f   218        17h
        default       spar-5859c8f9d-t9nr4                       0/1     CrashLoopBackOf
        f   219        17h
        default       spar-5b7588fc4c-58ngk                      0/1     CrashLoopBackOf
        f   509        42h
        default       spar-5b7588fc4c-jxxsr                      0/1     CrashLoopBackOf
        f   509        42h
        default       spar-5b7588fc4c-k8fd4                      0/1     CrashLoopBackOf
        f   509        42h
        default       webapp-7678f9457-tt7vm                     1/1     Running
            0          42h
        kube-system   coredns-7677f9bb54-2m4gq                   0/1     Pending
            0          42h
        kube-system   coredns-7677f9bb54-h6vtx                   1/1     Running
            0          42h
        kube-system   dns-autoscaler-5b7b5c9b6f-4dqq7            1/1     Running
            0          42h
        kube-system   kube-apiserver-kubenode01                  1/1     Running
            0          42h
        kube-system   kube-controller-manager-kubenode01         1/1     Running
            0          42h
        kube-system   kube-flannel-vtqsv                         1/1     Running
            0          42h
        kube-system   kube-proxy-xvsjd                           1/1     Running
            0          4m31s
        kube-system   kube-scheduler-kubenode01                  1/1     Running
            0          42h
        kube-system   nodelocaldns-v6m7d                         1/1     Running
            0          42h
[14:15:06] # Running command «free -m && uptime» on 192.168.1.121
        bash-4.4# free -m && uptime
                      total        used        free      shared  buff/cache   available
        Mem:           2928         258         563           0        2107        2522
        Swap:          3943           0        3943
         12:15:03 up 27 min,  load average: 0.58, 0.74, 0.70
[14:15:14] # Running command «free -m && uptime» on 95.216.208.159
        wire@arthur-demo:~$ free -m && uptime
                      total        used        free      shared  buff/cache   available
        Mem:          15661        5563         431           4        9666       10145
        Swap:             0           0           0
         14:15:10 up 1 day, 19:05,  0 users,  load average: 0.94, 1.19, 1.22
[14:15:21] # Running command «ls -l» on 192.168.1.121
        bash-4.4# ls -l
        total 120
        -rw-rw-r--  1 1000 1000 2752 Jul  9 11:55 Makefile
        -rw-rw-r--  1 1000 1000 8198 Jul  9 11:55 README.md
        -rw-rw-r--  1 1000 1000  477 Jul  9 11:55 admin_users.yml
        -rw-rw-r--  1 1000 1000  382 Jul  9 11:55 ansible.cfg
        -rw-rw-r--  1 1000 1000   75 Jul  9 11:55 bootstrap.yml
        -rw-rw-r--  1 1000 1000  511 Jul  9 11:55 cassandra-verify-ntp.yml
        -rw-rw-r--  1 1000 1000  918 Jul  9 11:55 cassandra.yml
        -rw-rw-r--  1 1000 1000 3068 Jul  9 11:55 elasticsearch.yml
        drwxrwxr-x  3 1000 1000 4096 Jul  9 11:55 files
        -rw-rw-r--  1 1000 1000 1086 Jul  9 11:55 get-logs.yml
        -rw-rw-r--  1 1000 1000 1266 Jul  9 11:55 helm_external.yml
        drwxrwxr-x  3 1000 1000 4096 Jul  9 11:55 host_vars
        drwxrwxr-x  5 1000 1000 4096 Jul  9 12:11 inventory
        -rw-rw-r--  1 1000 1000  689 Jul  9 11:55 iptables.yml
        -rw-rw-r--  1 1000 1000 1762 Jul  9 11:55 kube-minio-static-files.yml
        -rw-rw-r--  1 1000 1000 1332 Jul  9 11:55 kubernetes.yml
        -rw-rw-r--  1 1000 1000  821 Jul  9 11:55 kubernetes_logging.yml
        -rw-rw-r--  1 1000 1000 2767 Jul  9 11:55 minio.yml
        -rw-rw-r--  1 1000 1000 1552 Jul  9 11:55 provision-sft.yml
        -rw-rw-r--  1 1000 1000 3241 Jul  9 11:55 registry.yml
        -rw-rw-r--  1 1000 1000  821 Jul  9 11:55 restund.yml
        drwxrwxr-x  4 1000 1000 4096 Jul  9 11:55 roles
        drwxrwxr-x 18 1000 1000 4096 Jul  9 11:55 roles-external
        -rw-rw-r--  1 1000 1000  947 Jul  9 11:55 seed-offline-docker.yml
        -rw-rw-r--  1 1000 1000 1928 Jul  9 11:55 setup-offline-sources.yml
        drwxrwxr-x  2 1000 1000 4096 Jul  9 11:55 tasks
        drwxrwxr-x  2 1000 1000 4096 Jul  9 11:55 templates
        -rw-rw-r--  1 1000 1000 1098 Jul  9 11:55 tinc.yml
[14:15:29] # Running command «pwd» on 192.168.1.121
        bash-4.4# pwd
        /wire-server-deploy/ansible
[14:15:37] # Running command «cd /wire-server-deploy» on 192.168.1.121
        bash-4.4# cd /wire-server-deploy
[14:15:45] # Running command «ls -l» on 192.168.1.121
        bash-4.4# ls -l
        total 112
        -rw-rw-r--  1 1000 1000 16072 Jul  9 11:55 CHANGELOG.md
        -rw-rw-r--  1 1000 1000  1531 Jul  9 11:55 CONTRIBUTING.md
        -rw-rw-r--  1 1000 1000   252 Jul  9 11:55 Dockerfile
        -rw-rw-r--  1 1000 1000 34520 Jul  9 11:55 LICENSE
        -rw-rw-r--  1 1000 1000  5893 Jul  9 11:55 Makefile
        -rw-rw-r--  1 1000 1000  2251 Jul  9 11:55 README.md
        drwxrwxr-x  9 1000 1000  4096 Jul  9 11:55 ansible
        drwxrwxr-x  3 1000 1000  4096 Jul  9 11:55 bin
        -rw-rw-r--  1 1000 1000  2338 Jul  9 11:55 default.nix
        drwxrwxr-x  5 1000 1000  4096 Jul  9 11:55 examples
        drwxrwxr-x  2 1000 1000  4096 Jul  9 11:55 helm
        drwxrwxr-x  4 1000 1000  4096 Jul  9 11:55 nix
        drwxrwxr-x  2 1000 1000  4096 Jul  9 11:55 offline
        drwxrwxr-x  5 1000 1000  4096 Jul  9 11:55 terraform
        drwxrwxr-x 15 1000 1000  4096 Jul  9 11:55 values
        drwxrwxr-x  2 1000 1000  4096 Jul  9 12:00 wire-server
[14:15:53] # Running command «pwd» on 192.168.1.121
        bash-4.4# pwd
        /wire-server-deploy
[14:16:01] # Running command «cd wire-server/» on 192.168.1.121
        bash-4.4# cd wire-server/
[14:16:09] # Running command «ls -l» on 192.168.1.121
        bash-4.4# ls -l
        total 44
        -rw-rw-r-- 1 1000 1000 12606 Jul  9 11:58 prod-values.yaml
        -rw-rw-r-- 1 1000 1000    42 Jul  9 11:57 restund.txt
        -rw-rw-r-- 1 1000 1000  2131 Jul  9 11:59 secrets.yaml
        -rw-rw-r-- 1 1000 1000   744 Jul  9 11:59 spar.yaml
        -rw-rw-r-- 1 1000 1000  8385 Jul  9 12:00 values.yaml
        -rw-rw-r-- 1 1000 1000   150 Jul  9 11:58 zauth.txt
[14:16:16] # Running command «pwd» on 192.168.1.121
        bash-4.4# pwd
        /wire-server-deploy/wire-server
[14:17:26] # Running command «helm upgrade --install wire-server wire/wire-server -f values.yaml -f secrets.yaml --wait --debug» on 192.168.1.121
        if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://galley;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
                    proxy_set_header   Authorization  "";
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location ~* ^/teams/([^/]*)/members/csv$ {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://galley;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
                    proxy_set_header   Authorization  "";
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location ~* ^/teams/([^/]*)/legalhold(.*) {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://galley;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
                    proxy_set_header   Authorization  "";
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location ~* ^/custom-backend/by-domain/([^/]*)$ {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
                    zauth off;
        
                    # If zauth is off, limit by remote address if not part of limit exem
        ptionslimit_req zone=reqs_per_addr burst=5 nodelay;
                    limit_conn conns_per_addr 20;
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://galley;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location ~* ^/teams/api-docs {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
                    zauth off;
        
                    # If zauth is off, limit by remote address if not part of limit exem
        ptionslimit_req zone=reqs_per_addr burst=5 nodelay;
                    limit_conn conns_per_addr 20;
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://galley;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location ~* ^/teams/([^/]*)/features {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://galley;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
                    proxy_set_header   Authorization  "";
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location ~* ^/teams/([^/]*)/features/([^/])* {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://galley;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
                    proxy_set_header   Authorization  "";
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /galley-api/swagger-ui {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
                    zauth off;
        
                    # If zauth is off, limit by remote address if not part of limit exem
        ptionslimit_req zone=reqs_per_addr burst=5 nodelay;
                    limit_conn conns_per_addr 20;
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://galley;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /push {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://gundeck;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
                    proxy_set_header   Authorization  "";
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /presences {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://gundeck;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
                    proxy_set_header   Authorization  "";
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /notifications {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://gundeck;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
                    proxy_set_header   Authorization  "";
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /billing {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
                    zauth off;
        
                    # If zauth is off, limit by remote address if not part of limit exem
        ptionslimit_req zone=reqs_per_addr burst=5 nodelay;
                    limit_conn conns_per_addr 20;
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://ibis;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location ~* ^/teams/([^/]*)/billing(.*) {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://ibis;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
                    proxy_set_header   Authorization  "";
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /proxy {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://proxy;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
                    proxy_set_header   Authorization  "";
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /identity-providers {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://spar;
                    proxy_http_version 1.1;
                    client_max_body_size 256k;
        
        
                    proxy_set_header   Connection     "";
        
                    proxy_set_header   Authorization  "";
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /sso-initiate-bind {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://spar;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
                    proxy_set_header   Authorization  "";
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /sso/initiate-login {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
                    zauth off;
        
                    # If zauth is off, limit by remote address if not part of limit exem
        ptionslimit_req zone=reqs_per_addr burst=5 nodelay;
                    limit_conn conns_per_addr 20;
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://spar;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Credentials: true';
        
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /sso/finalize-login {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
                    zauth off;
        
                    # If zauth is off, limit by remote address if not part of limit exem
        ptionslimit_req zone=reqs_per_addr burst=5 nodelay;
                    limit_conn conns_per_addr 20;
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://spar;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Credentials: true';
        
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /sso {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
                    zauth off;
        
                    # If zauth is off, limit by remote address if not part of limit exem
        ptionslimit_req zone=reqs_per_addr burst=5 nodelay;
                    limit_conn conns_per_addr 20;
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://spar;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /scim/v2 {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
                    zauth off;
        
                    # If zauth is off, limit by remote address if not part of limit exem
        ptionslimit_req zone=reqs_per_addr burst=5 nodelay;
                    limit_conn conns_per_addr 20;
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://spar;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Credentials: true';
        
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /scim {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://spar;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
                    proxy_set_header   Authorization  "";
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
        
        
                # Swagger UI
        
                location /swagger-ui {
                    zauth  off;
                    gzip   off;
                    alias /opt/zwagger-ui;
                    types {
                        application/javascript  js;
                        text/css                css;
                        text/html               html;
                        image/png               png;
                    }
                }
              }
            }
          upstreams.txt: |2
            brig calling-test cannon cargohold galley gundeck ibis proxy spar
          zwagger-config.js: |2
            var environment = 'prod';
          zauth.acl: |
            a (blacklist (path "/provider")
                         (path "/provider/**")
                         (path "/bot")
                         (path "/bot/**")
                         (path "/i/**"))
        
            b (whitelist (path "/bot")
                         (path "/bot/**"))
        
            p (whitelist (path "/provider")
                         (path "/provider/**"))
        
            # LegalHold Access Tokens
            la (whitelist (path "/notifications")
                          (path "/assets/v3/**")
                          (path "/users")
                          (path "/users/**"))
        kind: ConfigMap
        metadata:
          creationTimestamp: null
          name: nginz
        ---
        # Source: wire-server/charts/spar/templates/configmap.yaml
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: "spar"
        data:
          spar.yaml: |
            logNetStrings: True # log using netstrings encoding (see http://cr.yp.to/pro
        to/netstrings.txt)
            logLevel: Debug
        
            brig:
              host: brig
              port: 8080
        
            galley:
              host: galley
              port: 8080
        
            cassandra:
              endpoint:
                host: cassandra-ephemeral
                port: 9042
              keyspace: spar
        
            maxttlAuthreq: 28800
            maxttlAuthresp: 28800
        
            richInfoLimit: 5000
        
            maxScimTokens: 0
        
            saml:
              version:     SAML2.0
              logLevel:    Debug
        
              spHost: 0.0.0.0
              spPort: 8080
              spAppUri: https://nginz-https.example.com
              spSsoUri: https://nginz-https.example.com/sso
        
              contacts:
                    - company: YourCompany
                      email: email:support@example.com
                      type: ContactSupport
        ---
        # Source: wire-server/charts/spar/templates/tests/configmap.yaml
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: "spar-integration"
        data:
          integration.yaml: |
            brig:
              host: brig
              port: 8080
        
            galley:
              host: galley
              port: 8080
        
            spar:
              host: spar
              port: 8080
        
            # Keep this in sync with brigs setTeamInvitationTimeout
            brigSettingsTeamInvitationTimeout: 10
        ---
        # Source: wire-server/charts/brig/templates/service.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: brig
          labels:
            wireService: brig
            chart: brig-2.109.0
            release: wire-server
            heritage: Helm
        spec:
          type: ClusterIP
          ports:
            - name: http
              port: 8080
              targetPort: 8080
          selector:
            wireService: brig
            release: wire-server
        ---
        # Source: wire-server/charts/brig/templates/tests/brig-integration.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: "brig-integration"
          labels:
            wireService: brig-integration
            chart: brig-2.109.0
            release: wire-server
            heritage: Helm
        spec:
          type: ClusterIP
          ports:
            - port: 9000
              targetPort: 9000
          selector:
            wireService: brig-integration
            release: wire-server
        ---
        # Source: wire-server/charts/brig/templates/tests/nginz-service.yaml
        # this service is needed for brig integration tests and allows brig to talk dire
        ctly to nginz over http
        # (this is not how you should normally configure nginz - use an ingress instead)
        apiVersion: v1
        kind: Service
        metadata:
          name: nginz-integration-http
        spec:
          type: ClusterIP
          ports:
            - port: 8080
              targetPort: 8080
          selector:
            wireService: nginz
        ---
        # Source: wire-server/charts/cannon/templates/headless-service.yaml
        # Note, this is a Headless service https://kubernetes.io/docs/concepts/services-
        networking/service/#headless-services
        # We use it this way so we can handle routing requests to specific cannons direc
        tly rather than distributing requests
        # between pods.
        #
        # Read more about this technique in the StatefulSet guide:
        # https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/
        apiVersion: v1
        kind: Service
        metadata:
          name: cannon
          labels:
            wireService: cannon
            chart: cannon-2.109.0
            release: wire-server
            heritage: Helm
        spec:
          type: ClusterIP
          # This is what makes it a Headless Service
          clusterIP: None
          ports:
            - name: http
              port: 8080
              targetPort: 8080
              protocol: TCP
          selector:
            wireService: cannon
            release: wire-server
        ---
        # Source: wire-server/charts/cargohold/templates/service.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: cargohold
          labels:
            wireService: cargohold
            chart: cargohold-2.109.0
            release: wire-server
            heritage: Helm
        spec:
          type: ClusterIP
          ports:
            - name: http
              port: 8080
              targetPort: 8080
          selector:
            wireService: cargohold
            release: wire-server
        ---
        # Source: wire-server/charts/galley/templates/service.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: galley
          labels:
            wireService: galley
            chart: galley-2.109.0
            release: wire-server
            heritage: Helm
        spec:
          type: ClusterIP
          ports:
            - name: http
              port: 8080
              targetPort: 8080
          selector:
            wireService: galley
            release: wire-server
        ---
        # Source: wire-server/charts/galley/templates/tests/galley-integration.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: "galley-integration"
          labels:
            wireService: galley-integration
            chart: galley-2.109.0
            release: wire-server
            heritage: Helm
        spec:
          type: ClusterIP
          ports:
            - port: 9000
              targetPort: 9000
          selector:
            wireService: galley-integration
            release: wire-server
        ---
        # Source: wire-server/charts/gundeck/templates/service.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: gundeck
          labels:
            wireService: gundeck
            chart: gundeck-2.109.0
            release: wire-server
            heritage: Helm
        spec:
          type: ClusterIP
          ports:
            - name: http
              port: 8080
              targetPort: 8080
          selector:
            wireService: gundeck
            release: wire-server
        ---
        # Source: wire-server/charts/spar/templates/service.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: spar
          labels:
            wireService: spar
            chart: spar-2.109.0
            release: wire-server
            heritage: Helm
        spec:
          type: ClusterIP
          ports:
            - name: http
              port: 8080
              targetPort: 8080
          selector:
            wireService: spar
            release: wire-server
        ---
        # Source: wire-server/charts/brig/templates/deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: brig
          labels:
            wireService: brig
            chart: brig-2.109.0
            release: wire-server
            heritage: Helm
        spec:
          replicas: 1
          strategy:
            type: RollingUpdate
            rollingUpdate:
              maxUnavailable: 0
              maxSurge: 1
          selector:
            matchLabels:
              wireService: brig
          template:
            metadata:
              labels:
                wireService: brig
                release: wire-server
              annotations:
                # An annotation of the configmap checksum ensures changes to the configm
        ap cause a redeployment upon `helm upgrade`
                checksum/configmap: 20e7bef7daa2d5b4aea9cf465f216c54e6308437dffc6f1f235a
        99d5057b84aa
                checksum/turnconfigmap: 5a4d15b64169b4c9aaaf4290832094df87d44d3e81ba2341
        8084650969176a9f
                checksum/secret: e162a21bf56375513c82a12d0db496dea67637d6eb9ce919a01df55
        a3d5c01c0
                fluentbit.io/parser: json
            spec:
              volumes:
                - name: "brig-config"
                  configMap:
                    name: "brig"
                - name: "turn-servers"
                  configMap:
                    name: "turn"
                - name: "brig-secrets"
                  secret:
                    secretName: "brig"
              containers:
                - name: brig
                  image: "quay.io/wire/brig:2.109.0"
                  imagePullPolicy: ""
                  volumeMounts:
                  - name: "brig-secrets"
                    mountPath: "/etc/wire/brig/secrets"
                  - name: "brig-config"
                    mountPath: "/etc/wire/brig/conf"
                  - name: "turn-servers"
                    mountPath: "/etc/wire/brig/turn"
                  env:
                  - name: LOG_LEVEL
                    value: Info
                  - name: AWS_ACCESS_KEY_ID
                    valueFrom:
                      secretKeyRef:
                        name: brig
                        key: awsKeyId
                  - name: AWS_SECRET_ACCESS_KEY
                    valueFrom:
                      secretKeyRef:
                        name: brig
                        key: awsSecretKey
                  # TODO: Is this the best way to do this?
                  - name: AWS_REGION
                    value: "eu-west-1"
                  ports:
                    - containerPort: 8080
                  livenessProbe:
                    httpGet:
                      scheme: HTTP
                      path: /i/status
                      port: 8080
                  readinessProbe:
                    httpGet:
                      scheme: HTTP
                      path: /i/status
                      port: 8080
                  resources:
                    limits:
                      cpu: 500m
                      memory: 512Mi
                    requests:
                      cpu: 100m
                      memory: 256Mi
        ---
        # Source: wire-server/charts/cargohold/templates/deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: cargohold
          labels:
            wireService: cargohold
            chart: cargohold-2.109.0
            release: wire-server
            heritage: Helm
        spec:
          replicas: 1
          strategy:
            type: RollingUpdate
            rollingUpdate:
              maxUnavailable: 0
              maxSurge: 1
          selector:
            matchLabels:
              wireService: cargohold
          template:
            metadata:
              labels:
                wireService: cargohold
                release: wire-server
              annotations:
                # An annotation of the configmap checksum ensures changes to the configm
        ap cause a redeployment upon `helm upgrade`
                checksum/configmap: 3e3db07fdaf882e9641b8fb91433cc67587e451e77a5c60b0815
        bb46f540c2d7
                checksum/secret: f9eefaee4d92e25640a67fde0322d1dd8f6454815376203f6a5968a
        ed605bab7
            spec:
              volumes:
                - name: "cargohold-config"
                  configMap:
                    name: "cargohold"
                - name: "cargohold-secrets"
                  secret:
                    secretName: "cargohold"
              containers:
                - name: cargohold
                  image: "quay.io/wire/cargohold:2.109.0"
                  imagePullPolicy: ""
                  volumeMounts:
                  - name: "cargohold-secrets"
                    mountPath: "/etc/wire/cargohold/secrets"
                  - name: "cargohold-config"
                    mountPath: "/etc/wire/cargohold/conf"
                  env:
                  - name: AWS_ACCESS_KEY_ID
                    valueFrom:
                      secretKeyRef:
                        name: cargohold
                        key: awsKeyId
                  - name: AWS_SECRET_ACCESS_KEY
                    valueFrom:
                      secretKeyRef:
                        name: cargohold
                        key: awsSecretKey
                  - name: AWS_REGION
                    value: "eu-west-1"
                  ports:
                    - containerPort: 8080
                  livenessProbe:
                    httpGet:
                      scheme: HTTP
                      path: /i/status
                      port: 8080
                  readinessProbe:
                    httpGet:
                      scheme: HTTP
                      path: /i/status
                      port: 8080
                  resources:
                    limits:
                      cpu: 500m
                      memory: 512Mi
                    requests:
                      cpu: 100m
                      memory: 256Mi
        ---
        # Source: wire-server/charts/galley/templates/deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: galley
          labels:
            wireService: galley
            chart: galley-2.109.0
            release: wire-server
            heritage: Helm
        spec:
          replicas: 1
          strategy:
            type: RollingUpdate
            rollingUpdate:
              maxUnavailable: 0
              maxSurge: 1
          selector:
            matchLabels:
              wireService: galley
          template:
            metadata:
              labels:
                wireService: galley
                release: wire-server
              annotations:
                # An annotation of the configmap checksum ensures changes to the configm
        ap cause a redeployment upon `helm upgrade`
                checksum/configmap: 8386d4d6a5cdbc27dfe81d4c869d80f7ae1315b005e74aaf498d
        77b4dd319767
                checksum/secret: f8daf2096cc053ac3eabd39e25fac0f13a2978cbadeee109dd46265
        82e9e23d0
            spec:
              volumes:
                - name: "galley-config"
                  configMap:
                    name: "galley"
                - name: "galley-secrets"
                  secret:
                    secretName: "galley"
              containers:
                - name: galley
                  image: "quay.io/wire/galley:2.109.0"
                  imagePullPolicy: ""
                  volumeMounts:
                  - name: "galley-secrets"
                    mountPath: "/etc/wire/galley/secrets"
                  - name: "galley-config"
                    mountPath: "/etc/wire/galley/conf"
                  env:
                  - name: AWS_ACCESS_KEY_ID
                    valueFrom:
                      secretKeyRef:
                        name: galley
                        key: awsKeyId
                  - name: AWS_SECRET_ACCESS_KEY
                    valueFrom:
                      secretKeyRef:
                        name: galley
                        key: awsSecretKey
                  - name: AWS_REGION
                    value: "eu-west-1"
                  ports:
                    - containerPort: 8080
                  livenessProbe:
                    httpGet:
                      scheme: HTTP
                      path: /i/status
                      port: 8080
                  readinessProbe:
                    httpGet:
                      scheme: HTTP
                      path: /i/status
                      port: 8080
                  resources:
                    limits:
                      cpu: 500m
                      memory: 512Mi
                    requests:
                      cpu: 100m
                      memory: 256Mi
        ---
        # Source: wire-server/charts/gundeck/templates/deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: gundeck
          labels:
            wireService: gundeck
            chart: gundeck-2.109.0
            release: wire-server
            heritage: Helm
        spec:
          replicas: 1
          strategy:
            type: RollingUpdate
            rollingUpdate:
              maxUnavailable: 0
              maxSurge: 1
          selector:
            matchLabels:
              wireService: gundeck
          template:
            metadata:
              labels:
                wireService: gundeck
                release: wire-server
              annotations:
                # An annotation of the configmap checksum ensures changes to the configm
        ap cause a redeployment upon `helm upgrade`
                checksum/configmap: 47137e013b588bfff5d8eefb7ebe42908f270301567188993fe6
        666628da46e4
                checksum/secret: a0b0dbb263b24d72edb8521f6e3aa4ca3153cff3e7ae83d775f96da
        14091efde
            spec:
              volumes:
                - name: "gundeck-config"
                  configMap:
                    name: "gundeck"
                - name: "gundeck-secrets"
                  secret:
                    secretName: "gundeck"
              containers:
                - name: gundeck
                  image: "quay.io/wire/gundeck:2.109.0"
                  imagePullPolicy: ""
                  volumeMounts:
                  - name: "gundeck-secrets"
                    mountPath: "/etc/wire/gundeck/secrets"
                  - name: "gundeck-config"
                    mountPath: "/etc/wire/gundeck/conf"
                  env:
                  - name: AWS_ACCESS_KEY_ID
                    valueFrom:
                      secretKeyRef:
                        name: gundeck
                        key: awsKeyId
                  - name: AWS_SECRET_ACCESS_KEY
                    valueFrom:
                      secretKeyRef:
                        name: gundeck
                        key: awsSecretKey
                  - name: AWS_REGION
                    value: "eu-west-1"
                  ports:
                    - containerPort: 8080
                  livenessProbe:
                    httpGet:
                      scheme: HTTP
                      path: /i/status
                      port: 8080
                  readinessProbe:
                    httpGet:
                      scheme: HTTP
                      path: /i/status
                      port: 8080
                  resources:
                    limits:
                      cpu: 500m
                      memory: 512Mi
                    requests:
                      cpu: 100m
                      memory: 256Mi
        ---
        # Source: wire-server/charts/nginz/templates/deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: nginz
          labels:
            wireService: nginz
            chart: nginz-2.109.0
            release: wire-server
            heritage: Helm
        spec:
          replicas: 1
          strategy:
            type: RollingUpdate
            rollingUpdate:
              maxUnavailable: 0
              maxSurge: 2
          selector:
            matchLabels:
              wireService: nginz
              app: nginz
          template:
            metadata:
              labels:
                wireService: nginz
                app: nginz
                release: wire-server
              annotations:
                # An annotation of the configmap checksum ensures changes to the configm
        ap cause a redeployment upon `helm upgrade`
                checksum/configmap: a1ac15c2b551259e97d2124f2d446f0a54eecffd2765c55ffa4d
        272484868c5c
                checksum/secret: 909bee23222fd503625059557e8b41e2f68feab2d3277b7c2137aa1
        700b2021a
                fluentbit.io/parser-nginz: nginz
            spec:
              terminationGracePeriodSeconds: 30 # should be higher than the drainTimeout
         (sleep duration of preStop)
              containers:
              - name: nginz-disco
                image: "quay.io/wire/nginz_disco:2.109.0"
                volumeMounts:
                - name: config
                  mountPath: /etc/wire/nginz/conf
                  readOnly: true
                - name: upstreams
                  mountPath: /etc/wire/nginz/upstreams
                  readOnly: false
              - name: nginz
                image: "quay.io/wire/nginz:2.109.0"
                lifecycle:
                  preStop:
                    exec:
                      # kubernetes by default sends a SIGTERM to the container,
                      # which would cause nginz to exit, breaking existing websocket con
        nections.
                      # Instead we sleep for a day, then terminate gracefully.
                      # (SIGTERM is still sent, but afterwards)
                      command: ["sh", "-c", "sleep 10 && nginx -c /etc/wire/nginz/conf/n
        ginx.conf -s quit"]
                volumeMounts:
                - name: secrets
                  mountPath: /etc/wire/nginz/secrets
                  readOnly: true
                - name: config
                  mountPath: /etc/wire/nginz/conf
                  readOnly: true
                - name: upstreams
                  mountPath: /etc/wire/nginz/upstreams
                  readOnly: true
                ports:
                - name: http
                  containerPort: 8080
                - name: tcp
                  containerPort: 8081
                readinessProbe:
                  httpGet:
                    path: /status
                    port: 8080
                    scheme: HTTP
                livenessProbe:
                  initialDelaySeconds: 30
                  timeoutSeconds: 1
                  httpGet:
                    path: /status
                    port: 8080
                    scheme: HTTP
                resources:
                    limits:
                      cpu: "2"
                      memory: 1024Mi
                    requests:
                      cpu: 100m
                      memory: 256Mi
              dnsPolicy: ClusterFirst
              restartPolicy: Always
              volumes:
              - name: config
                configMap:
                  name: nginz
              - name: secrets
                secret:
                  secretName: nginz
              - name: upstreams
                emptyDir: {}
        ---
        # Source: wire-server/charts/spar/templates/deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: spar
          labels:
            wireService: spar
            chart: spar-2.109.0
            release: wire-server
            heritage: Helm
        spec:
          replicas: 3
          strategy:
            type: RollingUpdate
            rollingUpdate:
              maxUnavailable: 0
              maxSurge: 3
          selector:
            matchLabels:
              wireService: spar
          template:
            metadata:
              labels:
                wireService: spar
                release: wire-server
              annotations:
                # An annotation of the configmap checksum ensures changes to the configm
        ap cause a redeployment upon `helm upgrade`
                checksum/configmap: e229d62ed9bcac9bc60222d069e8fa42205cff138eb485eccf34
        f18e4450e473
            spec:
              volumes:
                - name: "spar-config"
                  configMap:
                    name: "spar"
              containers:
                - name: spar
                  image: "quay.io/wire/spar:2.109.0"
                  imagePullPolicy: ""
                  volumeMounts:
                  - name: "spar-config"
                    mountPath: "/etc/wire/spar/conf"
                  env:
                  ports:
                    - containerPort: 8080
                  livenessProbe:
                    httpGet:
                      scheme: HTTP
                      path: /i/status
                      port: 8080
                  readinessProbe:
                    httpGet:
                      scheme: HTTP
                      path: /i/status
                      port: 8080
                  resources:
                    limits:
                      cpu: 500m
                      memory: 512Mi
                    requests:
                      cpu: 100m
                      memory: 128Mi
        ---
        # Source: wire-server/charts/webapp/templates/deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: webapp
          labels:
            wireService: webapp
            chart: webapp-2.109.0
            release: wire-server
            heritage: Helm
        spec:
          replicas: 1
          strategy:
            type: RollingUpdate
            rollingUpdate:
              maxUnavailable: 0
              maxSurge: 2
          selector:
            matchLabels:
              wireService: webapp
              app: webapp
          template:
            metadata:
              labels:
                wireService: webapp
                app: webapp
                release: wire-server
            spec:
              containers:
              - name: webapp
                image: "quay.io/wire/webapp:2021-06-01-production.0-v0.28.15-0a4d64"
                # Check variables here: https://github.com/wireapp/wire-webapp/wiki/Self
        -hosting
                env:
                  # it is vital that you don't add trailing '/' in this section!
                  - name: NODE_PORT
                    value: "8080"
                  - name: APP_BASE
                    value: "https://webapp.example.com"
                  - name: BACKEND_REST
                    value: "https://nginz-https.example.com"
                  - name: BACKEND_WS
                    value: "wss://nginz-ssl.example.com"
                  - name: APP_NAME
                    value: "Webapp"
                  - name: CSP_EXTRA_CONNECT_SRC
                    value: "https://*.example.com, wss://*.example.com"
                  - name: CSP_EXTRA_DEFAULT_SRC
                    value: "https://*.example.com"
                  - name: CSP_EXTRA_FONT_SRC
                    value: "https://*.example.com"
                  - name: CSP_EXTRA_FRAME_SRC
                    value: "https://*.example.com"
                  - name: CSP_EXTRA_IMG_SRC
                    value: "https://*.example.com"
                  - name: CSP_EXTRA_MANIFEST_SRC
                    value: "https://*.example.com"
                  - name: CSP_EXTRA_MEDIA_SRC
                    value: "https://*.example.com"
                  - name: CSP_EXTRA_OBJECT_SRC
                    value: "https://*.example.com"
                  - name: CSP_EXTRA_PREFETCH_SRC
                    value: "https://*.example.com"
                  - name: CSP_EXTRA_SCRIPT_SRC
                    value: "https://*.example.com"
                  - name: CSP_EXTRA_STYLE_SRC
                    value: "https://*.example.com"
                  - name: CSP_EXTRA_WORKER_SRC
                    value: "https://*.example.com"
                  - name: ENFORCE_HTTPS
                    value: "false"
                  - name: FEATURE_CHECK_CONSENT
                    value: "false"
                  - name: FEATURE_ENABLE_ACCOUNT_REGISTRATION
                    value: "true"
                  - name: FEATURE_ENABLE_DEBUG
                    value: "false"
                  - name: FEATURE_ENABLE_PHONE_LOGIN
                    value: "false"
                  - name: FEATURE_ENABLE_SSO
                    value: "false"
                  - name: FEATURE_SHOW_LOADING_INFORMATION
                    value: "false"
                  - name: URL_ACCOUNT_BASE
                    value: "https://account.example.com"
                  - name: URL_PRIVACY_POLICY
                    value: "https://www.example.com/terms-conditions"
                  - name: URL_SUPPORT_BASE
                    value: "https://www.example.com/support"
                  - name: URL_TEAMS_BASE
                    value: "https://teams.example.com"
                  - name: URL_TEAMS_CREATE
                    value: "https://teams.example.com"
                  - name: URL_TERMS_OF_USE_PERSONAL
                    value: "https://www.example.com/terms-conditions"
                  - name: URL_TERMS_OF_USE_TEAMS
                    value: "https://www.example.com/terms-conditions"
                  - name: URL_WEBSITE_BASE
                    value: "https://www.example.com"
                ports:
                - name: http
                  containerPort: 8080
                # NOTE: /test/ returns an HTML document a 200 response code
                readinessProbe:
                  httpGet:
                    path: /_health/
                    port: 8080
                    scheme: HTTP
                livenessProbe:
                  initialDelaySeconds: 30
                  timeoutSeconds: 3
                  httpGet:
                    path: /_health/
                    port: 8080
                    scheme: HTTP
                resources:
                    limits:
                      cpu: "1"
                      memory: 512Mi
                    requests:
                      cpu: 100m
                      memory: 128Mi
              dnsPolicy: ClusterFirst
              restartPolicy: Always
        ---
        # Source: wire-server/charts/cannon/templates/statefulset.yaml
        # Spins up pods with stable names; e.g. cannon-0 ... cannon-<replicaCount>
        # Specific pods can be accessed within the cluster at cannon-<n>.cannon.<namespa
        ce>
        # (the second 'cannon' is the name of the headless service)
        # Note: In fact, cannon-<n>.cannon can also be used to access the service but as
        suming
        # that we can have multiple namespaces accessing the same redis cluster, appendi
        ng `.<namespace>`
        # makes the service unambiguous
        apiVersion: apps/v1
        kind: StatefulSet
        metadata:
          name: cannon
          labels:
            wireService: cannon
            chart: cannon-2.109.0
            release: wire-server
            heritage: Helm
        spec:
          serviceName: cannon
          selector:
            matchLabels:
              wireService: cannon
          replicas: 1
          updateStrategy:
            type: RollingUpdate
          podManagementPolicy: Parallel
          template:
            metadata:
              labels:
                wireService: cannon
                release: wire-server
              annotations:
                checksum/configmap: 7e09622a8605894e3e6f59d44d29f6ce45f3e165b1f6277f6e13
        fcc8facd20b3
            spec:
              terminationGracePeriodSeconds: 10 # should be higher than the sleep durati
        on of preStop
              containers:
              - name: cannon
                image: "quay.io/wire/cannon:2.109.0"
                lifecycle:
                  preStop:
                    # kubernetes by default immediately sends a SIGTERM to the container
        ,
                    # which would cause cannon to exit, breaking existing websocket conn
        ections.
                    # Instead we sleep for a day. (SIGTERM is still sent, but after the
        preStop completes)
                    exec:
                      command: ["sleep", "10" ]
                volumeMounts:
                - name: empty
                  mountPath: /etc/wire/cannon/externalHost
                - name: cannon-config
                  mountPath: /etc/wire/cannon/conf
                ports:
                - name: http
                  containerPort: 8080
                readinessProbe:
                  httpGet:
                    path: /i/status
                    port: 8080
                    scheme: HTTP
                livenessProbe:
                  initialDelaySeconds: 30
                  timeoutSeconds: 1
                  httpGet:
                    path: /i/status
                    port: 8080
                    scheme: HTTP
                resources:
                    limits:
                      cpu: 500m
                      memory: 512Mi
                    requests:
                      cpu: 100m
                      memory: 256Mi
              initContainers:
              - name: cannon-configurator
                image: alpine:3.13.1
                command:
                - /bin/sh
                args:
                - -c
                # e.g. cannon-0.cannon.production
                - echo "${HOSTNAME}.cannon.default" > /etc/wire/cannon/externalHost/host
        .txt
                volumeMounts:
                - name: empty
                  mountPath: /etc/wire/cannon/externalHost
              dnsPolicy: ClusterFirst
              restartPolicy: Always
              volumes:
              - name: cannon-config
                configMap:
                  name: cannon
              - name: empty
                emptyDir: {}
        
        NOTES:
        TODO: write nice NOTES.txt
