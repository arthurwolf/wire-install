[20:14:08] # Logging to: wire-install-2021-Jul-15-20-14-08.log in log/ and tmp/
[20:14:08] # Shutting down virtual machine "Wire Demo Client Clone"
[20:14:09] # Deleting virtual machine "Wire Demo Client Clone"
[20:14:10] # Creating virtual machine "Wire Demo Client Clone" from snapshot
[20:14:11] # Starting virtual machine "Wire Demo Client Clone"
[20:14:19] # Initial install of tmux on 192.168.1.121
[20:14:29] # Running command «sudo date --set="Thu Jul 15 2021 20:14:14 GMT+0200 (Central European Summer Time)"» on 192.168.1.121
        wire@wire-client:~$ sudo date --set="Thu Jul 15 2021 20:14:14 GMT+0200 (Central
        European Summer Time)"
        [sudo] password for wire:
        Thu Jul 15 18:14:14 UTC 2021
[20:14:35] # Running command «sudo date --set="Thu Jul 15 2021 20:14:29 GMT+0200 (Central European Summer Time)"» on 95.216.208.159
        wire@arthur-demo:~$ sudo date --set="Thu Jul 15 2021 20:14:29 GMT+0200 (Central
        European Summer Time)"
        Thu Jul 15 20:14:29 CEST 2021
[20:14:40] # Running command «cat /proc/cpuinfo | grep processor | wc -l» on 192.168.1.121
        wire@wire-client:~$ cat /proc/cpuinfo | grep processor | wc -l
        2
[20:14:46] # Running command «free -m && uptime» on 192.168.1.121
        wire@wire-client:~$ free -m && uptime
                      total        used        free      shared  buff/cache   available
        Mem:           2928         111        2615           0         201        2671
        Swap:          3943           0        3943
         18:14:31 up 1 min,  0 users,  load average: 0.77, 0.38, 0.15
[20:14:51] # Running command «cat /proc/cpuinfo | grep processor | wc -l» on 95.216.208.159
        wire@arthur-demo:~$ cat /proc/cpuinfo | grep processor | wc -l
        8
[20:14:56] # Running command «free -m && uptime» on 95.216.208.159
        wire@arthur-demo:~$ free -m && uptime
                      total        used        free      shared  buff/cache   available
        Mem:          15661        5609         664           4        9386       10151
        Swap:             0           0           0
         20:14:54 up 3 days,  9:52,  0 users,  load average: 0.84, 1.02, 1.17
[20:15:02] # Running command «cd» on 192.168.1.121
        wire@wire-client:~$ cd
[20:15:08] # Running command «cat /etc/hostname» on 192.168.1.121
        wire@wire-client:~$ cat /etc/hostname
        wire-client
[20:15:14] # Running command «pwd» on 192.168.1.121
        wire@wire-client:~$ pwd
        /home/wire
[20:15:19] # Running command «date» on 192.168.1.121
        wire@wire-client:~$ date
        Thu Jul 15 18:15:17 UTC 2021
[20:15:25] # Running command «uptime» on 192.168.1.121
        wire@wire-client:~$ uptime
         18:15:22 up 2 min,  0 users,  load average: 0.44, 0.35, 0.14
[20:15:47] # Running command «sudo apt update» on 192.168.1.121
        wire@wire-client:~$ sudo apt update
        [sudo] password for wire:
        Hit:1 http://fr.archive.ubuntu.com/ubuntu bionic InRelease
        Get:2 http://fr.archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]
        Get:3 http://fr.archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]
        Get:4 http://fr.archive.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]
        Get:5 http://fr.archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [21
        31 kB]
        Get:6 http://fr.archive.ubuntu.com/ubuntu bionic-updates/main Translation-en [42
        2 kB]
        Get:7 http://fr.archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packag
        es [389 kB]
        Get:8 http://fr.archive.ubuntu.com/ubuntu bionic-updates/restricted Translation-
        en [52.8 kB]
        Get:9 http://fr.archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages
         [1739 kB]
        Get:10 http://fr.archive.ubuntu.com/ubuntu bionic-updates/universe Translation-e
        n [371 kB]
        Get:11 http://fr.archive.ubuntu.com/ubuntu bionic-security/main amd64 Packages [
        1784 kB]
        Get:12 http://fr.archive.ubuntu.com/ubuntu bionic-security/main Translation-en [
        329 kB]
        Get:13 http://fr.archive.ubuntu.com/ubuntu bionic-security/restricted amd64 Pack
        ages [365 kB]
        Get:14 http://fr.archive.ubuntu.com/ubuntu bionic-security/restricted Translatio
        n-en [48.9 kB]
        Get:15 http://fr.archive.ubuntu.com/ubuntu bionic-security/universe amd64 Packag
        es [1130 kB]
        Get:16 http://fr.archive.ubuntu.com/ubuntu bionic-security/universe Translation-
        en [256 kB]
        Fetched 9270 kB in 3s (3118 kB/s)
        Reading package lists... Done
        Building dependency tree
        Reading state information... Done
        71 packages can be upgraded. Run 'apt list --upgradable' to see them.
[20:16:58] # Running command «sudo dpkg --configure -a» on 192.168.1.121
        wire@wire-client:~$ sudo dpkg --configure -a
        Setting up initramfs-tools (0.130ubuntu3.9) ...
        update-initramfs: deferring update (trigger activated)
        Processing triggers for initramfs-tools (0.130ubuntu3.9) ...
        update-initramfs: Generating /boot/initrd.img-4.15.0-144-generic
[20:18:27] # Running command «sudo apt install docker.io» on 192.168.1.121
        wire@wire-client:~$ sudo apt install docker.io
        Reading package lists... Done
        Building dependency tree
        Reading state information... Done
        The following additional packages will be installed:
          bridge-utils containerd pigz runc ubuntu-fan
        Suggested packages:
          ifupdown aufs-tools cgroupfs-mount | cgroup-lite debootstrap docker-doc
          rinse zfs-fuse | zfsutils
        The following NEW packages will be installed:
          bridge-utils containerd docker.io pigz runc ubuntu-fan
        0 upgraded, 6 newly installed, 0 to remove and 71 not upgraded.
        Need to get 72.1 MB of archives.
        After this operation, 351 MB of additional disk space will be used.
        Do you want to continue? [Y/n] y
        Get:1 http://fr.archive.ubuntu.com/ubuntu bionic/universe amd64 pigz amd64 2.4-1
         [57.4 kB]
        Get:2 http://fr.archive.ubuntu.com/ubuntu bionic/main amd64 bridge-utils amd64 1
        .5-15ubuntu1 [30.1 kB]
        Get:3 http://fr.archive.ubuntu.com/ubuntu bionic-updates/universe amd64 runc amd
        64 1.0.0~rc95-0ubuntu1~18.04.1 [4087 kB]
        Get:4 http://fr.archive.ubuntu.com/ubuntu bionic-updates/universe amd64 containe
        rd amd64 1.4.4-0ubuntu1~18.04.2 [31.0 MB]
        Get:5 http://fr.archive.ubuntu.com/ubuntu bionic-updates/universe amd64 docker.i
        o amd64 20.10.2-0ubuntu1~18.04.2 [36.9 MB]
        Get:6 http://fr.archive.ubuntu.com/ubuntu bionic/main amd64 ubuntu-fan all 0.12.
        10 [34.7 kB]
        Fetched 72.1 MB in 3s (24.1 MB/s)
        Preconfiguring packages ...
        Selecting previously unselected package pigz.
        (Reading database ... 67143 files and directories currently installed.)
        Preparing to unpack .../0-pigz_2.4-1_amd64.deb ...
        Unpacking pigz (2.4-1) ...
        Selecting previously unselected package bridge-utils.
        Preparing to unpack .../1-bridge-utils_1.5-15ubuntu1_amd64.deb ...
        Unpacking bridge-utils (1.5-15ubuntu1) ...
        Selecting previously unselected package runc.
        Preparing to unpack .../2-runc_1.0.0~rc95-0ubuntu1~18.04.1_amd64.deb ...
        Unpacking runc (1.0.0~rc95-0ubuntu1~18.04.1) ...
        Selecting previously unselected package containerd.
        Preparing to unpack .../3-containerd_1.4.4-0ubuntu1~18.04.2_amd64.deb ...
        Unpacking containerd (1.4.4-0ubuntu1~18.04.2) ...
        Selecting previously unselected package docker.io.
        Preparing to unpack .../4-docker.io_20.10.2-0ubuntu1~18.04.2_amd64.deb ...
        Unpacking docker.io (20.10.2-0ubuntu1~18.04.2) ...
        Selecting previously unselected package ubuntu-fan.
        Preparing to unpack .../5-ubuntu-fan_0.12.10_all.deb ...
        Unpacking ubuntu-fan (0.12.10) ...
        Setting up runc (1.0.0~rc95-0ubuntu1~18.04.1) ...
        Setting up containerd (1.4.4-0ubuntu1~18.04.2) ...
        Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service →
         /lib/systemd/system/containerd.service.
        Setting up bridge-utils (1.5-15ubuntu1) ...
        Setting up ubuntu-fan (0.12.10) ...
        Created symlink /etc/systemd/system/multi-user.target.wants/ubuntu-fan.service →
         /lib/systemd/system/ubuntu-fan.service.
        Setting up pigz (2.4-1) ...
        Setting up docker.io (20.10.2-0ubuntu1~18.04.2) ...
        Adding group `docker' (GID 113) ...
        Done.
        Created symlink /etc/systemd/system/multi-user.target.wants/docker.service → /li
        b/systemd/system/docker.service.
        Created symlink /etc/systemd/system/sockets.target.wants/docker.socket → /lib/sy
        stemd/system/docker.socket.
        Processing triggers for systemd (237-3ubuntu10.42) ...
        Processing triggers for man-db (2.8.3-2ubuntu0.1) ...
        Processing triggers for ureadahead (0.100.0-21) ...
[20:18:33] # Running command «docker -v» on 192.168.1.121
        wire@wire-client:~$ docker -v
        Docker version 20.10.2, build 20.10.2-0ubuntu1~18.04.2
[20:18:39] # Running command «sudo rm -rf /home/wire/wire*» on 192.168.1.121
        wire@wire-client:~$ sudo rm -rf /home/wire/wire*
[20:18:45] # Running command «ls -l /home/wire/» on 192.168.1.121
        wire@wire-client:~$ ls -l /home/wire/
        total 0
[20:18:51] # Running command «date» on 192.168.1.121
        wire@wire-client:~$ date
        Thu Jul 15 18:18:48 UTC 2021
[20:18:56] # Running command «uptime» on 192.168.1.121
        wire@wire-client:~$ uptime
         18:18:54 up 6 min,  0 users,  load average: 0.96, 0.76, 0.36
[20:19:04] # Running command «sudo apt update» on 192.168.1.121
        wire@wire-client:~$ sudo apt update
        Hit:1 http://fr.archive.ubuntu.com/ubuntu bionic InRelease
        Hit:2 http://fr.archive.ubuntu.com/ubuntu bionic-updates InRelease
        Hit:3 http://fr.archive.ubuntu.com/ubuntu bionic-backports InRelease
        Hit:4 http://fr.archive.ubuntu.com/ubuntu bionic-security InRelease
        Reading package lists... Done
        Building dependency tree
        Reading state information... Done
        71 packages can be upgraded. Run 'apt list --upgradable' to see them.
[20:19:10] # Running command «sudo dpkg --configure -a» on 192.168.1.121
        wire@wire-client:~$ sudo dpkg --configure -a
[20:19:15] # Running command «date» on 95.216.208.159
        wire@arthur-demo:~$ date
        Thu Jul 15 20:19:13 CEST 2021
[20:19:20] # Running command «uptime» on 95.216.208.159
        wire@arthur-demo:~$ uptime
         20:19:18 up 3 days,  9:56,  0 users,  load average: 0.96, 1.01, 1.13
[20:19:31] # Running command «sudo apt update» on 95.216.208.159
        wire@arthur-demo:~$ sudo apt update
        Hit:1 http://mirror.hetzner.de/ubuntu/packages bionic InRelease
        Get:2 http://mirror.hetzner.de/ubuntu/packages bionic-updates InRelease [88.7 kB
        ]
        Hit:3 https://download.docker.com/linux/ubuntu bionic InRelease
        Hit:4 https://mirror.hetzner.com/ubuntu/packages bionic InRelease
        Get:5 http://mirror.hetzner.de/ubuntu/packages bionic-backports InRelease [74.6
        kB]
        Get:6 https://mirror.hetzner.com/ubuntu/packages bionic-updates InRelease [88.7
        kB]
        Get:7 http://mirror.hetzner.de/ubuntu/packages bionic-security InRelease [88.7 k
        B]
        Get:8 https://mirror.hetzner.com/ubuntu/packages bionic-backports InRelease [74.
        6 kB]
        Get:9 https://mirror.hetzner.com/ubuntu/security bionic-security InRelease [88.7
         kB]
        Fetched 504 kB in 1s (357 kB/s)
        Reading package lists... Done
        Building dependency tree
        Reading state information... Done
        3 packages can be upgraded. Run 'apt list --upgradable' to see them.
[20:19:36] # Running command «sudo dpkg --configure -a» on 95.216.208.159
        wire@arthur-demo:~$ sudo dpkg --configure -a
[20:19:44] # Running command «sudo apt install docker.io» on 192.168.1.121
        wire@wire-client:~$ sudo apt install docker.io
        Reading package lists... Done
        Building dependency tree
        Reading state information... Done
        docker.io is already the newest version (20.10.2-0ubuntu1~18.04.2).
        0 upgraded, 0 newly installed, 0 to remove and 71 not upgraded.
[20:19:50] # Running command «docker -v» on 192.168.1.121
        wire@wire-client:~$ docker -v
        Docker version 20.10.2, build 20.10.2-0ubuntu1~18.04.2
[20:20:00] # Running command «git clone --branch master https://github.com/wireapp/wire-server-deploy.git» on 192.168.1.121
        wire@wire-client:~$ git clone --branch master https://github.com/wireapp/wire-se
        rver-deploy.git
        Cloning into 'wire-server-deploy'...
        remote: Enumerating objects: 18498, done.
        remote: Counting objects: 100% (331/331), done.
        remote: Compressing objects: 100% (210/210), done.
        remote: Total 18498 (delta 163), reused 232 (delta 115), pack-reused 18167
        Receiving objects: 100% (18498/18498), 2.19 MiB | 5.51 MiB/s, done.
        Resolving deltas: 100% (13819/13819), done.
[20:20:06] # Running command «ls -l» on 192.168.1.121
        wire@wire-client:~$ ls -l
        total 4
        drwxrwxr-x 12 wire wire 4096 Jul 15 18:19 wire-server-deploy
[20:20:12] # Running command «cd ~/wire-server-deploy» on 192.168.1.121
        wire@wire-client:~$ cd ~/wire-server-deploy
[20:20:18] # Running command «pwd» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy$ pwd
        /home/wire/wire-server-deploy
[20:20:23] # Running command «ls -l» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy$ ls -l
        total 108
        -rw-rw-r--  1 wire wire 16072 Jul 15 18:19 CHANGELOG.md
        -rw-rw-r--  1 wire wire  1531 Jul 15 18:19 CONTRIBUTING.md
        -rw-rw-r--  1 wire wire   252 Jul 15 18:19 Dockerfile
        -rw-rw-r--  1 wire wire 34520 Jul 15 18:19 LICENSE
        -rw-rw-r--  1 wire wire  5893 Jul 15 18:19 Makefile
        -rw-rw-r--  1 wire wire  2251 Jul 15 18:19 README.md
        drwxrwxr-x  9 wire wire  4096 Jul 15 18:19 ansible
        drwxrwxr-x  3 wire wire  4096 Jul 15 18:19 bin
        -rw-rw-r--  1 wire wire  2338 Jul 15 18:19 default.nix
        drwxrwxr-x  5 wire wire  4096 Jul 15 18:19 examples
        drwxrwxr-x  2 wire wire  4096 Jul 15 18:19 helm
        drwxrwxr-x  4 wire wire  4096 Jul 15 18:19 nix
        drwxrwxr-x  2 wire wire  4096 Jul 15 18:19 offline
        drwxrwxr-x  5 wire wire  4096 Jul 15 18:19 terraform
        drwxrwxr-x 15 wire wire  4096 Jul 15 18:19 values
[20:20:48] # Running command «git submodule update --init --recursive» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy$ git submodule update --init --recursive
        Submodule 'ansible/roles-external/ANXS.apt' (https://github.com/ANXS/apt.git) re
        gistered for path 'ansible/roles-external/ANXS.apt'
        Submodule 'ansible/roles-external/admin_users' (https://github.com/cchurch/ansib
        le-role-admin-users.git) registered for path 'ansible/roles-external/admin_users
        '
        Submodule 'ansible/roles-external/andrewrothstein.unarchive-deps' (https://githu
        b.com/andrewrothstein/ansible-unarchive-deps) registered for path 'ansible/roles
        -external/andrewrothstein.unarchive-deps'
        Submodule 'ansible/roles-external/ansible-cassandra' (https://github.com/wireapp
        /ansible-cassandra.git) registered for path 'ansible/roles-external/ansible-cass
        andra'
        Submodule 'ansible/roles-external/ansible-minio' (https://github.com/wireapp/ans
        ible-minio.git) registered for path 'ansible/roles-external/ansible-minio'
        Submodule 'ansible/roles-external/ansible-ntp-verify' (https://github.com/wireap
        p/ansible-ntp-verify.git) registered for path 'ansible/roles-external/ansible-nt
        p-verify'
        Submodule 'ansible/roles-external/ansible-restund' (https://github.com/wireapp/a
        nsible-restund.git) registered for path 'ansible/roles-external/ansible-restund'
        Submodule 'ansible/roles-external/ansible-role-java' (https://github.com/geerlin
        gguy/ansible-role-java.git) registered for path 'ansible/roles-external/ansible-
        role-java'
        Submodule 'ansible/roles-external/ansible-role-ntp' (https://github.com/geerling
        guy/ansible-role-ntp.git) registered for path 'ansible/roles-external/ansible-ro
        le-ntp'
        Submodule 'ansible/roles-external/ansible-tinc' (https://github.com/wireapp/ansi
        ble-tinc.git) registered for path 'ansible/roles-external/ansible-tinc'
        Submodule 'ansible/roles-external/cloudalchemy.node-exporter' (https://github.co
        m/cloudalchemy/ansible-node-exporter) registered for path 'ansible/roles-externa
        l/cloudalchemy.node-exporter'
        Submodule 'ansible/roles-external/elasticsearch' (https://github.com/elastic/ans
        ible-elasticsearch.git) registered for path 'ansible/roles-external/elasticsearc
        h'
        Submodule 'ansible/roles-external/hostname' (https://github.com/ANXS/hostname.gi
        t) registered for path 'ansible/roles-external/hostname'
        Submodule 'ansible/roles-external/kubespray' (https://github.com/kubernetes-sigs
        /kubespray.git) registered for path 'ansible/roles-external/kubespray'
        Submodule 'ansible/roles-external/logrotate' (https://github.com/nickhammond/ans
        ible-logrotate.git) registered for path 'ansible/roles-external/logrotate'
        Submodule 'ansible/roles-external/sft' (https://github.com/wireapp/ansible-sft.g
        it) registered for path 'ansible/roles-external/sft'
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/ANXS.apt'...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/admin_users'.
        ..
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/andrewrothste
        in.unarchive-deps'...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/ansible-cassa
        ndra'...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/ansible-minio
        '...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/ansible-ntp-v
        erify'...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/ansible-restu
        nd'...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/ansible-role-
        java'...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/ansible-role-
        ntp'...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/ansible-tinc'
        ...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/cloudalchemy.
        node-exporter'...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/elasticsearch
        '...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/hostname'...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/kubespray'...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/logrotate'...
        Cloning into '/home/wire/wire-server-deploy/ansible/roles-external/sft'...
        Submodule path 'ansible/roles-external/ANXS.apt': checked out 'f602ba7e88abfbb3a
        f6679a8ca47207dc3e9d9c4'
        Submodule path 'ansible/roles-external/admin_users': checked out 'd5bcef7e925ee1
        acf4e42359f0a95ed788eea58f'
        Submodule path 'ansible/roles-external/andrewrothstein.unarchive-deps': checked
        out '4485543262cfe04170d1ec02c8ccb95c44a7a222'
        Submodule path 'ansible/roles-external/ansible-cassandra': checked out '8a0f029f
        533856e8270d7fd74d75c099a677b2e3'
        Submodule path 'ansible/roles-external/ansible-minio': checked out '22ab28f75c00
        7a0c48dc47db574773773ef19d22'
        Submodule path 'ansible/roles-external/ansible-ntp-verify': checked out '4c3d0c6
        7d32d2d74444f4db45b2a4d2efdc7d590'
        Submodule path 'ansible/roles-external/ansible-restund': checked out '9807313a7c
        72ffa40e74f69d239404fd87db65ab'
        Submodule path 'ansible/roles-external/ansible-role-java': checked out '13b42705
        5702d9e91558cad7dcccab7db91e5663'
        Submodule path 'ansible/roles-external/ansible-role-ntp': checked out 'af1ec6238
        5c899a3e3f24407d8417adcdc9eea60'
        Submodule path 'ansible/roles-external/ansible-tinc': checked out '42951a951f638
        1e387174178bf3bff228b6a5dc5'
        Submodule path 'ansible/roles-external/cloudalchemy.node-exporter': checked out
        '8dc13ae077e3da1a71c268b114cd4fb8103ced80'
        Submodule path 'ansible/roles-external/elasticsearch': checked out '389a3ff45f8f
        51de95313ca0354cedcdc92b16f4'
        Submodule path 'ansible/roles-external/hostname': checked out 'da6f329b2984e84d2
        248d4251e0c679c53dfbb30'
        Submodule path 'ansible/roles-external/kubespray': checked out 'c7658c0256bfeb50
        913ac73f638a760680e4dd6d'
        Submodule path 'ansible/roles-external/logrotate': checked out '91d570f68c44261d
        2051a99a2b3c7d736306bf0d'
        Submodule path 'ansible/roles-external/sft': checked out '126fde847dfc9deffeef8b
        a133c541c16628a63a'
[20:20:54] # Running command «mkdir -p wire-server» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy$ mkdir -p wire-server
[20:21:00] # Running command «cd wire-server» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy$ cd wire-server
[20:21:06] # Running command «pwd» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ pwd
        /home/wire/wire-server-deploy/wire-server
[20:21:12] # Running command «curl -sSL https://raw.githubusercontent.com/wireapp/wire-server-deploy/master/values/wire-server/demo-secrets.example.yaml > secrets.yaml» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ curl -sSL https://raw.githubu
        sercontent.com/wireapp/wire-server-deploy/master/values/wire-server/demo-secrets
        .example.yaml > secrets.yaml
[20:21:18] # Running command «curl -sSL https://raw.githubusercontent.com/wireapp/wire-server-deploy/master/values/wire-server/demo-values.example.yaml > values.yaml» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ curl -sSL https://raw.githubu
        sercontent.com/wireapp/wire-server-deploy/master/values/wire-server/demo-values.
        example.yaml > values.yaml
[20:21:23] # Running command «openssl rand -base64 64 | env LC_CTYPE=C tr -dc a-zA-Z0-9 | head -c 42 > restund.txt» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ openssl rand -base64 64 | env
         LC_CTYPE=C tr -dc a-zA-Z0-9 | head -c 42 > restund.txt
[20:22:45] # Running command «sudo docker run --rm quay.io/wire/alpine-intermediate /dist/zauth -m gen-keypair -i 1 > zauth.txt» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ sudo docker run --rm quay.io/
        wire/alpine-intermediate /dist/zauth -m gen-keypair -i 1 > zauth.txt
        Unable to find image 'quay.io/wire/alpine-intermediate:latest' locally
        latest: Pulling from wire/alpine-intermediate
        188c0c94c7c5: Pulling fs layer
        edebdc4a6437: Pulling fs layer
        d9b6d5749552: Pulling fs layer
        0dd342b35df2: Pulling fs layer
        ea7c73d2e35b: Pulling fs layer
        0dd342b35df2: Waiting
        ea7c73d2e35b: Waiting
        edebdc4a6437: Download complete
        188c0c94c7c5: Verifying Checksum
        188c0c94c7c5: Download complete
        d9b6d5749552: Verifying Checksum
        d9b6d5749552: Download complete
        ea7c73d2e35b: Verifying Checksum
        ea7c73d2e35b: Download complete
        188c0c94c7c5: Pull complete
        edebdc4a6437: Pull complete
        d9b6d5749552: Pull complete
        0dd342b35df2: Verifying Checksum
        0dd342b35df2: Download complete
        0dd342b35df2: Pull complete
        ea7c73d2e35b: Pull complete
        Digest: sha256:bfb7dfd21e389ec699527f3cdad32d158ecab12577404c4578001af7c899d044
        Status: Downloaded newer image for quay.io/wire/alpine-intermediate:latest
[20:22:51] # Running command «ls -l» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ ls -l
        total 20
        -rw-rw-r-- 1 wire wire   42 Jul 15 18:21 restund.txt
        -rw-rw-r-- 1 wire wire 1949 Jul 15 18:21 secrets.yaml
        -rw-rw-r-- 1 wire wire 7642 Jul 15 18:21 values.yaml
        -rw-rw-r-- 1 wire wire  150 Jul 15 18:22 zauth.txt
[20:22:59] # Running command «curl -sSL https://raw.githubusercontent.com/wireapp/wire-server-deploy/master/values/wire-server/prod-values.example.yaml > prod-values.yaml» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ curl -sSL https://raw.githubu
        sercontent.com/wireapp/wire-server-deploy/master/values/wire-server/prod-values.
        example.yaml > prod-values.yaml
[20:23:05] # Running command «grep 'spar:' prod-values.yaml -A 24 > spar.yaml» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ grep 'spar:' prod-values.yaml
         -A 24 > spar.yaml
[20:23:11] # Running command «sed -i 's/cassandra-external/cassandra-ephemeral/' spar.yaml» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ sed -i 's/cassandra-external/
        cassandra-ephemeral/' spar.yaml
[20:23:17] # Running command «ls -l» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ ls -l
        total 40
        -rw-rw-r-- 1 wire wire 12606 Jul 15 18:22 prod-values.yaml
        -rw-rw-r-- 1 wire wire    42 Jul 15 18:21 restund.txt
        -rw-rw-r-- 1 wire wire  1949 Jul 15 18:21 secrets.yaml
        -rw-rw-r-- 1 wire wire   744 Jul 15 18:23 spar.yaml
        -rw-rw-r-- 1 wire wire  7642 Jul 15 18:21 values.yaml
        -rw-rw-r-- 1 wire wire   150 Jul 15 18:22 zauth.txt
[20:23:23] # Running command «cat restund.txt && echo» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ cat restund.txt && echo
        WGyGLRWkJg0JbfFojj4pz2gPHUGDeZnrkiKcsJss3t
[20:23:29] # Running command «cat zauth.txt» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ cat zauth.txt
        public: kXBYCngqtHX88ZFpx0oiM9wcbuuCZDqUBlmPuO9YBgM=
        secret: XRnqwKnREGlvksoNBETCcmnwGW2lg7RbjyJ_3h4sIdWRcFgKeCq0dfzxkWnHSiIz3Bxu64Jk
        OpQGWY-471gGAw==
[20:23:34] # Running command «sed -i 's/secret:$/secret: WGyGLRWkJg0JbfFojj4pz2gPHUGDeZnrkiKcsJss3t/' secrets.yaml» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ sed -i 's/secret:$/secret: WG
        yGLRWkJg0JbfFojj4pz2gPHUGDeZnrkiKcsJss3t/' secrets.yaml
[20:23:40] # Running command «sed -i 's/publicKeys: "<public key>"/publicKeys: "kXBYCngqtHX88ZFpx0oiM9wcbuuCZDqUBlmPuO9YBgM="/' secrets.yaml» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ sed -i 's/publicKeys: "<publi
        c key>"/publicKeys: "kXBYCngqtHX88ZFpx0oiM9wcbuuCZDqUBlmPuO9YBgM="/' secrets.yam
        l
[20:23:46] # Running command «sed -i 's/privateKeys: "<private key>"/privateKeys: "XRnqwKnREGlvksoNBETCcmnwGW2lg7RbjyJ_3h4sIdWRcFgKeCq0dfzxkWnHSiIz3Bxu64JkOpQGWY-471gGAw=="/' secrets.yaml» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ sed -i 's/privateKeys: "<priv
        ate key>"/privateKeys: "XRnqwKnREGlvksoNBETCcmnwGW2lg7RbjyJ_3h4sIdWRcFgKeCq0dfzx
        kWnHSiIz3Bxu64JkOpQGWY-471gGAw=="/' secrets.yaml
[20:23:52] # Running command «cat secrets.yaml | grep 'WGyGLRWkJg0JbfFojj4pz2gPHUGDeZnrkiKcsJss3t'» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ cat secrets.yaml | grep 'WGyG
        LRWkJg0JbfFojj4pz2gPHUGDeZnrkiKcsJss3t'
              secret: WGyGLRWkJg0JbfFojj4pz2gPHUGDeZnrkiKcsJss3t
[20:23:58] # Running command «cat secrets.yaml | grep 'kXBYCngqtHX88ZFpx0oiM9wcbuuCZDqUBlmPuO9YBgM='» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ cat secrets.yaml | grep 'kXBY
        CngqtHX88ZFpx0oiM9wcbuuCZDqUBlmPuO9YBgM='
              publicKeys: "kXBYCngqtHX88ZFpx0oiM9wcbuuCZDqUBlmPuO9YBgM="
              publicKeys: "kXBYCngqtHX88ZFpx0oiM9wcbuuCZDqUBlmPuO9YBgM="
[20:24:03] # Running command «cat secrets.yaml | grep 'XRnqwKnREGlvksoNBETCcmnwGW2lg7RbjyJ_3h4sIdWRcFgKeCq0dfzxkWnHSiIz3Bxu64JkOpQGWY-471gGAw=='» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ cat secrets.yaml | grep 'XRnq
        wKnREGlvksoNBETCcmnwGW2lg7RbjyJ_3h4sIdWRcFgKeCq0dfzxkWnHSiIz3Bxu64JkOpQGWY-471gG
        Aw=='
              privateKeys: "XRnqwKnREGlvksoNBETCcmnwGW2lg7RbjyJ_3h4sIdWRcFgKeCq0dfzxkWnH
        SiIz3Bxu64JkOpQGWY-471gGAw=="
[20:24:09] # Running command «cat values.yaml | grep spar» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ cat values.yaml | grep spar
          spar: false # enable if you want/need Single-Sign-On (SSO)
[20:24:15] # Running command «sed -i 's/spar: false/spar: true/' values.yaml» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ sed -i 's/spar: false/spar: t
        rue/' values.yaml
[20:24:21] # Running command «cat spar.yaml >> values.yaml» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ cat spar.yaml >> values.yaml
[20:24:27] # Running command «cat values.yaml | grep spar» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ cat values.yaml | grep spar
          spar: true # enable if you want/need Single-Sign-On (SSO)
        spar:
[20:24:33] # Running command «cd ~/wire-server-deploy» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ cd ~/wire-server-deploy
[20:24:39] # Running command «pwd» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy$ pwd
        /home/wire/wire-server-deploy
[20:24:44] # Running command «ls -l» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy$ ls -l
        total 112
        -rw-rw-r--  1 wire wire 16072 Jul 15 18:19 CHANGELOG.md
        -rw-rw-r--  1 wire wire  1531 Jul 15 18:19 CONTRIBUTING.md
        -rw-rw-r--  1 wire wire   252 Jul 15 18:19 Dockerfile
        -rw-rw-r--  1 wire wire 34520 Jul 15 18:19 LICENSE
        -rw-rw-r--  1 wire wire  5893 Jul 15 18:19 Makefile
        -rw-rw-r--  1 wire wire  2251 Jul 15 18:19 README.md
        drwxrwxr-x  9 wire wire  4096 Jul 15 18:19 ansible
        drwxrwxr-x  3 wire wire  4096 Jul 15 18:19 bin
        -rw-rw-r--  1 wire wire  2338 Jul 15 18:19 default.nix
        drwxrwxr-x  5 wire wire  4096 Jul 15 18:19 examples
        drwxrwxr-x  2 wire wire  4096 Jul 15 18:19 helm
        drwxrwxr-x  4 wire wire  4096 Jul 15 18:19 nix
        drwxrwxr-x  2 wire wire  4096 Jul 15 18:19 offline
        drwxrwxr-x  5 wire wire  4096 Jul 15 18:19 terraform
        drwxrwxr-x 15 wire wire  4096 Jul 15 18:19 values
        drwxrwxr-x  2 wire wire  4096 Jul 15 18:24 wire-server
[20:25:29] # Running command «curl -L https://nixos.org/nix/install | sh» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy$ curl -L https://nixos.org/nix/install | s
        h
          % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                         Dload  Upload   Total   Spent    Left  Speed
          0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
        100  2619  100  2619    0     0   3182      0 --:--:-- --:--:-- --:--:--  7960
        downloading Nix 2.3.14 binary tarball for x86_64-linux from 'https://releases.ni
        xos.org/nix/nix-2.3.14/nix-2.3.14-x86_64-linux.tar.xz' to '/tmp/nix-binary-tarba
        ll-unpack.wb0V09th6a'...
          % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                         Dload  Upload   Total   Spent    Left  Speed
        100 25.7M  100 25.7M    0     0  18.2M      0  0:00:01  0:00:01 --:--:-- 18.2M
        Note: a multi-user installation is possible. See https://nixos.org/nix/manual/#s
        ect-multi-user-installation
        performing a single-user installation of Nix...
        directory /nix does not exist; creating it by running 'mkdir -m 0755 /nix && cho
        wn wire /nix' using sudo
        copying Nix to /nix/store...
        
        installing 'nix-2.3.14'
        building '/nix/store/3jgh2s7zvrm4b4c00zqsirzy1wwic7bs-user-environment.drv'...
        created 6 symlinks in user environment
        unpacking channels...
        created 1 symlinks in user environment
        modifying /home/wire/.profile...
        
        Installation finished!  To ensure that the necessary environment
        variables are set, either log in again, or type
        
          . /home/wire/.nix-profile/etc/profile.d/nix.sh
        
        in your shell.
[20:25:40] # Running command «. /home/wire/.nix-profile/etc/profile.d/nix.sh» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy$ . /home/wire/.nix-profile/etc/profile.d/n
        ix.sh
[20:26:01] # Running command «sudo apt install direnv» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy$ sudo apt install direnv
        Reading package lists... Done
        Building dependency tree
        Reading state information... Done
        The following NEW packages will be installed:
          direnv
        0 upgraded, 1 newly installed, 0 to remove and 71 not upgraded.
        Need to get 750 kB of archives.
        After this operation, 2437 kB of additional disk space will be used.
        Get:1 http://fr.archive.ubuntu.com/ubuntu bionic/universe amd64 direnv amd64 2.1
        5.0-1 [750 kB]
        Fetched 750 kB in 0s (1820 kB/s)
        Selecting previously unselected package direnv.
        (Reading database ... 67462 files and directories currently installed.)
        Preparing to unpack .../direnv_2.15.0-1_amd64.deb ...
        Unpacking direnv (2.15.0-1) ...
        Setting up direnv (2.15.0-1) ...
        Processing triggers for man-db (2.8.3-2ubuntu0.1) ...
[20:26:07] # Running command «eval "$(direnv hook bash)"» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy$ eval "$(direnv hook bash)"
        direnv: error .envrc is blocked. Run `direnv allow` to approve its content.
[20:37:36] # Running command «direnv allow» on 192.168.1.121
        'https://cache.nixos.org'...
        copying path '/nix/store/yl69v76azrz4daiqksrhb8nnmdiqdjg9-python3-3.8.8' from 'h
        ttps://cache.nixos.org'...
        copying path '/nix/store/6vjmwq0nxsqm4skc8mrwif0zgy2gmn6r-stdenv-linux' from 'ht
        tps://cache.nixos.org'...
        copying path '/nix/store/8619ynafa0q6zn78kiawyhp1ssl6bn77-python3.8-MarkupSafe-1
        .1.1' from 'https://cache.nixos.org'...
        copying path '/nix/store/bpb9fnkf5kcmfxim2vcrnqr5pninznif-python3.8-PyYAML-5.4.1
        ' from 'https://cache.nixos.org'...
        copying path '/nix/store/nfqmmmy98870czalvv0zhbzj77ayl09v-python3.8-bcdoc-0.16.0
        ' from 'https://cache.nixos.org'...
        copying path '/nix/store/8437mm6b8041f34d22kghmd0dy4714l8-python3.8-certifi-2020
        .12.5' from 'https://cache.nixos.org'...
        copying path '/nix/store/d1w1vivmz2cqqgb74bbkcbrzyx68rv26-python3.8-chardet-3.0.
        4' from 'https://cache.nixos.org'...
        copying path '/nix/store/v28bassl74ilf1nd3h7srbasmcbxpnkw-python3.8-colorama-0.4
        .3' from 'https://cache.nixos.org'...
        copying path '/nix/store/ih58p5gpl1jgx4q90hy6qd8pyn8jjd45-python3.8-dicttoxml-1.
        7.4' from 'https://cache.nixos.org'...
        copying path '/nix/store/fzg09xw042c36r14vnb8cgzzh1ncdrgw-python3.8-dnspython-2.
        0.0' from 'https://cache.nixos.org'...
        copying path '/nix/store/w8w51lhbdnswyv0aniijkqmslks1qhq2-python3.8-docutils-0.1
        6' from 'https://cache.nixos.org'...
        copying path '/nix/store/2mzm9sfdgla2fxxywnj0lk8ghpzybmnb-python3.8-httplib2-0.1
        8.1' from 'https://cache.nixos.org'...
        copying path '/nix/store/skb8mfs4rlhgd9das5hrq2y71w9n4v3s-python3.8-idna-2.10' f
        rom 'https://cache.nixos.org'...
        copying path '/nix/store/68dmlsc6rzl7mxg4kzs0dmnvb3zfcz1a-python3.8-lxml-4.6.2'
        from 'https://cache.nixos.org'...
        copying path '/nix/store/p2hq4hp4yfgqhd6axzb7dqv85k7jhg7d-python3.8-netaddr-0.8.
        0' from 'https://cache.nixos.org'...
        copying path '/nix/store/77d44x14saj68flmzqrjj0b968c5fkcm-python3.8-ordereddict-
        1.1' from 'https://cache.nixos.org'...
        copying path '/nix/store/s8l0xiy297x7zszlpnchz6v2kpkgbia7-python3.8-ply-3.11' fr
        om 'https://cache.nixos.org'...
        copying path '/nix/store/dkzk05sbcr0l0i3ddmqs6420jk09k7dn-python3.8-ptyprocess-0
        .6.0' from 'https://cache.nixos.org'...
        copying path '/nix/store/w1n6iwyqqb3fl83jpcd5rs75fi2zjmzn-python3.8-jmespath-0.1
        0.0' from 'https://cache.nixos.org'...
        copying path '/nix/store/7pvr707i18pw6mhl916z7qi2g04z4ng4-python3.8-pexpect-4.8.
        0' from 'https://cache.nixos.org'...
        copying path '/nix/store/8bc1zgvk9sr3gr91ijzack7y0gl1ayk5-python3.8-pyasn1-0.4.8
        ' from 'https://cache.nixos.org'...
        copying path '/nix/store/0srilpn8g18smy1jsqm0fipyq809r5hl-python3.8-pycparser-2.
        20' from 'https://cache.nixos.org'...
        copying path '/nix/store/a4yz66dysqjhhb2mgcbi1p604jwsfjxy-python3.8-pycryptodome
        -3.9.9' from 'https://cache.nixos.org'...
        copying path '/nix/store/2r3ha7lqn2b7wxk4c5xrnhhfdai90hh7-python3.8-cffi-1.14.4-
        dev' from 'https://cache.nixos.org'...
        copying path '/nix/store/0iydv2525x3jri0arkvj7ihxdbfcgqlb-python3.8-pycrypto-3.9
        .9' from 'https://cache.nixos.org'...
        copying path '/nix/store/wirg14c5f05v3wbhgnib8m20a9hjl3x4-python3.8-pyparsing-2.
        4.7' from 'https://cache.nixos.org'...
        copying path '/nix/store/zhl23i0kbyai219azhirvq3lm6xn7fx0-python3.8-pysocks-1.7.
        1' from 'https://cache.nixos.org'...
        copying path '/nix/store/z15qv90y1hsg25xi6kjas4na03l8z966-python3.8-rsa-3.4.2' f
        rom 'https://cache.nixos.org'...
        copying path '/nix/store/awfrwyr8x0p3ln5mdd29vah854gna9pc-python3.8-selectors2-2
        .0.2' from 'https://cache.nixos.org'...
        copying path '/nix/store/z9rg0ff8qdmvg7v83fq0ixzp2vlpl3fl-python3.8-setuptools-5
        0.3.1' from 'https://cache.nixos.org'...
        copying path '/nix/store/zvqnirbkqcpcix2809j5rbw57kzv0420-python3.8-setuptools_s
        cm-4.1.2' from 'https://cache.nixos.org'...
        copying path '/nix/store/2p9ikvz5k1vbxqgaxvyi3v0j2c9vc8ga-python3.8-Jinja2-2.11.
        2' from 'https://cache.nixos.org'...
        copying path '/nix/store/rz63fw9z52wxy3glfvsjdn147a8m88sx-python3.8-simplejson-3
        .17.0' from 'https://cache.nixos.org'...
        copying path '/nix/store/hmmahv1vcp71w8f63iyra8dkgfqv1pxc-python3.8-six-1.15.0'
        from 'https://cache.nixos.org'...
        copying path '/nix/store/16cbd7gnb99mbh9mbl5clcq3qk87hf8c-python3.8-wcwidth-0.2.
        5' from 'https://cache.nixos.org'...
        copying path '/nix/store/ijqfsnkv2j0691icvfcd86y32ymmygk2-python3.8-bcrypt-3.2.0
        ' from 'https://cache.nixos.org'...
        copying path '/nix/store/r8w1nsgq0l8sd58gr0v0ayxd944z0630-python3.8-httpretty-1.
        0.3' from 'https://cache.nixos.org'...
        copying path '/nix/store/31x3gi5f8yabhyjxfj9y8bkzgnallh9m-python3.8-more-itertoo
        ls-8.6.0' from 'https://cache.nixos.org'...
        copying path '/nix/store/gbz6zd37j94pzk5n5m38y97ml0pahm59-python3.8-packaging-20
        .7' from 'https://cache.nixos.org'...
        copying path '/nix/store/20mbk331kg0ypd5b9jfvq2k1pkvn6fbc-python3.8-prettytable-
        2.0.0' from 'https://cache.nixos.org'...
        copying path '/nix/store/94gy4ifzj8fcmbjbvsdixh6qacppw0qn-python3.8-cryptography
        -3.4.6-dev' from 'https://cache.nixos.org'...
        copying path '/nix/store/cnb9j33alqj23dh9137slk8s2j1zhp8n-python3.8-pynacl-1.4.0
        ' from 'https://cache.nixos.org'...
        copying path '/nix/store/6bplql7f2sdqc309m0p86p0jjb64m18a-python3.8-pyOpenSSL-20
        .0.0-dev' from 'https://cache.nixos.org'...
        copying path '/nix/store/68ns6mx6yx20zi2wd7jlp5yzk9ll1mvi-python3.8-paramiko-2.7
        .2' from 'https://cache.nixos.org'...
        copying path '/nix/store/zr2zmzz73392lmxjnnva1dh0zfnwrric-python3.8-python-dateu
        til-2.8.1' from 'https://cache.nixos.org'...
        copying path '/nix/store/q759sf9ivsp92ixbkr67y39yk9di9y5a-python3.8-ncclient-0.6
        .9' from 'https://cache.nixos.org'...
        copying path '/nix/store/abp1gh00jgg04y1bg8nr1j95kc03i62a-python3.8-urllib3-1.26
        .2' from 'https://cache.nixos.org'...
        copying path '/nix/store/1mzplhlcjg4qg0scyxn9riwxlai1krg1-python3.8-xmltodict-0.
        12.0' from 'https://cache.nixos.org'...
        copying path '/nix/store/66c31w0k9r4ygf3qxih6rvpc4p35zndh-python3.8-botocore-1.2
        0.18' from 'https://cache.nixos.org'...
        copying path '/nix/store/jprs1kn7hvbv904zx2z0qdkm21qlssg1-python3.8-requests-2.2
        5.1' from 'https://cache.nixos.org'...
        copying path '/nix/store/8zibvzilcpgwqk437s5jxaq6k5iikwgh-python3.8-s3transfer-0
        .3.3' from 'https://cache.nixos.org'...
        copying path '/nix/store/yb4rxa3s7fdcpnrjb860057wfpbrz4z2-python3.8-boto-2.49.0'
         from 'https://cache.nixos.org'...
        copying path '/nix/store/s5aswmvyyqwdiavdq1hbig19ad6zmhd3-awscli-1.19.18' from '
        https://cache.nixos.org'...
        copying path '/nix/store/hs70rxb3ji7nnwpqhcwpgf3alhj68pr6-python3.8-boto3-1.17.1
        8' from 'https://cache.nixos.org'...
        copying path '/nix/store/3xazli888dgfks818a1bccf869wzc8w9-python3.8-dopy-2016-01
        -04' from 'https://cache.nixos.org'...
        copying path '/nix/store/935ccvz1gq2j4q69gh4by5n0wc6g99z1-python3.8-requests-too
        lbelt-0.9.1' from 'https://cache.nixos.org'...
        copying path '/nix/store/407c5v2x920kri79w0sb9gb1g82q28mg-python3.8-ansible-2.9.
        12' from 'https://cache.nixos.org'...
        copying path '/nix/store/2kiy29h83ph379jiyqab0wzjdh0mzi93-python3.8-zipp-3.4.0'
        from 'https://cache.nixos.org'...
        copying path '/nix/store/mby5cajr19n4akyga9pbjrcimmmr3z9s-util-linux-2.36.1-bin'
         from 'https://cache.nixos.org'...
        copying path '/nix/store/j4b0nzp2z8ps9zlsj6g0is5k4zy69zh5-python3.8-importlib-me
        tadata-1.7.0' from 'https://cache.nixos.org'...
        copying path '/nix/store/a5pcs43vdiv5r2rl8dsppc557pmq2s0d-fuse-3.10.2' from 'htt
        ps://cache.nixos.org'...
        copying path '/nix/store/saxxi6iwf2dda1lmfbcg7wxyiibc8y6f-python3.8-argcomplete-
        1.12.2' from 'https://cache.nixos.org'...
        copying path '/nix/store/798575gdggljb7c2vphm9w3gmgrizm99-fuse-overlayfs-1.4.0'
        from 'https://cache.nixos.org'...
        copying path '/nix/store/wp92ixig9x9ir1fcdkggnih0ff00a8qc-python3.8-yq-2.12.0' f
        rom 'https://cache.nixos.org'...
        copying path '/nix/store/vbyifyycrq5rl8jij32zpw4fryqlwhyn-systemd-minimal-247.3'
         from 'https://cache.nixos.org'...
        copying path '/nix/store/kfg661h6yxids2hx9yqxci0n5zp9c4vl-zlib-1.2.11-dev' from
        'https://cache.nixos.org'...
        copying path '/nix/store/wjljrsmgrvimqa205q2c2h8h22xcx137-libusb-1.0.24' from 'h
        ttps://cache.nixos.org'...
        copying path '/nix/store/fly6y2h058mp7d1rvfg07kpak3xrx6gg-curl-7.74.0-dev' from
        'https://cache.nixos.org'...
        copying path '/nix/store/ppshfkniccljxvzfpzbfnlgvkhg58g0w-hidapi-0.10.1' from 'h
        ttps://cache.nixos.org'...
        copying path '/nix/store/yz46zif5vbbk4zvi3w59vp5jvl7sflfw-lvm2-2.03.11-lib' from
         'https://cache.nixos.org'...
        copying path '/nix/store/51chih49v71ywy33n34vb8jgzlinx1zq-libfido2-1.5.0' from '
        https://cache.nixos.org'...
        copying path '/nix/store/b9r234wnx3a6snzb3gma244y0b7dmm4b-cryptsetup-2.3.4' from
         'https://cache.nixos.org'...
        copying path '/nix/store/yvqp7nay7r0j7dx381yvnyxn7ijilp60-openssh-8.4p1' from 'h
        ttps://cache.nixos.org'...
        copying path '/nix/store/4712fsjq670jsqjrpkkmpfzhi9g1k4nj-systemd-247.3' from 'h
        ttps://cache.nixos.org'...
        copying path '/nix/store/sav7kixag3gl1yfb224bhi02nlzy3nyb-git-2.30.1' from 'http
        s://cache.nixos.org'...
        copying path '/nix/store/8ln5lkk8w4dbss3wwqln660xzs1345dm-pcsclite-1.9.1-bin' fr
        om 'https://cache.nixos.org'...
        building '/nix/store/6svbgjf3qpdvchhvy1ifgvxb57lpaj9w-calicoctl-linux-amd64.drv'
        ...
        
        trying https://github.com/projectcalico/calicoctl/releases/download/v3.16.6/cali
        coctl-linux-amd64
          % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                         Dload  Upload   Total   Spent    Left  Speed
        100   627  100   627    0     0   1786      0 --:--:-- --:--:-- --:--:--  1791
        100 38.4M  100 38.4M    0     0  11.4M      0  0:00:03  0:00:03 --:--:-- 13.7M
        copying path '/nix/store/dpnqpw833nfmvg9ck5jphmi6jppxyqyh-pcsclite-1.9.1' from '
        https://cache.nixos.org'...
        building '/nix/store/n8lslj8l2sn8kawkginpc7y0cwd8ain8-cni-plugins-linux-amd64-v0
        .9.0.tgz.drv'...
        
        trying https://github.com/containernetworking/plugins/releases/download/v0.9.0/c
        ni-plugins-linux-amd64-v0.9.0.tgz
          % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                         Dload  Upload   Total   Spent    Left  Speed
        100   640  100   640    0     0   2126      0 --:--:-- --:--:-- --:--:--  2126
        100 37.8M  100 37.8M    0     0  13.1M      0  0:00:02  0:00:02 --:--:-- 16.8M
        copying path '/nix/store/gxcxs69x75rnc67223ijbw5js4vqihpg-gnupg-2.2.27' from 'ht
        tps://cache.nixos.org'...
        building '/nix/store/fx1s0yj8gf7rfnpvydphfyzwqnlmr8jn-elasticsearch-oss-6.6.0.de
        b.drv'...
        
        trying https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-6.
        6.0.deb
          % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                         Dload  Upload   Total   Spent    Left  Speed
        100 32.8M  100 32.8M    0     0  15.0M      0  0:00:02  0:00:02 --:--:-- 15.0M
        copying path '/nix/store/6wjp6gxwq3nhrpf7a3s9jrhih99ghy2j-aptly-1.4.0' from 'htt
        ps://cache.nixos.org'...
        copying path '/nix/store/gr4kx84a68zhf419sirff1m4k9qmrirv-gpgme-1.15.1' from 'ht
        tps://cache.nixos.org'...
        building '/nix/store/6f9bkgr4bx3i3sihhlnzl9w44p8l6l3h-etcd-v3.4.13-linux-amd64.t
        ar.gz.drv'...
        
        trying https://github.com/coreos/etcd/releases/download/v3.4.13/etcd-v3.4.13-lin
        ux-amd64.tar.gz
          % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                         Dload  Upload   Total   Spent    Left  Speed
        100   155  100   155    0     0    856      0 --:--:-- --:--:-- --:--:--   856
        100   637    0   637    0     0   1596      0 --:--:-- --:--:-- --:--:--  622k
        100 16.5M  100 16.5M    0     0  12.2M      0  0:00:01  0:00:01 --:--:-- 23.2M
        copying path '/nix/store/dz922iz0bzjxv4fcpig3g7pgdchlj1vv-skopeo-1.2.2' from 'ht
        tps://cache.nixos.org'...
        building '/nix/store/npw4xpr7h33jybmh4172hqvmqy9kjjcp-generate-gpg1-key.drv'...
        building '/nix/store/22bkj9mf6i8j33m9nsnki9q2nh9mlc0n-create-container-dump.drv'
        ...
        building '/nix/store/ydqjazyp565fq959czywfrq8mfdvsllx-jmx_prometheus_javaagent-0
        .10.jar.drv'...
        
        trying https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent
        /0.10/jmx_prometheus_javaagent-0.10.jar
          % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                         Dload  Upload   Total   Spent    Left  Speed
        100  348k  100  348k    0     0  2166k      0 --:--:-- --:--:-- --:--:-- 2152k
        building '/nix/store/p2rxpfcmq6q311krc0swsimjjzpiby9q-kubeadm.drv'...
        
        trying https://storage.googleapis.com/kubernetes-release/release/v1.19.7/bin/lin
        ux/amd64/kubeadm
          % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                         Dload  Upload   Total   Spent    Left  Speed
        100 37.2M  100 37.2M    0     0  14.2M      0  0:00:02  0:00:02 --:--:-- 14.2M
        building '/nix/store/cqcq1gwfl857h36lh2cin1hvrinq95fd-kubectl.drv'...
        
        trying https://storage.googleapis.com/kubernetes-release/release/v1.19.7/bin/lin
        ux/amd64/kubectl
          % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                         Dload  Upload   Total   Spent    Left  Speed
        100 40.9M  100 40.9M    0     0  15.5M      0  0:00:02  0:00:02 --:--:-- 15.5M
        building '/nix/store/bjxa3nrmkr4wcrw9vzq1jkaaavy48d7k-kubelet.drv'...
        
        trying https://storage.googleapis.com/kubernetes-release/release/v1.19.7/bin/lin
        ux/amd64/kubelet
          % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                         Dload  Upload   Total   Spent    Left  Speed
        100  104M  100  104M    0     0  22.4M      0  0:00:04  0:00:04 --:--:-- 22.4M
        building '/nix/store/2llcj18qq7pay3qriawm5r74fzkc52b0-kubectl-1.19.7.drv'...
        patching sources
        configuring
        no configure script, doing nothing
        installing
        post-installation fixup
        shrinking RPATHs of ELF executables and libraries in /nix/store/pz9c5mxqshlwfv97
        n3q9jcsqzigqhb26-kubectl-1.19.7
        shrinking /nix/store/pz9c5mxqshlwfv97n3q9jcsqzigqhb26-kubectl-1.19.7/bin/kubectl
        patchelf: cannot find section '.dynamic'. The input file is most likely statical
        ly linked
        strip is /nix/store/cp1sa3xxvl71cypiinw2c62i5s33chlr-binutils-2.35.1/bin/strip
        stripping (with command strip and flags -S) in /nix/store/pz9c5mxqshlwfv97n3q9jc
        sqzigqhb26-kubectl-1.19.7/bin
        patching script interpreter paths in /nix/store/pz9c5mxqshlwfv97n3q9jcsqzigqhb26
        -kubectl-1.19.7
        checking for references to /build/ in /nix/store/pz9c5mxqshlwfv97n3q9jcsqzigqhb2
        6-kubectl-1.19.7...
        patchelf: cannot find section '.dynamic'. The input file is most likely statical
        ly linked
        building '/nix/store/vslcwcv6yk21lfh04ydlmikkgq82bdnj-mc.RELEASE.2020-10-03T02-5
        4-56Z.drv'...
        
        trying https://dl.min.io/client/mc/release/linux-amd64/archive/mc.RELEASE.2020-1
        0-03T02-54-56Z
          % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                         Dload  Upload   Total   Spent    Left  Speed
        100 20.2M  100 20.2M    0     0  2182k      0  0:00:09  0:00:09 --:--:-- 3722k
        building '/nix/store/8vqnwpx33z8chw26jr4i7paprf220kh7-minio.RELEASE.2020-10-28T0
        8-16-50Z.drv'...
        
        trying https://dl.min.io/server/minio/release/linux-amd64/archive/minio.RELEASE.
        2020-10-28T08-16-50Z
          % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                         Dload  Upload   Total   Spent    Left  Speed
        100 46.0M  100 46.0M    0     0  1677k      0  0:00:28  0:00:28 --:--:-- 1455k
        building '/nix/store/nv97s8hnl0w79s210k7xxmj9vji64bpq-mirror-apt.drv'...
        building '/nix/store/6s5dgg65hkvlq6pgackrxq35r9sy7wmn-profile-env.drv'...
        building '/nix/store/p94nkkqm3brh9fpaw62gl2zlg3bcg13q-python3-3.8.8-env.drv'...
        created 375 symlinks in user environment
        building '/nix/store/3dlmm20yvqn81vzllpjdb8mx2l7g7b6g-source.drv'...
        
        trying https://github.com/hypnoglow/helm-s3/archive/v0.10.0.tar.gz
          % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                         Dload  Upload   Total   Spent    Left  Speed
        100   126  100   126    0     0    531      0 --:--:-- --:--:-- --:--:--   531
        100 76680    0 76680    0     0   139k      0 --:--:-- --:--:-- --:--:--  139k
        unpacking source archive /build/v0.10.0.tar.gz
        building '/nix/store/fx6f8rwj31nj7yxkwhyv96277ap1ma94-source.drv'...
        
        trying https://github.com/databus23/helm-diff/archive/v3.1.3.tar.gz
          % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                         Dload  Upload   Total   Spent    Left  Speed
        100   127  100   127    0     0    470      0 --:--:-- --:--:-- --:--:--   470
        100 44177    0 44177    0     0  37629      0 --:--:--  0:00:01 --:--:--  737k
        unpacking source archive /build/v3.1.3.tar.gz
        building '/nix/store/gbj77qg90x9jk6idgldwc9zhb9p7ivji-helm-s3-0.10.0-go-modules.
        drv'...
        unpacking sources
        unpacking source archive /nix/store/d4z49ksly0059sdw2dlshr6dci6mqff6-source
        source root is source
        patching sources
        configuring
        building
        go: downloading github.com/aws/aws-sdk-go v1.27.0
        go: downloading github.com/pkg/errors v0.9.1
        go: downloading gopkg.in/alecthomas/kingpin.v2 v2.2.6
        go: downloading github.com/google/go-cmp v0.4.0
        go: downloading github.com/minio/minio-go/v6 v6.0.40
        go: downloading helm.sh/helm/v3 v3.4.0
        go: downloading github.com/Masterminds/semver v1.5.0
        go: downloading github.com/Masterminds/semver/v3 v3.1.0
        go: downloading github.com/ghodss/yaml v1.0.0
        go: downloading k8s.io/helm v2.17.0+incompatible
        go: downloading sigs.k8s.io/yaml v1.2.0
        go: downloading github.com/stretchr/testify v1.6.1
        go: downloading github.com/alecthomas/template v0.0.0-20190718012654-fb15b899a75
        1
        go: downloading github.com/alecthomas/units v0.0.0-20190717042225-c3de453c63f4
        go: downloading github.com/minio/sha256-simd v0.1.1
        go: downloading golang.org/x/net v0.0.0-20200707034311-ab3426394381
        go: downloading gopkg.in/yaml.v2 v2.2.8
        go: downloading github.com/spf13/pflag v1.0.5
        go: downloading k8s.io/cli-runtime v0.19.2
        go: downloading golang.org/x/crypto v0.0.0-20200622213623-75b288015ac9
        go: downloading github.com/BurntSushi/toml v0.3.1
        go: downloading github.com/cyphar/filepath-securejoin v0.2.2
        go: downloading github.com/gobwas/glob v0.2.3
        go: downloading github.com/golang/protobuf v1.4.2
        go: downloading k8s.io/apimachinery v0.19.2
        go: downloading k8s.io/client-go v0.19.2
        go: downloading github.com/davecgh/go-spew v1.1.1
        go: downloading github.com/pmezard/go-difflib v1.0.0
        go: downloading gopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c
        go: downloading github.com/jmespath/go-jmespath v0.0.0-20180206201540-c2b33e8439
        af
        go: downloading github.com/mitchellh/go-homedir v1.1.0
        go: downloading gopkg.in/ini.v1 v1.49.0
        go: downloading github.com/evanphx/json-patch v4.9.0+incompatible
        go: downloading github.com/spf13/cobra v1.0.0
        go: downloading google.golang.org/protobuf v1.24.0
        go: downloading golang.org/x/sys v0.0.0-20200622214017-ed371f2e16b4
        go: downloading golang.org/x/text v0.3.3
        go: downloading k8s.io/klog v1.0.0
        go: downloading github.com/gogo/protobuf v1.3.1
        go: downloading k8s.io/klog/v2 v2.2.0
        go: downloading sigs.k8s.io/structured-merge-diff/v4 v4.0.1
        go: downloading github.com/liggitt/tabwriter v0.0.0-20181228230101-89fcab3d43de
        go: downloading github.com/googleapis/gnostic v0.4.1
        go: downloading k8s.io/api v0.19.2
        go: downloading sigs.k8s.io/kustomize v2.0.3+incompatible
        go: downloading github.com/gregjones/httpcache v0.0.0-20180305231024-9cad4c3443a
        7
        go: downloading github.com/peterbourgon/diskv v2.0.1+incompatible
        go: downloading github.com/imdario/mergo v0.3.8
        go: downloading github.com/inconshreveable/mousetrap v1.0.0
        go: downloading github.com/google/gofuzz v1.1.0
        go: downloading github.com/go-logr/logr v0.2.0
        go: downloading github.com/json-iterator/go v1.1.10
        go: downloading github.com/modern-go/reflect2 v1.0.1
        go: downloading golang.org/x/oauth2 v0.0.0-20191202225959-858c2ad4c8b6
        go: downloading golang.org/x/time v0.0.0-20191024005414-555d28b269f0
        go: downloading k8s.io/utils v0.0.0-20200729134348-d5654de09c73
        go: downloading github.com/google/btree v1.0.0
        go: downloading gopkg.in/inf.v0 v0.9.1
        go: downloading github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1
        dd
        go: downloading google.golang.org/appengine v1.6.5
        go: downloading github.com/go-openapi/spec v0.19.3
        go: downloading k8s.io/kube-openapi v0.0.0-20200805222855-6aeccd4b50c6
        go: downloading github.com/go-openapi/jsonpointer v0.19.3
        go: downloading github.com/go-openapi/jsonreference v0.19.3
        go: downloading github.com/go-openapi/swag v0.19.5
        go: downloading github.com/emicklei/go-restful v2.9.5+incompatible
        go: downloading github.com/PuerkitoBio/purell v1.1.1
        go: downloading github.com/mailru/easyjson v0.7.0
        go: downloading github.com/PuerkitoBio/urlesc v0.0.0-20170810143723-de5bf2ad4578
        installing
        building '/nix/store/x0kbgq0ddqv7850w49d1zk91sv0ngv3y-helm-diff-3.1.3-go-modules
        .drv'...
        unpacking sources
        unpacking source archive /nix/store/w1p937cb6yf99g9icfmjjri6bp30vcxz-source
        source root is source
        patching sources
        configuring
        building
        go: downloading github.com/Masterminds/semver v1.5.0
        go: downloading github.com/ghodss/yaml v1.0.0
        go: downloading github.com/mgutz/ansi v0.0.0-20200706080929-d51e80ef957d
        go: downloading github.com/spf13/cobra v1.0.0
        go: downloading github.com/spf13/pflag v1.0.5
        go: downloading google.golang.org/grpc v1.30.0
        go: downloading k8s.io/client-go v0.18.6
        go: downloading k8s.io/helm v2.16.9+incompatible
        go: downloading github.com/aryann/difflib v0.0.0-20170710044230-e206f873d14a
        go: downloading k8s.io/api v0.18.6
        go: downloading k8s.io/apimachinery v0.18.6
        go: downloading github.com/stretchr/testify v1.5.1
        go: downloading gopkg.in/yaml.v2 v2.3.0
        go: downloading github.com/mattn/go-colorable v0.1.7
        go: downloading github.com/inconshreveable/mousetrap v1.0.0
        go: downloading golang.org/x/net v0.0.0-20191004110552-13f9640d40b9
        go: downloading github.com/golang/protobuf v1.3.3
        go: downloading github.com/json-iterator/go v1.1.8
        go: downloading github.com/modern-go/reflect2 v1.0.1
        go: downloading k8s.io/klog v1.0.0
        go: downloading sigs.k8s.io/yaml v1.2.0
        go: downloading github.com/gogo/protobuf v1.3.1
        go: downloading github.com/mattn/go-isatty v0.0.12
        go: downloading golang.org/x/sys v0.0.0-20200223170610-d5e6a3e2c0ae
        go: downloading google.golang.org/genproto v0.0.0-20190819201941-24fa4b261c55
        go: downloading github.com/pkg/errors v0.9.1
        go: downloading github.com/BurntSushi/toml v0.3.1
        go: downloading github.com/cyphar/filepath-securejoin v0.2.2
        go: downloading github.com/gobwas/glob v0.2.3
        go: downloading golang.org/x/crypto v0.0.0-20200220183623-bac4c82f6975
        go: downloading github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1
        dd
        go: downloading sigs.k8s.io/structured-merge-diff/v3 v3.0.0
        go: downloading github.com/google/gofuzz v1.1.0
        go: downloading gopkg.in/inf.v0 v0.9.1
        go: downloading github.com/davecgh/go-spew v1.1.1
        go: downloading github.com/pmezard/go-difflib v1.0.0
        go: downloading github.com/Masterminds/sprig v2.22.0+incompatible
        go: downloading golang.org/x/text v0.3.2
        go: downloading github.com/Masterminds/goutils v1.1.0
        go: downloading github.com/google/uuid v1.1.1
        go: downloading github.com/huandu/xstrings v1.3.2
        go: downloading github.com/imdario/mergo v0.3.10
        go: downloading github.com/mitchellh/copystructure v1.0.0
        go: downloading github.com/mitchellh/reflectwalk v1.0.1
        installing
        building '/nix/store/ylm9zrdwz9z1zyki2fjgyw1y481hldk4-helm-s3-0.10.0.drv'...
        unpacking sources
        unpacking source archive /nix/store/d4z49ksly0059sdw2dlshr6dci6mqff6-source
        source root is source
        patching sources
        configuring
        building
        Building subPackage ./cmd/helms3
        internal/unsafeheader
        internal/cpu
        internal/bytealg
        runtime/internal/atomic
        runtime/internal/sys
        runtime/internal/math
        runtime
        internal/reflectlite
        errors
        internal/race
        sync/atomic
        sync
        internal/oserror
        syscall
        time
        context
        math/bits
        math
        unicode/utf8
        strconv
        unicode
        reflect
        sort
        internal/fmtsort
        io
        internal/syscall/unix
        internal/poll
        internal/syscall/execenv
        internal/testlog
        path
        io/fs
        os
        fmt
        bytes
        strings
        encoding/hex
        github.com/aws/aws-sdk-go/aws/awserr
        path/filepath
        io/ioutil
        github.com/aws/aws-sdk-go/internal/ini
        github.com/aws/aws-sdk-go/internal/shareddefaults
        github.com/aws/aws-sdk-go/aws/credentials
        encoding
        encoding/binary
        encoding/base64
        unicode/utf16
        encoding/json
        regexp/syntax
        regexp
        github.com/aws/aws-sdk-go/aws/endpoints
        github.com/aws/aws-sdk-go/internal/sdkio
        log
        bufio
        compress/flate
        hash
        hash/crc32
        compress/gzip
        container/list
        crypto/internal/subtle
        crypto/subtle
        crypto/cipher
        crypto/aes
        math/rand
        math/big
        crypto/rand
        crypto
        crypto/des
        crypto/elliptic
        crypto/internal/randutil
        crypto/sha512
        encoding/asn1
        vendor/golang.org/x/crypto/cryptobyte/asn1
        vendor/golang.org/x/crypto/cryptobyte
        crypto/ecdsa
        crypto/ed25519/internal/edwards25519
        crypto/ed25519
        crypto/hmac
        crypto/md5
        crypto/rc4
        crypto/rsa
        crypto/sha1
        crypto/sha256
        crypto/dsa
        crypto/x509/pkix
        encoding/pem
        vendor/golang.org/x/net/dns/dnsmessage
        internal/nettrace
        internal/singleflight
        runtime/cgo
        net
        net/url
        crypto/x509
        vendor/golang.org/x/crypto/internal/subtle
        vendor/golang.org/x/crypto/chacha20
        vendor/golang.org/x/crypto/poly1305
        vendor/golang.org/x/sys/cpu
        vendor/golang.org/x/crypto/chacha20poly1305
        vendor/golang.org/x/crypto/curve25519
        vendor/golang.org/x/crypto/hkdf
        crypto/tls
        vendor/golang.org/x/text/transform
        vendor/golang.org/x/text/unicode/bidi
        vendor/golang.org/x/text/secure/bidirule
        vendor/golang.org/x/text/unicode/norm
        vendor/golang.org/x/net/idna
        net/textproto
        vendor/golang.org/x/net/http/httpguts
        vendor/golang.org/x/net/http/httpproxy
        vendor/golang.org/x/net/http2/hpack
        mime
        mime/quotedprintable
        mime/multipart
        net/http/httptrace
        net/http/internal
        net/http
        github.com/aws/aws-sdk-go/aws
        github.com/aws/aws-sdk-go/aws/client/metadata
        github.com/jmespath/go-jmespath
        github.com/aws/aws-sdk-go/aws/awsutil
        github.com/aws/aws-sdk-go/aws/request
        github.com/aws/aws-sdk-go/internal/sdkrand
        net/http/httputil
        github.com/aws/aws-sdk-go/aws/client
        github.com/aws/aws-sdk-go/aws/corehandlers
        os/exec
        github.com/aws/aws-sdk-go/aws/credentials/processcreds
        github.com/aws/aws-sdk-go/internal/sdkmath
        github.com/aws/aws-sdk-go/private/protocol
        github.com/aws/aws-sdk-go/private/protocol/rest
        github.com/aws/aws-sdk-go/aws/signer/v4
        encoding/xml
        github.com/aws/aws-sdk-go/private/protocol/query/queryutil
        github.com/aws/aws-sdk-go/private/protocol/xml/xmlutil
        github.com/aws/aws-sdk-go/private/protocol/query
        github.com/aws/aws-sdk-go/service/sts
        github.com/aws/aws-sdk-go/service/sts/stsiface
        github.com/aws/aws-sdk-go/aws/credentials/stscreds
        github.com/aws/aws-sdk-go/aws/csm
        github.com/aws/aws-sdk-go/internal/sdkuri
        github.com/aws/aws-sdk-go/aws/ec2metadata
        github.com/aws/aws-sdk-go/aws/credentials/ec2rolecreds
        github.com/aws/aws-sdk-go/private/protocol/json/jsonutil
        github.com/aws/aws-sdk-go/aws/credentials/endpointcreds
        github.com/aws/aws-sdk-go/aws/defaults
        github.com/aws/aws-sdk-go/aws/session
        github.com/aws/aws-sdk-go/aws/arn
        github.com/aws/aws-sdk-go/internal/s3err
        github.com/aws/aws-sdk-go/private/protocol/eventstream
        github.com/aws/aws-sdk-go/private/protocol/eventstream/eventstreamapi
        github.com/aws/aws-sdk-go/private/protocol/restxml
        github.com/aws/aws-sdk-go/service/s3/internal/arn
        github.com/aws/aws-sdk-go/service/s3
        github.com/aws/aws-sdk-go/service/s3/s3iface
        github.com/aws/aws-sdk-go/service/s3/s3manager
        github.com/Masterminds/semver
        database/sql/driver
        github.com/Masterminds/semver/v3
        gopkg.in/yaml.v2
        github.com/ghodss/yaml
        github.com/pkg/errors
        helm.sh/helm/v3/pkg/chart
        os/user
        archive/tar
        helm.sh/helm/v3/internal/ignore
        helm.sh/helm/v3/internal/sympath
        sigs.k8s.io/yaml
        helm.sh/helm/v3/pkg/chart/loader
        encoding/csv
        flag
        github.com/spf13/pflag
        helm.sh/helm/v3/pkg/helmpath/xdg
        k8s.io/client-go/util/homedir
        helm.sh/helm/v3/pkg/helmpath
        github.com/evanphx/json-patch
        text/template/parse
        text/template
        github.com/spf13/cobra
        github.com/gogo/protobuf/proto
        github.com/gogo/protobuf/sortkeys
        github.com/google/gofuzz
        gopkg.in/inf.v0
        k8s.io/apimachinery/pkg/api/resource
        k8s.io/apimachinery/third_party/forked/golang/reflect
        k8s.io/apimachinery/pkg/conversion
        k8s.io/apimachinery/pkg/selection
        k8s.io/apimachinery/pkg/fields
        k8s.io/apimachinery/pkg/util/sets
        k8s.io/apimachinery/pkg/util/errors
        k8s.io/apimachinery/pkg/util/validation/field
        k8s.io/apimachinery/pkg/util/validation
        github.com/go-logr/logr
        k8s.io/klog/v2
        k8s.io/apimachinery/pkg/labels
        go/token
        go/scanner
        go/ast
        internal/lazyregexp
        go/doc
        go/parser
        k8s.io/apimachinery/pkg/conversion/queryparams
        k8s.io/apimachinery/pkg/runtime/schema
        k8s.io/apimachinery/pkg/util/json
        runtime/debug
        k8s.io/apimachinery/pkg/util/naming
        k8s.io/apimachinery/pkg/util/runtime
        github.com/modern-go/concurrent
        github.com/modern-go/reflect2
        github.com/json-iterator/go
        sigs.k8s.io/structured-merge-diff/v4/value
        k8s.io/apimachinery/pkg/runtime
        k8s.io/apimachinery/pkg/types
        k8s.io/apimachinery/pkg/util/intstr
        golang.org/x/text/transform
        golang.org/x/text/unicode/bidi
        golang.org/x/text/secure/bidirule
        golang.org/x/text/unicode/norm
        golang.org/x/net/idna
        golang.org/x/net/http/httpguts
        golang.org/x/net/http2/hpack
        golang.org/x/net/http2
        k8s.io/apimachinery/pkg/util/net
        k8s.io/apimachinery/pkg/watch
        k8s.io/apimachinery/pkg/apis/meta/v1
        k8s.io/apimachinery/pkg/api/meta
        github.com/liggitt/tabwriter
        k8s.io/apimachinery/pkg/apis/meta/v1/unstructured
        k8s.io/apimachinery/pkg/util/duration
        k8s.io/client-go/third_party/forked/golang/template
        k8s.io/client-go/util/jsonpath
        k8s.io/cli-runtime/pkg/printers
        hash/fnv
        google.golang.org/protobuf/internal/detrand
        google.golang.org/protobuf/internal/errors
        google.golang.org/protobuf/encoding/protowire
        google.golang.org/protobuf/internal/pragma
        google.golang.org/protobuf/reflect/protoreflect
        google.golang.org/protobuf/reflect/protoregistry
        google.golang.org/protobuf/internal/encoding/messageset
        google.golang.org/protobuf/internal/flags
        google.golang.org/protobuf/internal/strs
        google.golang.org/protobuf/internal/encoding/text
        google.golang.org/protobuf/internal/fieldnum
        google.golang.org/protobuf/internal/mapsort
        google.golang.org/protobuf/internal/set
        google.golang.org/protobuf/internal/fieldsort
        google.golang.org/protobuf/runtime/protoiface
        google.golang.org/protobuf/proto
        google.golang.org/protobuf/encoding/prototext
        google.golang.org/protobuf/internal/descfmt
        google.golang.org/protobuf/internal/descopts
        google.golang.org/protobuf/internal/encoding/defval
        google.golang.org/protobuf/internal/filedesc
        google.golang.org/protobuf/internal/encoding/tag
        google.golang.org/protobuf/internal/genname
        google.golang.org/protobuf/internal/impl
        google.golang.org/protobuf/internal/filetype
        google.golang.org/protobuf/internal/version
        google.golang.org/protobuf/runtime/protoimpl
        github.com/golang/protobuf/proto
        google.golang.org/protobuf/types/known/anypb
        github.com/golang/protobuf/ptypes/any
        google.golang.org/protobuf/types/known/durationpb
        github.com/golang/protobuf/ptypes/duration
        google.golang.org/protobuf/types/known/timestamppb
        github.com/golang/protobuf/ptypes/timestamp
        github.com/golang/protobuf/ptypes
        github.com/googleapis/gnostic/extensions
        github.com/googleapis/gnostic/compiler
        github.com/googleapis/gnostic/openapiv2
        golang.org/x/text/encoding/internal/identifier
        golang.org/x/text/encoding
        golang.org/x/text/encoding/internal
        golang.org/x/text/internal/utf8internal
        golang.org/x/text/runes
        golang.org/x/text/encoding/unicode
        k8s.io/api/core/v1
        k8s.io/apimachinery/pkg/api/errors
        k8s.io/apimachinery/pkg/runtime/serializer/recognizer
        k8s.io/apimachinery/pkg/util/framer
        k8s.io/apimachinery/pkg/util/yaml
        k8s.io/apimachinery/pkg/runtime/serializer/json
        k8s.io/apimachinery/pkg/runtime/serializer/protobuf
        k8s.io/apimachinery/pkg/runtime/serializer/versioning
        k8s.io/apimachinery/pkg/runtime/serializer
        k8s.io/apimachinery/pkg/apis/meta/v1/unstructured/unstructuredscheme
        k8s.io/cli-runtime/pkg/kustomize/k8sdeps/kv
        sigs.k8s.io/kustomize/pkg/gvk
        sigs.k8s.io/kustomize/pkg/image
        sigs.k8s.io/kustomize/pkg/patch
        sigs.k8s.io/kustomize/pkg/types
        sigs.k8s.io/kustomize/pkg/ifc
        k8s.io/cli-runtime/pkg/kustomize/k8sdeps/configmapandsecret
        k8s.io/cli-runtime/pkg/kustomize/k8sdeps/kunstruct
        sigs.k8s.io/kustomize/pkg/internal/error
        sigs.k8s.io/kustomize/pkg/resid
        sigs.k8s.io/kustomize/pkg/resource
        sigs.k8s.io/kustomize/pkg/resmap
        sigs.k8s.io/kustomize/pkg/expansion
        encoding/gob
        github.com/mailru/easyjson/jlexer
        github.com/mailru/easyjson/buffer
        github.com/mailru/easyjson/jwriter
        github.com/go-openapi/swag
        github.com/go-openapi/jsonpointer
        github.com/PuerkitoBio/urlesc
        golang.org/x/text/width
        github.com/PuerkitoBio/purell
        github.com/go-openapi/jsonreference
        github.com/go-openapi/spec
        hash/adler32
        compress/zlib
        github.com/emicklei/go-restful/log
        github.com/emicklei/go-restful
        k8s.io/kube-openapi/pkg/common
        sigs.k8s.io/kustomize/pkg/transformers/config/defaultconfig
        sigs.k8s.io/kustomize/pkg/transformers/config
        sigs.k8s.io/kustomize/pkg/transformers
        k8s.io/cli-runtime/pkg/kustomize/k8sdeps/transformer/hash
        github.com/davecgh/go-spew/spew
        k8s.io/apimachinery/pkg/util/mergepatch
        k8s.io/apimachinery/third_party/forked/golang/json
        k8s.io/kube-openapi/pkg/util/proto
        k8s.io/apimachinery/pkg/util/strategicpatch
        k8s.io/api/admissionregistration/v1
        k8s.io/api/admissionregistration/v1beta1
        k8s.io/api/apps/v1
        k8s.io/api/apps/v1beta1
        k8s.io/api/apps/v1beta2
        k8s.io/api/authentication/v1
        k8s.io/api/authentication/v1beta1
        k8s.io/api/authorization/v1
        k8s.io/api/authorization/v1beta1
        k8s.io/api/autoscaling/v1
        k8s.io/api/autoscaling/v2beta1
        k8s.io/api/autoscaling/v2beta2
        k8s.io/api/batch/v1
        k8s.io/api/batch/v1beta1
        k8s.io/api/batch/v2alpha1
        k8s.io/api/certificates/v1
        k8s.io/api/certificates/v1beta1
        k8s.io/api/coordination/v1
        k8s.io/api/coordination/v1beta1
        k8s.io/api/discovery/v1alpha1
        k8s.io/api/discovery/v1beta1
        k8s.io/api/events/v1
        k8s.io/api/events/v1beta1
        k8s.io/api/extensions/v1beta1
        k8s.io/api/flowcontrol/v1alpha1
        k8s.io/api/networking/v1
        k8s.io/api/networking/v1beta1
        k8s.io/api/node/v1alpha1
        k8s.io/api/node/v1beta1
        k8s.io/api/policy/v1beta1
        k8s.io/api/rbac/v1
        k8s.io/api/rbac/v1alpha1
        k8s.io/api/rbac/v1beta1
        k8s.io/api/scheduling/v1
        k8s.io/api/scheduling/v1alpha1
        k8s.io/api/scheduling/v1beta1
        k8s.io/api/settings/v1alpha1
        k8s.io/api/storage/v1
        k8s.io/api/storage/v1alpha1
        k8s.io/api/storage/v1beta1
        k8s.io/client-go/kubernetes/scheme
        k8s.io/cli-runtime/pkg/kustomize/k8sdeps/transformer/patch
        k8s.io/cli-runtime/pkg/kustomize/k8sdeps/transformer
        k8s.io/apimachinery/pkg/api/equality
        k8s.io/apimachinery/pkg/apis/meta/v1/validation
        k8s.io/apimachinery/pkg/api/validation
        k8s.io/cli-runtime/pkg/kustomize/k8sdeps/validator
        sigs.k8s.io/kustomize/pkg/ifc/transformer
        sigs.k8s.io/kustomize/pkg/factory
        k8s.io/cli-runtime/pkg/kustomize/k8sdeps
        sigs.k8s.io/kustomize/pkg/constants
        sigs.k8s.io/kustomize/pkg/fs
        sigs.k8s.io/kustomize/pkg/git
        sigs.k8s.io/kustomize/pkg/loader
        sigs.k8s.io/kustomize/pkg/patch/transformer
        sigs.k8s.io/kustomize/pkg/target
        sigs.k8s.io/kustomize/pkg/commands/build
        k8s.io/cli-runtime/pkg/kustomize
        k8s.io/apimachinery/pkg/version
        k8s.io/apimachinery/pkg/runtime/serializer/streaming
        k8s.io/apimachinery/pkg/util/clock
        k8s.io/client-go/pkg/version
        golang.org/x/sys/internal/unsafeheader
        golang.org/x/sys/unix
        golang.org/x/crypto/ssh/terminal
        k8s.io/client-go/pkg/apis/clientauthentication
        k8s.io/client-go/pkg/apis/clientauthentication/v1alpha1
        k8s.io/client-go/pkg/apis/clientauthentication/v1beta1
        k8s.io/client-go/tools/clientcmd/api
        k8s.io/client-go/tools/metrics
        golang.org/x/net/context/ctxhttp
        golang.org/x/oauth2/internal
        golang.org/x/oauth2
        k8s.io/apimachinery/pkg/util/wait
        k8s.io/client-go/util/connrotation
        container/heap
        golang.org/x/time/rate
        k8s.io/client-go/util/workqueue
        k8s.io/client-go/transport
        k8s.io/client-go/plugin/pkg/client/auth/exec
        k8s.io/client-go/rest/watch
        k8s.io/client-go/util/keyutil
        k8s.io/client-go/util/cert
        k8s.io/utils/integer
        k8s.io/client-go/util/flowcontrol
        k8s.io/client-go/rest
        k8s.io/client-go/discovery
        k8s.io/client-go/dynamic
        k8s.io/client-go/restmapper
        k8s.io/cli-runtime/pkg/resource
        github.com/gregjones/httpcache
        github.com/google/btree
        github.com/peterbourgon/diskv
        github.com/gregjones/httpcache/diskcache
        k8s.io/client-go/discovery/cached/disk
        github.com/imdario/mergo
        k8s.io/client-go/tools/auth
        k8s.io/client-go/tools/clientcmd/api/v1
        k8s.io/client-go/tools/clientcmd/api/latest
        k8s.io/client-go/tools/clientcmd
        k8s.io/cli-runtime/pkg/genericclioptions
        helm.sh/helm/v3/pkg/cli
        golang.org/x/crypto/openpgp/errors
        golang.org/x/crypto/openpgp/armor
        compress/bzip2
        golang.org/x/crypto/cast5
        golang.org/x/crypto/openpgp/elgamal
        golang.org/x/crypto/openpgp/s2k
        image/color
        image
        image/internal/imageutil
        image/jpeg
        golang.org/x/crypto/openpgp/packet
        golang.org/x/crypto/openpgp
        golang.org/x/crypto/openpgp/clearsign
        helm.sh/helm/v3/pkg/provenance
        helm.sh/helm/v3/internal/third_party/dep/fs
        helm.sh/helm/v3/internal/fileutil
        helm.sh/helm/v3/internal/urlutil
        helm.sh/helm/v3/internal/tlsutil
        helm.sh/helm/v3/internal/version
        helm.sh/helm/v3/pkg/plugin
        helm.sh/helm/v3/pkg/getter
        helm.sh/helm/v3/pkg/repo
        github.com/BurntSushi/toml
        github.com/cyphar/filepath-securejoin
        github.com/gobwas/glob/util/runes
        github.com/gobwas/glob/util/strings
        github.com/gobwas/glob/match
        github.com/gobwas/glob/syntax/lexer
        github.com/gobwas/glob/syntax/ast
        github.com/gobwas/glob/compiler
        github.com/gobwas/glob/syntax
        github.com/gobwas/glob
        k8s.io/helm/pkg/ignore
        k8s.io/helm/pkg/proto/hapi/chart
        k8s.io/helm/pkg/proto/hapi/version
        k8s.io/helm/pkg/sympath
        k8s.io/helm/pkg/version
        k8s.io/helm/pkg/chartutil
        k8s.io/helm/pkg/helm/helmpath
        k8s.io/helm/pkg/helm/environment
        k8s.io/helm/pkg/provenance
        html
        html/template
        k8s.io/helm/pkg/plugin
        k8s.io/helm/pkg/urlutil
        k8s.io/helm/pkg/tlsutil
        k8s.io/helm/pkg/getter
        k8s.io/helm/pkg/repo
        github.com/hypnoglow/helm-s3/internal/helmutil
        github.com/hypnoglow/helm-s3/internal/awss3
        github.com/hypnoglow/helm-s3/internal/awsutil
        github.com/alecthomas/template/parse
        github.com/alecthomas/template
        github.com/alecthomas/units
        gopkg.in/alecthomas/kingpin.v2
        github.com/hypnoglow/helm-s3/cmd/helms3
        Building subPackage ./internal/awss3
        Building subPackage ./internal/awsutil
        Building subPackage ./internal/helmutil
        Building subPackage ./tests/e2e
        github.com/hypnoglow/helm-s3/tests/e2e
        running tests
        go test $(go list ./... | grep -v e2e)
        ?       github.com/hypnoglow/helm-s3/cmd/helms3 [no test files]
        ?       github.com/hypnoglow/helm-s3/internal/awss3     [no test files]
        ok      github.com/hypnoglow/helm-s3/internal/awsutil   0.006s
        ok      github.com/hypnoglow/helm-s3/internal/helmutil  0.046s
        installing
        post-installation fixup
        shrinking RPATHs of ELF executables and libraries in /nix/store/n1cfd9naa76ap7xf
        xb42gfakq93abcjs-helm-s3-0.10.0
        shrinking /nix/store/n1cfd9naa76ap7xfxb42gfakq93abcjs-helm-s3-0.10.0/helm-s3/bin
        /helms3
        strip is /nix/store/cp1sa3xxvl71cypiinw2c62i5s33chlr-binutils-2.35.1/bin/strip
        patching script interpreter paths in /nix/store/n1cfd9naa76ap7xfxb42gfakq93abcjs
        -helm-s3-0.10.0
        checking for references to /build/ in /nix/store/n1cfd9naa76ap7xfxb42gfakq93abcj
        s-helm-s3-0.10.0...
        building '/nix/store/wm1czmqzpnx32673mhv8v9d9ikgkf425-helm-diff-3.1.3.drv'...
        unpacking sources
        unpacking source archive /nix/store/w1p937cb6yf99g9icfmjjri6bp30vcxz-source
        source root is source
        patching sources
        configuring
        building
        Building subPackage .
        internal/unsafeheader
        internal/cpu
        internal/bytealg
        runtime/internal/atomic
        runtime/internal/sys
        runtime/internal/math
        runtime
        internal/reflectlite
        errors
        math/bits
        math
        unicode/utf8
        strconv
        internal/race
        sync/atomic
        sync
        unicode
        reflect
        sort
        internal/fmtsort
        io
        internal/oserror
        syscall
        internal/syscall/unix
        time
        internal/poll
        internal/syscall/execenv
        internal/testlog
        path
        io/fs
        os
        fmt
        bytes
        encoding
        encoding/binary
        encoding/base64
        strings
        unicode/utf16
        encoding/json
        regexp/syntax
        regexp
        github.com/Masterminds/semver
        github.com/aryann/difflib
        bufio
        gopkg.in/yaml.v2
        log
        github.com/golang/protobuf/proto
        github.com/golang/protobuf/ptypes/any
        github.com/golang/protobuf/ptypes/timestamp
        k8s.io/helm/pkg/proto/hapi/chart
        k8s.io/helm/pkg/proto/hapi/release
        github.com/databus23/helm-diff/v3/manifest
        golang.org/x/sys/unix
        github.com/mattn/go-isatty
        github.com/mattn/go-colorable
        github.com/mgutz/ansi
        github.com/gogo/protobuf/proto
        github.com/gogo/protobuf/sortkeys
        math/rand
        math/big
        gopkg.in/inf.v0
        k8s.io/apimachinery/pkg/api/resource
        github.com/google/gofuzz
        k8s.io/apimachinery/third_party/forked/golang/reflect
        k8s.io/apimachinery/pkg/conversion
        k8s.io/apimachinery/pkg/selection
        k8s.io/apimachinery/pkg/fields
        k8s.io/apimachinery/pkg/util/sets
        k8s.io/apimachinery/pkg/util/errors
        k8s.io/apimachinery/pkg/util/validation/field
        context
        vendor/golang.org/x/net/dns/dnsmessage
        internal/nettrace
        internal/singleflight
        runtime/cgo
        net
        k8s.io/apimachinery/pkg/util/validation
        flag
        os/user
        path/filepath
        k8s.io/klog
        k8s.io/apimachinery/pkg/labels
        go/token
        go/scanner
        go/ast
        internal/lazyregexp
        net/url
        text/template/parse
        text/template
        go/doc
        go/parser
        k8s.io/apimachinery/pkg/conversion/queryparams
        k8s.io/apimachinery/pkg/runtime/schema
        k8s.io/apimachinery/pkg/util/json
        runtime/debug
        k8s.io/apimachinery/pkg/util/naming
        compress/flate
        hash
        hash/crc32
        compress/gzip
        container/list
        crypto/internal/subtle
        crypto/subtle
        crypto/cipher
        crypto/aes
        crypto/rand
        crypto
        crypto/des
        crypto/elliptic
        crypto/internal/randutil
        crypto/sha512
        encoding/asn1
        vendor/golang.org/x/crypto/cryptobyte/asn1
        vendor/golang.org/x/crypto/cryptobyte
        crypto/ecdsa
        crypto/ed25519/internal/edwards25519
        crypto/ed25519
        crypto/hmac
        crypto/md5
        crypto/rc4
        crypto/rsa
        crypto/sha1
        crypto/sha256
        crypto/dsa
        encoding/hex
        crypto/x509/pkix
        encoding/pem
        io/ioutil
        crypto/x509
        vendor/golang.org/x/crypto/internal/subtle
        vendor/golang.org/x/crypto/chacha20
        vendor/golang.org/x/crypto/poly1305
        vendor/golang.org/x/sys/cpu
        vendor/golang.org/x/crypto/chacha20poly1305
        vendor/golang.org/x/crypto/curve25519
        vendor/golang.org/x/crypto/hkdf
        crypto/tls
        vendor/golang.org/x/text/transform
        vendor/golang.org/x/text/unicode/bidi
        vendor/golang.org/x/text/secure/bidirule
        vendor/golang.org/x/text/unicode/norm
        vendor/golang.org/x/net/idna
        net/textproto
        vendor/golang.org/x/net/http/httpguts
        vendor/golang.org/x/net/http/httpproxy
        vendor/golang.org/x/net/http2/hpack
        mime
        mime/quotedprintable
        mime/multipart
        net/http/httptrace
        net/http/internal
        net/http
        k8s.io/apimachinery/pkg/util/runtime
        github.com/modern-go/concurrent
        github.com/modern-go/reflect2
        github.com/json-iterator/go
        sigs.k8s.io/structured-merge-diff/v3/value
        k8s.io/apimachinery/pkg/runtime
        k8s.io/apimachinery/pkg/types
        k8s.io/apimachinery/pkg/util/intstr
        golang.org/x/text/transform
        golang.org/x/text/unicode/bidi
        golang.org/x/text/secure/bidirule
        golang.org/x/text/unicode/norm
        golang.org/x/net/idna
        golang.org/x/net/http/httpguts
        golang.org/x/net/http2/hpack
        golang.org/x/net/http2
        k8s.io/apimachinery/pkg/util/net
        k8s.io/apimachinery/pkg/watch
        k8s.io/apimachinery/pkg/apis/meta/v1
        k8s.io/api/core/v1
        k8s.io/apimachinery/pkg/runtime/serializer/recognizer
        k8s.io/apimachinery/pkg/util/framer
        sigs.k8s.io/yaml
        k8s.io/apimachinery/pkg/util/yaml
        k8s.io/apimachinery/pkg/runtime/serializer/json
        k8s.io/api/admissionregistration/v1
        k8s.io/api/admissionregistration/v1beta1
        k8s.io/api/apps/v1
        k8s.io/api/apps/v1beta1
        k8s.io/api/apps/v1beta2
        k8s.io/api/auditregistration/v1alpha1
        k8s.io/api/authentication/v1
        k8s.io/api/authentication/v1beta1
        k8s.io/api/authorization/v1
        k8s.io/api/authorization/v1beta1
        k8s.io/api/autoscaling/v1
        k8s.io/api/autoscaling/v2beta1
        k8s.io/api/autoscaling/v2beta2
        k8s.io/api/batch/v1
        k8s.io/api/batch/v1beta1
        k8s.io/api/batch/v2alpha1
        k8s.io/api/certificates/v1beta1
        k8s.io/api/coordination/v1
        k8s.io/api/coordination/v1beta1
        k8s.io/api/discovery/v1alpha1
        k8s.io/api/discovery/v1beta1
        k8s.io/api/events/v1beta1
        k8s.io/api/extensions/v1beta1
        k8s.io/api/flowcontrol/v1alpha1
        k8s.io/api/networking/v1
        k8s.io/api/networking/v1beta1
        k8s.io/api/node/v1alpha1
        k8s.io/api/node/v1beta1
        k8s.io/api/policy/v1beta1
        k8s.io/api/rbac/v1
        k8s.io/api/rbac/v1alpha1
        k8s.io/api/rbac/v1beta1
        k8s.io/api/scheduling/v1
        k8s.io/api/scheduling/v1alpha1
        k8s.io/api/scheduling/v1beta1
        k8s.io/api/settings/v1alpha1
        k8s.io/api/storage/v1
        k8s.io/api/storage/v1alpha1
        k8s.io/api/storage/v1beta1
        k8s.io/apimachinery/pkg/runtime/serializer/protobuf
        k8s.io/apimachinery/pkg/apis/meta/v1/unstructured
        k8s.io/apimachinery/pkg/runtime/serializer/versioning
        k8s.io/apimachinery/pkg/runtime/serializer
        k8s.io/client-go/kubernetes/scheme
        github.com/databus23/helm-diff/v3/diff
        github.com/ghodss/yaml
        encoding/csv
        github.com/spf13/pflag
        github.com/spf13/cobra
        golang.org/x/net/internal/timeseries
        html
        html/template
        text/tabwriter
        golang.org/x/net/trace
        google.golang.org/grpc/backoff
        google.golang.org/grpc/internal/grpclog
        google.golang.org/grpc/grpclog
        google.golang.org/grpc/connectivity
        google.golang.org/grpc/attributes
        google.golang.org/grpc/credentials/internal
        google.golang.org/grpc/serviceconfig
        google.golang.org/grpc/internal
        google.golang.org/grpc/credentials
        google.golang.org/grpc/metadata
        google.golang.org/grpc/resolver
        google.golang.org/grpc/balancer
        google.golang.org/grpc/balancer/base
        google.golang.org/grpc/internal/grpcrand
        google.golang.org/grpc/balancer/roundrobin
        google.golang.org/grpc/codes
        google.golang.org/grpc/encoding
        google.golang.org/grpc/encoding/proto
        google.golang.org/grpc/internal/backoff
        google.golang.org/grpc/internal/balancerload
        github.com/golang/protobuf/ptypes/duration
        github.com/golang/protobuf/ptypes
        google.golang.org/grpc/binarylog/grpc_binarylog_v1
        google.golang.org/grpc/internal/grpcutil
        google.golang.org/genproto/googleapis/rpc/status
        google.golang.org/grpc/internal/status
        google.golang.org/grpc/status
        google.golang.org/grpc/internal/binarylog
        google.golang.org/grpc/internal/buffer
        google.golang.org/grpc/internal/channelz
        google.golang.org/grpc/internal/envconfig
        google.golang.org/grpc/internal/grpcsync
        google.golang.org/grpc/balancer/grpclb/state
        google.golang.org/grpc/internal/resolver/dns
        google.golang.org/grpc/internal/resolver/passthrough
        google.golang.org/grpc/internal/serviceconfig
        google.golang.org/grpc/internal/syscall
        google.golang.org/grpc/keepalive
        google.golang.org/grpc/peer
        google.golang.org/grpc/stats
        google.golang.org/grpc/tap
        google.golang.org/grpc/internal/transport
        net/http/httputil
        google.golang.org/grpc
        k8s.io/client-go/util/homedir
        github.com/pkg/errors
        k8s.io/helm/internal/third_party/dep/fs
        archive/tar
        github.com/BurntSushi/toml
        github.com/cyphar/filepath-securejoin
        github.com/gobwas/glob/util/runes
        github.com/gobwas/glob/util/strings
        github.com/gobwas/glob/match
        github.com/gobwas/glob/syntax/lexer
        github.com/gobwas/glob/syntax/ast
        github.com/gobwas/glob/compiler
        github.com/gobwas/glob/syntax
        github.com/gobwas/glob
        k8s.io/apimachinery/pkg/version
        k8s.io/helm/pkg/ignore
        k8s.io/helm/pkg/proto/hapi/version
        k8s.io/helm/pkg/sympath
        k8s.io/helm/pkg/version
        k8s.io/helm/pkg/chartutil
        k8s.io/helm/pkg/helm/helmpath
        k8s.io/helm/pkg/helm/environment
        k8s.io/helm/pkg/plugin
        k8s.io/helm/pkg/urlutil
        k8s.io/helm/pkg/tlsutil
        os/exec
        k8s.io/helm/pkg/getter
        golang.org/x/crypto/openpgp/errors
        golang.org/x/crypto/openpgp/armor
        compress/bzip2
        hash/adler32
        compress/zlib
        golang.org/x/crypto/cast5
        golang.org/x/crypto/openpgp/elgamal
        golang.org/x/crypto/openpgp/s2k
        image/color
        image
        image/internal/imageutil
        image/jpeg
        golang.org/x/crypto/openpgp/packet
        golang.org/x/crypto/openpgp
        golang.org/x/crypto/openpgp/clearsign
        k8s.io/helm/pkg/provenance
        k8s.io/helm/pkg/repo
        k8s.io/helm/pkg/resolver
        k8s.io/helm/pkg/downloader
        golang.org/x/net/context
        google.golang.org/grpc/health/grpc_health_v1
        k8s.io/helm/pkg/releaseutil
        k8s.io/helm/pkg/manifest
        k8s.io/helm/pkg/proto/hapi/services
        encoding/base32
        github.com/Masterminds/goutils
        database/sql/driver
        github.com/google/uuid
        github.com/huandu/xstrings
        github.com/imdario/mergo
        github.com/mitchellh/reflectwalk
        github.com/mitchellh/copystructure
        golang.org/x/crypto/pbkdf2
        golang.org/x/crypto/scrypt
        github.com/Masterminds/sprig
        k8s.io/helm/pkg/engine
        k8s.io/helm/pkg/renderutil
        k8s.io/helm/pkg/storage/errors
        k8s.io/helm/pkg/helm
        k8s.io/helm/pkg/strvals
        github.com/databus23/helm-diff/v3/cmd
        github.com/databus23/helm-diff/v3
        Building subPackage ./cmd
        Building subPackage ./diff
        Building subPackage ./manifest
        running tests
        === RUN   TestPrintDiffWithContext
        === RUN   TestPrintDiffWithContext/context-disabled
        === RUN   TestPrintDiffWithContext/context-0
        === RUN   TestPrintDiffWithContext/context-1
        === RUN   TestPrintDiffWithContext/context-2
        === RUN   TestPrintDiffWithContext/context-3
        --- PASS: TestPrintDiffWithContext (0.00s)
            --- PASS: TestPrintDiffWithContext/context-disabled (0.00s)
            --- PASS: TestPrintDiffWithContext/context-0 (0.00s)
            --- PASS: TestPrintDiffWithContext/context-1 (0.00s)
            --- PASS: TestPrintDiffWithContext/context-2 (0.00s)
            --- PASS: TestPrintDiffWithContext/context-3 (0.00s)
        === RUN   TestManifests
        === RUN   TestManifests/OnChange
        === RUN   TestManifests/OnNoChange
        === RUN   TestManifests/OnChangeSimple
        === RUN   TestManifests/OnNoChangeSimple
        === RUN   TestManifests/OnChangeTemplate
        === RUN   TestManifests/OnChangeJSON
        === RUN   TestManifests/OnNoChangeTemplate
        === RUN   TestManifests/OnChangeCustomTemplate
        --- PASS: TestManifests (0.00s)
            --- PASS: TestManifests/OnChange (0.00s)
            --- PASS: TestManifests/OnNoChange (0.00s)
            --- PASS: TestManifests/OnChangeSimple (0.00s)
            --- PASS: TestManifests/OnNoChangeSimple (0.00s)
            --- PASS: TestManifests/OnChangeTemplate (0.00s)
            --- PASS: TestManifests/OnChangeJSON (0.00s)
            --- PASS: TestManifests/OnNoChangeTemplate (0.00s)
            --- PASS: TestManifests/OnChangeCustomTemplate (0.00s)
        PASS
        ok      github.com/databus23/helm-diff/v3/diff  0.011s
        === RUN   TestPod
        --- PASS: TestPod (0.00s)
        === RUN   TestPodNamespace
        --- PASS: TestPodNamespace (0.00s)
        === RUN   TestPodHook
        --- PASS: TestPodHook (0.00s)
        === RUN   TestDeployV1
        --- PASS: TestDeployV1 (0.00s)
        === RUN   TestDeployV1Beta1
        --- PASS: TestDeployV1Beta1 (0.00s)
        === RUN   TestList
        --- PASS: TestList (0.00s)
        === RUN   TestEmpty
        --- PASS: TestEmpty (0.00s)
        PASS
        ok      github.com/databus23/helm-diff/v3/manifest      0.006s
        installing
        post-installation fixup
        shrinking RPATHs of ELF executables and libraries in /nix/store/qgs64hfxbinj6lw1
        4s4i6v4g9nzv8w72-helm-diff-3.1.3
        shrinking /nix/store/qgs64hfxbinj6lw14s4i6v4g9nzv8w72-helm-diff-3.1.3/helm-diff/
        bin/diff
        strip is /nix/store/cp1sa3xxvl71cypiinw2c62i5s33chlr-binutils-2.35.1/bin/strip
        patching script interpreter paths in /nix/store/qgs64hfxbinj6lw14s4i6v4g9nzv8w72
        -helm-diff-3.1.3
        checking for references to /build/ in /nix/store/qgs64hfxbinj6lw14s4i6v4g9nzv8w7
        2-helm-diff-3.1.3...
        building '/nix/store/v87c63c026qc7yah4g27hl2mq9wls2v6-source.drv'...
        
        trying https://github.com/jkroepke/helm-secrets/archive/v3.4.1.tar.gz
          % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                         Dload  Upload   Total   Spent    Left  Speed
        100   129  100   129    0     0    648      0 --:--:-- --:--:-- --:--:--   648
        100 46387    0 46387    0     0  87193      0 --:--:-- --:--:-- --:--:-- 87193
        unpacking source archive /build/v3.4.1.tar.gz
        building '/nix/store/1a038ivksyjlbhzyr0nkf3nbdka6mjrv-wire-binaries.drv'...
        building '/nix/store/kc1wh80nmw0zbmfchn6dvavqphay30fd-helm-secrets-3.4.1.drv'...
        unpacking sources
        unpacking source archive /nix/store/5na91i97szlbw2jhw85fblr1s2bd4if0-source
        source root is source
        patching sources
        configuring
        no configure script, doing nothing
        installing
        post-installation fixup
        shrinking RPATHs of ELF executables and libraries in /nix/store/m67aiqqyipxnyq94
        jfq0s43cs3c43rqn-helm-secrets-3.4.1
        strip is /nix/store/cp1sa3xxvl71cypiinw2c62i5s33chlr-binutils-2.35.1/bin/strip
        patching script interpreter paths in /nix/store/m67aiqqyipxnyq94jfq0s43cs3c43rqn
        -helm-secrets-3.4.1
        /nix/store/m67aiqqyipxnyq94jfq0s43cs3c43rqn-helm-secrets-3.4.1/helm-secrets/scri
        pts/.run.sh-wrapped: interpreter directive changed from "#!/usr/bin/env sh" to "
        /nix/store/f7jzmxq9bpbxsg69cszx56mw14n115n5-bash-4.4-p23/bin/sh"
        /nix/store/m67aiqqyipxnyq94jfq0s43cs3c43rqn-helm-secrets-3.4.1/helm-secrets/scri
        pts/commands/downloader.sh: interpreter directive changed from "#!/usr/bin/env s
        h" to "/nix/store/f7jzmxq9bpbxsg69cszx56mw14n115n5-bash-4.4-p23/bin/sh"
        /nix/store/m67aiqqyipxnyq94jfq0s43cs3c43rqn-helm-secrets-3.4.1/helm-secrets/scri
        pts/install.sh: interpreter directive changed from "#!/usr/bin/env sh" to "/nix/
        store/f7jzmxq9bpbxsg69cszx56mw14n115n5-bash-4.4-p23/bin/sh"
        checking for references to /build/ in /nix/store/m67aiqqyipxnyq94jfq0s43cs3c43rq
        n-helm-secrets-3.4.1...
        building '/nix/store/24nqp300d3bg2wswci068fk3xz053lwp-kubeadm.drv'...
        building '/nix/store/92gdp8dnkli136p272fk6xjb0ymj9hl6-helm-plugins.drv'...
        building '/nix/store/m6rywdjmvdrgivx4nwasklml0iwk9ndk-helm-3.5.2.drv'...
        building '/nix/store/k94r99xr01mnq2a2359rd9972fv9s2a7-helmfile-0.138.4.drv'...
        unpacking sources
        unpacking source archive /nix/store/jy4laas6pm5xv1pfaxqpiaa0l1by533b-source
        source root is source
        patching sources
        configuring
        building
        Building subPackage ./.
        internal/unsafeheader
        internal/cpu
        internal/bytealg
        runtime/internal/atomic
        runtime/internal/sys
        runtime/internal/math
        runtime
        internal/reflectlite
        errors
        math/bits
        math
        unicode/utf8
        strconv
        internal/race
        sync/atomic
        sync
        unicode
        reflect
        sort
        internal/fmtsort
        io
        internal/oserror
        syscall
        internal/syscall/unix
        time
        internal/poll
        internal/syscall/execenv
        internal/testlog
        path
        io/fs
        os
        fmt
        bytes
        strings
        bufio
        encoding
        encoding/binary
        encoding/base64
        unicode/utf16
        encoding/json
        github.com/google/go-cmp/cmp/internal/flags
        math/rand
        github.com/google/go-cmp/cmp/internal/diff
        regexp/syntax
        regexp
        github.com/google/go-cmp/cmp/internal/function
        github.com/google/go-cmp/cmp/internal/value
        github.com/google/go-cmp/cmp
        golang.org/x/sys/internal/unsafeheader
        golang.org/x/sys/unix
        github.com/mattn/go-isatty
        github.com/mattn/go-colorable
        github.com/fatih/color
        github.com/mattn/go-runewidth
        github.com/gosuri/uitable/util/strutil
        github.com/gosuri/uitable/util/wordwrap
        github.com/gosuri/uitable
        github.com/imdario/mergo
        hash
        crypto
        crypto/sha1
        encoding/hex
        context
        database/sql/driver
        github.com/Masterminds/semver/v3
        github.com/davecgh/go-spew/spew
        github.com/hashicorp/go-version
        github.com/r3labs/diff
        github.com/roboll/helmfile/pkg/app/version
        github.com/roboll/helmfile/pkg/maputil
        gopkg.in/yaml.v2
        github.com/roboll/helmfile/pkg/environment
        flag
        go.uber.org/atomic
        go.uber.org/multierr
        go.uber.org/zap/buffer
        go.uber.org/zap/internal/bufferpool
        go.uber.org/zap/internal/color
        go.uber.org/zap/internal/exit
        go.uber.org/zap/zapcore
        path/filepath
        io/ioutil
        log
        compress/flate
        hash/crc32
        compress/gzip
        container/list
        crypto/internal/subtle
        crypto/subtle
        crypto/cipher
        crypto/aes
        math/big
        crypto/rand
        crypto/des
        crypto/elliptic
        crypto/internal/randutil
        crypto/sha512
        encoding/asn1
        vendor/golang.org/x/crypto/cryptobyte/asn1
        vendor/golang.org/x/crypto/cryptobyte
        crypto/ecdsa
        crypto/ed25519/internal/edwards25519
        crypto/ed25519
        crypto/hmac
        crypto/md5
        crypto/rc4
        crypto/rsa
        crypto/sha256
        crypto/dsa
        crypto/x509/pkix
        encoding/pem
        vendor/golang.org/x/net/dns/dnsmessage
        internal/nettrace
        internal/singleflight
        runtime/cgo
        net
        net/url
        crypto/x509
        vendor/golang.org/x/crypto/internal/subtle
        vendor/golang.org/x/crypto/chacha20
        vendor/golang.org/x/crypto/poly1305
        vendor/golang.org/x/sys/cpu
        vendor/golang.org/x/crypto/chacha20poly1305
        vendor/golang.org/x/crypto/curve25519
        vendor/golang.org/x/crypto/hkdf
        crypto/tls
        vendor/golang.org/x/text/transform
        vendor/golang.org/x/text/unicode/bidi
        vendor/golang.org/x/text/secure/bidirule
        vendor/golang.org/x/text/unicode/norm
        vendor/golang.org/x/net/idna
        net/textproto
        vendor/golang.org/x/net/http/httpguts
        vendor/golang.org/x/net/http/httpproxy
        vendor/golang.org/x/net/http2/hpack
        mime
        mime/quotedprintable
        mime/multipart
        net/http/httptrace
        net/http/internal
        net/http
        go.uber.org/zap
        os/exec
        github.com/roboll/helmfile/pkg/helmexec
        encoding/base32
        github.com/Masterminds/goutils
        github.com/google/uuid
        github.com/huandu/xstrings
        github.com/mitchellh/reflectwalk
        github.com/mitchellh/copystructure
        html
        text/template/parse
        text/template
        html/template
        github.com/spf13/cast
        golang.org/x/crypto/blowfish
        golang.org/x/crypto/bcrypt
        golang.org/x/crypto/pbkdf2
        golang.org/x/crypto/scrypt
        hash/adler32
        github.com/Masterminds/sprig/v3
        github.com/ghodss/yaml
        github.com/golang/mock/gomock
        github.com/hashicorp/golang-lru/simplelru
        github.com/hashicorp/golang-lru
        github.com/variantdev/vals/pkg/api
        github.com/variantdev/vals/pkg/config
        gopkg.in/yaml.v3
        github.com/variantdev/vals/pkg/expansion
        github.com/aws/aws-sdk-go/aws/awserr
        github.com/aws/aws-sdk-go/internal/ini
        github.com/aws/aws-sdk-go/internal/shareddefaults
        github.com/aws/aws-sdk-go/internal/sync/singleflight
        github.com/aws/aws-sdk-go/aws/credentials
        github.com/aws/aws-sdk-go/aws/endpoints
        github.com/aws/aws-sdk-go/internal/sdkio
        github.com/aws/aws-sdk-go/aws
        github.com/jmespath/go-jmespath
        github.com/aws/aws-sdk-go/aws/awsutil
        github.com/aws/aws-sdk-go/aws/client/metadata
        github.com/aws/aws-sdk-go/aws/request
        github.com/aws/aws-sdk-go/internal/sdkrand
        net/http/httputil
        github.com/aws/aws-sdk-go/aws/client
        github.com/aws/aws-sdk-go/internal/strings
        github.com/aws/aws-sdk-go/internal/sdkmath
        github.com/aws/aws-sdk-go/private/protocol
        github.com/aws/aws-sdk-go/private/protocol/rest
        github.com/aws/aws-sdk-go/aws/signer/v4
        github.com/aws/aws-sdk-go/private/protocol/json/jsonutil
        github.com/aws/aws-sdk-go/private/protocol/jsonrpc
        github.com/aws/aws-sdk-go/service/secretsmanager
        encoding/xml
        github.com/aws/aws-sdk-go/private/protocol/query/queryutil
        github.com/aws/aws-sdk-go/private/protocol/xml/xmlutil
        github.com/aws/aws-sdk-go/private/protocol/query
        github.com/aws/aws-sdk-go/service/sts
        github.com/aws/aws-sdk-go/service/sts/stsiface
        github.com/aws/aws-sdk-go/aws/credentials/stscreds
        github.com/aws/aws-sdk-go/aws/corehandlers
        github.com/aws/aws-sdk-go/aws/credentials/processcreds
        github.com/aws/aws-sdk-go/aws/csm
        github.com/aws/aws-sdk-go/internal/sdkuri
        github.com/aws/aws-sdk-go/aws/ec2metadata
        github.com/aws/aws-sdk-go/aws/credentials/ec2rolecreds
        github.com/aws/aws-sdk-go/aws/credentials/endpointcreds
        github.com/aws/aws-sdk-go/aws/defaults
        github.com/aws/aws-sdk-go/aws/session
        github.com/variantdev/vals/pkg/awsclicompat
        github.com/variantdev/vals/pkg/providers/awssecrets
        github.com/Azure/azure-sdk-for-go/version
        github.com/Azure/go-autorest/autorest/date
        github.com/Azure/go-autorest/tracing
        github.com/dgrijalva/jwt-go
        net/http/cookiejar
        github.com/Azure/go-autorest/autorest/adal
        github.com/Azure/go-autorest/logger
        github.com/Azure/go-autorest/autorest
        github.com/Azure/go-autorest/autorest/azure
        github.com/Azure/go-autorest/autorest/to
        github.com/Azure/go-autorest/autorest/validation
        github.com/Azure/azure-sdk-for-go/services/keyvault/2016-10-01/keyvault
        github.com/Azure/azure-sdk-for-go/profiles/latest/keyvault/keyvault
        github.com/dimchansky/utfbom
        github.com/mitchellh/go-homedir
        github.com/Azure/go-autorest/autorest/azure/cli
        golang.org/x/crypto/pkcs12/internal/rc2
        golang.org/x/crypto/pkcs12
        github.com/Azure/go-autorest/autorest/azure/auth
        github.com/variantdev/vals/pkg/providers/azurekeyvault
        github.com/variantdev/vals/pkg/providers/echo
        github.com/variantdev/vals/pkg/providers/file
        golang.org/x/net/internal/timeseries
        text/tabwriter
        golang.org/x/net/trace
        google.golang.org/grpc/backoff
        google.golang.org/grpc/internal/grpclog
        google.golang.org/grpc/grpclog
        google.golang.org/grpc/connectivity
        hash/fnv
        google.golang.org/protobuf/internal/detrand
        google.golang.org/protobuf/internal/errors
        google.golang.org/protobuf/encoding/protowire
        google.golang.org/protobuf/internal/pragma
        google.golang.org/protobuf/reflect/protoreflect
        google.golang.org/protobuf/reflect/protoregistry
        google.golang.org/protobuf/internal/encoding/messageset
        google.golang.org/protobuf/internal/flags
        go/token
        google.golang.org/protobuf/internal/strs
        google.golang.org/protobuf/internal/encoding/text
        google.golang.org/protobuf/internal/genid
        google.golang.org/protobuf/internal/mapsort
        google.golang.org/protobuf/internal/set
        google.golang.org/protobuf/internal/fieldsort
        google.golang.org/protobuf/runtime/protoiface
        google.golang.org/protobuf/proto
        google.golang.org/protobuf/encoding/prototext
        google.golang.org/protobuf/internal/descfmt
        google.golang.org/protobuf/internal/descopts
        google.golang.org/protobuf/internal/encoding/defval
        google.golang.org/protobuf/internal/filedesc
        google.golang.org/protobuf/internal/encoding/tag
        google.golang.org/protobuf/internal/impl
        google.golang.org/protobuf/internal/filetype
        google.golang.org/protobuf/internal/version
        google.golang.org/protobuf/runtime/protoimpl
        github.com/golang/protobuf/proto
        google.golang.org/grpc/attributes
        google.golang.org/grpc/serviceconfig
        google.golang.org/grpc/internal
        google.golang.org/grpc/internal/credentials
        google.golang.org/grpc/credentials
        google.golang.org/grpc/metadata
        google.golang.org/grpc/resolver
        google.golang.org/grpc/balancer
        google.golang.org/grpc/balancer/base
        google.golang.org/grpc/internal/grpcrand
        google.golang.org/grpc/balancer/roundrobin
        google.golang.org/grpc/codes
        google.golang.org/grpc/encoding
        google.golang.org/grpc/encoding/proto
        google.golang.org/grpc/internal/backoff
        google.golang.org/grpc/internal/balancerload
        google.golang.org/protobuf/types/known/anypb
        github.com/golang/protobuf/ptypes/any
        google.golang.org/protobuf/types/known/durationpb
        github.com/golang/protobuf/ptypes/duration
        google.golang.org/protobuf/types/known/timestamppb
        github.com/golang/protobuf/ptypes/timestamp
        github.com/golang/protobuf/ptypes
        google.golang.org/grpc/binarylog/grpc_binarylog_v1
        google.golang.org/grpc/internal/grpcutil
        google.golang.org/genproto/googleapis/rpc/status
        google.golang.org/grpc/internal/status
        google.golang.org/grpc/status
        google.golang.org/grpc/internal/binarylog
        google.golang.org/grpc/internal/buffer
        google.golang.org/grpc/internal/channelz
        google.golang.org/grpc/internal/envconfig
        google.golang.org/grpc/internal/grpcsync
        google.golang.org/grpc/balancer/grpclb/state
        google.golang.org/grpc/internal/resolver/dns
        google.golang.org/grpc/internal/resolver/passthrough
        google.golang.org/grpc/internal/serviceconfig
        golang.org/x/text/transform
        golang.org/x/text/unicode/bidi
        golang.org/x/text/secure/bidirule
        golang.org/x/text/unicode/norm
        golang.org/x/net/idna
        golang.org/x/net/http/httpguts
        golang.org/x/net/http2/hpack
        golang.org/x/net/http2
        google.golang.org/grpc/internal/syscall
        google.golang.org/grpc/keepalive
        google.golang.org/grpc/peer
        google.golang.org/grpc/stats
        google.golang.org/grpc/tap
        google.golang.org/grpc/internal/transport
        google.golang.org/grpc
        github.com/googleapis/gax-go/v2
        google.golang.org/protobuf/types/descriptorpb
        google.golang.org/genproto/googleapis/api/annotations
        google.golang.org/genproto/googleapis/type/expr
        google.golang.org/genproto/googleapis/iam/v1
        cloud.google.com/go/iam
        google.golang.org/api/iterator
        golang.org/x/net/context/ctxhttp
        golang.org/x/oauth2/internal
        golang.org/x/oauth2
        cloud.google.com/go/compute/metadata
        golang.org/x/oauth2/jws
        golang.org/x/oauth2/jwt
        os/user
        golang.org/x/oauth2/google
        google.golang.org/api/internal/impersonate
        google.golang.org/api/internal
        google.golang.org/api/option
        go.opencensus.io/resource
        go.opencensus.io/metric/metricdata
        runtime/pprof
        go.opencensus.io/tag
        go.opencensus.io/stats/internal
        go.opencensus.io/stats
        go.opencensus.io/internal/tagencoding
        go.opencensus.io/metric/metricproducer
        go.opencensus.io/stats/view
        github.com/golang/groupcache/lru
        go.opencensus.io
        go.opencensus.io/internal
        go.opencensus.io/trace/internal
        go.opencensus.io/trace/tracestate
        runtime/trace
        go.opencensus.io/trace
        go.opencensus.io/trace/propagation
        go.opencensus.io/plugin/ocgrpc
        google.golang.org/api/transport/cert
        google.golang.org/api/transport/internal/dca
        google.golang.org/grpc/balancer/grpclb/grpc_lb_v1
        google.golang.org/grpc/balancer/grpclb
        google.golang.org/grpc/credentials/alts/internal
        google.golang.org/grpc/credentials/alts/internal/proto/grpc_gcp
        google.golang.org/grpc/credentials/alts/internal/authinfo
        google.golang.org/grpc/credentials/alts/internal/conn
        google.golang.org/grpc/credentials/alts/internal/handshaker
        google.golang.org/grpc/credentials/alts/internal/handshaker/service
        google.golang.org/grpc/credentials/alts
        google.golang.org/grpc/credentials/oauth
        google.golang.org/grpc/credentials/google
        google.golang.org/api/transport/grpc
        google.golang.org/protobuf/types/known/emptypb
        google.golang.org/protobuf/types/known/fieldmaskpb
        google.golang.org/genproto/googleapis/cloud/secretmanager/v1beta1
        cloud.google.com/go/secretmanager/apiv1beta1
        github.com/variantdev/vals/pkg/providers/gcpsecrets
        github.com/aws/aws-sdk-go/aws/arn
        github.com/aws/aws-sdk-go/internal/s3shared/arn
        github.com/aws/aws-sdk-go/internal/s3shared
        github.com/aws/aws-sdk-go/internal/s3shared/s3err
        github.com/aws/aws-sdk-go/private/checksum
        github.com/aws/aws-sdk-go/private/protocol/eventstream
        github.com/aws/aws-sdk-go/private/protocol/eventstream/eventstreamapi
        github.com/aws/aws-sdk-go/private/protocol/restxml
        github.com/aws/aws-sdk-go/service/s3
        github.com/aws/aws-sdk-go/service/s3/s3iface
        github.com/variantdev/vals/pkg/providers/s3
        github.com/sirupsen/logrus
        github.com/goware/prefixer
        github.com/mitchellh/go-wordwrap
        database/sql
        github.com/lib/pq/oid
        github.com/lib/pq/scram
        github.com/lib/pq
        github.com/mozilla-services/yaml
        github.com/pkg/errors
        go.mozilla.org/sops/v3/logging
        go.mozilla.org/sops/v3/audit
        go.mozilla.org/sops/v3/keys
        go.mozilla.org/sops/v3/azkv
        golang.org/x/net/context
        google.golang.org/api/internal/third_party/uritemplates
        google.golang.org/api/googleapi
        google.golang.org/api/internal/gensupport
        google.golang.org/api/option/internaloption
        go.opencensus.io/plugin/ochttp/propagation/b3
        go.opencensus.io/plugin/ochttp
        google.golang.org/api/googleapi/transport
        google.golang.org/api/transport/http/internal/propagation
        google.golang.org/api/transport/http
        google.golang.org/api/cloudkms/v1
        go.mozilla.org/sops/v3/gcpkms
        github.com/hashicorp/errwrap
        github.com/hashicorp/go-cleanhttp
        github.com/hashicorp/go-multierror
        github.com/hashicorp/go-hclog
        github.com/hashicorp/go-retryablehttp
        github.com/hashicorp/go-rootcerts
        github.com/hashicorp/hcl/hcl/strconv
        github.com/hashicorp/hcl/hcl/token
        github.com/hashicorp/hcl/hcl/ast
        github.com/hashicorp/hcl/hcl/scanner
        github.com/hashicorp/hcl/hcl/parser
        github.com/hashicorp/hcl/json/token
        github.com/hashicorp/hcl/json/scanner
        github.com/hashicorp/hcl/json/parser
        github.com/hashicorp/hcl
        github.com/hashicorp/vault/sdk/helper/consts
        github.com/hashicorp/vault/sdk/helper/hclutil
        compress/lzw
        github.com/golang/snappy
        github.com/pierrec/lz4/internal/xxh32
        runtime/debug
        github.com/pierrec/lz4
        github.com/hashicorp/vault/sdk/helper/compressutil
        github.com/hashicorp/vault/sdk/helper/jsonutil
        github.com/hashicorp/go-sockaddr
        github.com/ryanuber/go-glob
        github.com/hashicorp/vault/sdk/helper/strutil
        github.com/mitchellh/mapstructure
        github.com/hashicorp/vault/sdk/helper/parseutil
        golang.org/x/time/rate
        golang.org/x/crypto/ed25519
        gopkg.in/square/go-jose.v2/cipher
        gopkg.in/square/go-jose.v2/json
        gopkg.in/square/go-jose.v2
        gopkg.in/square/go-jose.v2/jwt
        github.com/hashicorp/vault/api
        go.mozilla.org/sops/v3/hcvault
        github.com/aws/aws-sdk-go/service/kms
        github.com/aws/aws-sdk-go/service/kms/kmsiface
        go.mozilla.org/sops/v3/kms
        golang.org/x/crypto/ssh/terminal
        github.com/howeyc/gopass
        go.mozilla.org/gopgagent
        golang.org/x/crypto/openpgp/errors
        golang.org/x/crypto/openpgp/armor
        compress/bzip2
        compress/zlib
        golang.org/x/crypto/cast5
        golang.org/x/crypto/openpgp/elgamal
        golang.org/x/crypto/openpgp/s2k
        image/color
        image
        image/internal/imageutil
        image/jpeg
        golang.org/x/crypto/openpgp/packet
        golang.org/x/crypto/openpgp
        go.mozilla.org/sops/v3/pgp
        go.mozilla.org/sops/v3/keyservice
        go.mozilla.org/sops/v3/shamir
        go.mozilla.org/sops/v3
        go.mozilla.org/sops/v3/aes
        go.mozilla.org/sops/v3/cmd/sops/codes
        go.mozilla.org/sops/v3/cmd/sops/formats
        go.mozilla.org/sops/v3/stores
        go.mozilla.org/sops/v3/stores/dotenv
        gopkg.in/ini.v1
        go.mozilla.org/sops/v3/stores/ini
        go.mozilla.org/sops/v3/stores/json
        go.mozilla.org/sops/v3/stores/yaml
        github.com/blang/semver
        gopkg.in/urfave/cli.v1
        go.mozilla.org/sops/v3/version
        go.mozilla.org/sops/v3/cmd/sops/common
        go.mozilla.org/sops/v3/decrypt
        github.com/variantdev/vals/pkg/providers/sops
        github.com/aws/aws-sdk-go/service/ssm
        github.com/aws/aws-sdk-go/service/ssm/ssmiface
        github.com/variantdev/vals/pkg/providers/ssm
        text/scanner
        github.com/alecthomas/participle/lexer
        github.com/alecthomas/participle
        github.com/lestrrat-go/strftime
        github.com/pbnjay/strptime
        github.com/itchyny/gojq
        github.com/fujiwara/tfstate-lookup/tfstate
        github.com/variantdev/vals/pkg/providers/tfstate
        github.com/variantdev/vals/pkg/providers/vault
        github.com/variantdev/vals/pkg/stringmapprovider
        github.com/variantdev/vals/pkg/stringprovider
        github.com/variantdev/vals
        github.com/roboll/helmfile/pkg/plugins
        golang.org/x/sync/errgroup
        github.com/roboll/helmfile/pkg/tmpl
        github.com/roboll/helmfile/pkg/event
        archive/tar
        archive/zip
        cloud.google.com/go/internal
        cloud.google.com/go/internal/optional
        google.golang.org/genproto/googleapis/rpc/code
        cloud.google.com/go/internal/trace
        cloud.google.com/go/internal/version
        google.golang.org/api/storage/v1
        cloud.google.com/go/storage
        github.com/bgentry/go-netrc/netrc
        github.com/hashicorp/go-getter/helper/url
        github.com/hashicorp/go-safetemp
        github.com/mitchellh/go-testing-interface
        github.com/ulikunitz/xz/internal/xlog
        github.com/ulikunitz/xz/internal/hash
        github.com/ulikunitz/xz/lzma
        hash/crc64
        github.com/ulikunitz/xz
        github.com/hashicorp/go-getter
        github.com/roboll/helmfile/pkg/remote
        github.com/tatsushid/go-prettytable
        github.com/otiai10/copy
        github.com/variantdev/chartify
        github.com/variantdev/dag/pkg/dag
        github.com/roboll/helmfile/pkg/state
        github.com/roboll/helmfile/pkg/argparser
        os/signal
        github.com/roboll/helmfile/pkg/app
        github.com/shurcooL/sanitized_anchor_name
        github.com/russross/blackfriday/v2
        github.com/cpuguy83/go-md2man/v2/md2man
        github.com/urfave/cli
        github.com/roboll/helmfile
        installing
        post-installation fixup
        shrinking RPATHs of ELF executables and libraries in /nix/store/ha6sggy1rxk4wsmf
        4nzll7pz4bkq0v1f-helmfile-0.138.4
        shrinking /nix/store/ha6sggy1rxk4wsmf4nzll7pz4bkq0v1f-helmfile-0.138.4/bin/.helm
        file-wrapped
        strip is /nix/store/cp1sa3xxvl71cypiinw2c62i5s33chlr-binutils-2.35.1/bin/strip
        stripping (with command strip and flags -S) in /nix/store/ha6sggy1rxk4wsmf4nzll7
        pz4bkq0v1f-helmfile-0.138.4/bin
        patching script interpreter paths in /nix/store/ha6sggy1rxk4wsmf4nzll7pz4bkq0v1f
        -helmfile-0.138.4
        checking for references to /build/ in /nix/store/ha6sggy1rxk4wsmf4nzll7pz4bkq0v1
        f-helmfile-0.138.4...
        building '/nix/store/hvvmxsr08007l1h9ws069lk00qfi7b4m-list-helm-containers.drv'.
        ..
        building '/nix/store/anlifb5929h4vxfx2z2y2b1d9vklsk76-wire-server-deploy.drv'...
        created 745 symlinks in user environment
        /nix/store/1iwi052fr70hdv02n2dr9d094lk5nrlr-wire-server-deploy
        direnv: loading .nix-env/.profile
        direnv: export +LOCALHOST_PYTHON ~NIX_PATH ~PATH
[20:37:48] # Running command «ansible --version» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy$ ansible --version
        ansible 2.9.12
          config file = None
          configured module search path = ['/home/wire/.ansible/plugins/modules', '/usr/
        share/ansible/plugins/modules']
          ansible python module location = /nix/store/407c5v2x920kri79w0sb9gb1g82q28mg-p
        ython3.8-ansible-2.9.12/lib/python3.8/site-packages/ansible
          executable location = /nix/store/407c5v2x920kri79w0sb9gb1g82q28mg-python3.8-an
        sible-2.9.12/bin/ansible
          python version = 3.8.8 (default, Feb 19 2021, 11:04:50) [GCC 10.2.0]
[20:37:54] # Running command «cd ansible» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy$ cd ansible
[20:37:59] # Running command «pwd» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ pwd
        /home/wire/wire-server-deploy/ansible
[20:38:05] # Running command «cp inventory/demo/hosts.example.ini inventory/demo/hosts.ini» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ cp inventory/demo/hosts.example.i
        ni inventory/demo/hosts.ini
[20:38:11] # Running command «ls -l inventory/demo/hosts.ini» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ ls -l inventory/demo/hosts.ini
        -rw-rw-r-- 1 wire wire 1066 Jul 15 18:38 inventory/demo/hosts.ini
[20:38:17] # Running command «sed -i 's/X.X.X.X/95.216.208.159/g' inventory/demo/hosts.ini» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ sed -i 's/X.X.X.X/95.216.208.159/
        g' inventory/demo/hosts.ini
[20:38:23] # Running command «cat inventory/demo/hosts.ini | grep ansible_host» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ cat inventory/demo/hosts.ini | gr
        ep ansible_host
        # * 'ansible_host' is the IP to ssh into
        kubenode01    ansible_host=95.216.208.159 etcd_member_name=etcd1
[20:38:28] # Running command «ssh-keygen -f ~/.ssh/id_rsa -t rsa -P ''» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ ssh-keygen -f ~/.ssh/id_rsa -t rs
        a -P ''
        Generating public/private rsa key pair.
        Your identification has been saved in /home/wire/.ssh/id_rsa.
        Your public key has been saved in /home/wire/.ssh/id_rsa.pub.
        The key fingerprint is:
        SHA256:VSp7QG3jURZTlQDAo7yNgTsX1r/LaW4b8UcD3EdX6kQ wire@wire-client
        The key's randomart image is:
        +---[RSA 2048]----+
        |        oo.oB+E.B|
        |       . o=+.o.+.|
        |      o =o+o oo..|
        |     . * *.  o. .|
        |      o S o.  .o |
        |     o + o .o . .|
        |      o    ... . |
        |          .+o .  |
        |          +*o    |
        +----[SHA256]-----+
[20:38:40] # Running command «ssh-copy-id wire@95.216.208.159» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ ssh-copy-id wire@95.216.208.159
        /usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/home/wire/.ssh/i
        d_rsa.pub"
        The authenticity of host '95.216.208.159 (95.216.208.159)' can't be established.
        ECDSA key fingerprint is SHA256:9P/4B+/vRqo+/k0fTlfmA5352gh69Y6MF+4VVwIcxXM.
        Are you sure you want to continue connecting (yes/no)? yes
        /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter
        out any that are already installed
        /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompt
        ed now it is to install the new keys
        wire@95.216.208.159's password:
        
        Number of key(s) added: 1
        
        Now try logging into the machine, with:   "ssh 'wire@95.216.208.159'"
        and check to make sure that only the key(s) you wanted were added.
        
[20:38:45] # Running command «cat /etc/hostname» on 95.216.208.159
        wire@arthur-demo:~$ cat /etc/hostname
        kubenode01
[20:38:50] # Running command «sudo tail -n 2 /etc/sudoers» on 95.216.208.159
        wire@arthur-demo:~$ sudo tail -n 2 /etc/sudoers
        wire ALL=(ALL) NOPASSWD:ALL
        wire ALL=(ALL) NOPASSWD:ALL
[20:38:55] # Running command «echo 'wire ALL=(ALL) NOPASSWD:ALL' | sudo tee -a /etc/sudoers» on 95.216.208.159
        wire@arthur-demo:~$ echo 'wire ALL=(ALL) NOPASSWD:ALL' | sudo tee -a /etc/sudoer
        s
        wire ALL=(ALL) NOPASSWD:ALL
[20:39:00] # Running command «sudo tail -n 2 /etc/sudoers» on 95.216.208.159
        wire@arthur-demo:~$ sudo tail -n 2 /etc/sudoers
        wire ALL=(ALL) NOPASSWD:ALL
        wire ALL=(ALL) NOPASSWD:ALL
[20:39:06] # Running command «cat inventory/demo/hosts.ini | grep ansible_user» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ cat inventory/demo/hosts.ini | gr
        ep ansible_user
        # ansible_user = ...
[20:39:12] # Running command «sed -i 's/# ansible_user = .../ansible_user = wire/g' inventory/demo/hosts.ini» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ sed -i 's/# ansible_user = .../an
        sible_user = wire/g' inventory/demo/hosts.ini
[20:39:18] # Running command «cat inventory/demo/hosts.ini | grep ansible_user» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ cat inventory/demo/hosts.ini | gr
        ep ansible_user
        ansible_user = wire
[20:45:23] # Running command «ansible-playbook -i inventory/demo/hosts.ini kubernetes.yml -vv» on 192.168.1.121
        TASK [kubernetes-apps/csi_driver/vsphere : vSphere CSI Driver | Generate a CSI s
        ecret manifest] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/csi_driver/vsphere/tasks/main.yml:27
        skipping: [kubenode01] => {"censored": "the output has been hidden due to the fa
        ct that 'no_log: true' was specified for this result", "changed": false}
        
        TASK [kubernetes-apps/csi_driver/vsphere : vSphere CSI Driver | Apply a CSI secr
        et manifest] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/csi_driver/vsphere/tasks/main.yml:34
        skipping: [kubenode01] => {"censored": "the output has been hidden due to the fa
        ct that 'no_log: true' was specified for this result", "changed": false}
        
        TASK [kubernetes-apps/csi_driver/vsphere : vSphere CSI Driver | Apply Manifests]
         ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/csi_driver/vsphere/tasks/main.yml:42
        skipping: [kubenode01] => (item=vsphere-csi-controller-rbac.yml)  => {"ansible_l
        oop_var": "item", "changed": false, "item": {"ansible_loop_var": "item", "change
        d": false, "item": "vsphere-csi-controller-rbac.yml", "skip_reason": "Conditiona
        l result was False", "skipped": true}, "skip_reason": "Conditional result was Fa
        lse"}
        skipping: [kubenode01] => (item=vsphere-csi-controller-ss.yml)  => {"ansible_loo
        p_var": "item", "changed": false, "item": {"ansible_loop_var": "item", "changed"
        : false, "item": "vsphere-csi-controller-ss.yml", "skip_reason": "Conditional re
        sult was False", "skipped": true}, "skip_reason": "Conditional result was False"
        }
        skipping: [kubenode01] => (item=vsphere-csi-node.yml)  => {"ansible_loop_var": "
        item", "changed": false, "item": {"ansible_loop_var": "item", "changed": false,
        "item": "vsphere-csi-node.yml", "skip_reason": "Conditional result was False", "
        skipped": true}, "skip_reason": "Conditional result was False"}
        
        TASK [kubernetes-apps/persistent_volumes/openstack : Kubernetes Persistent Volum
        es | Lay down OpenStack Cinder Storage Class template] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/persistent_volumes/openstack/tasks/main.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/persistent_volumes/openstack : Kubernetes Persistent Volum
        es | Add OpenStack Cinder Storage Class] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/persistent_volumes/openstack/tasks/main.yml:10
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/persistent_volumes/cinder-csi : Kubernetes Persistent Volu
        mes | Copy Cinder CSI Storage Class template] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/persistent_volumes/cinder-csi/tasks/main.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/persistent_volumes/cinder-csi : Kubernetes Persistent Volu
        mes | Add Cinder CSI Storage Class] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/persistent_volumes/cinder-csi/tasks/main.yml:10
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/persistent_volumes/aws-ebs-csi : Kubernetes Persistent Vol
        umes | Copy AWS EBS CSI Storage Class template] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/persistent_volumes/aws-ebs-csi/tasks/main.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/persistent_volumes/aws-ebs-csi : Kubernetes Persistent Vol
        umes | Add AWS EBS CSI Storage Class] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/persistent_volumes/aws-ebs-csi/tasks/main.yml:10
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/persistent_volumes/azuredisk-csi : Kubernetes Persistent V
        olumes | Copy Azure CSI Storage Class template] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/persistent_volumes/azuredisk-csi/tasks/main.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/persistent_volumes/azuredisk-csi : Kubernetes Persistent V
        olumes | Add Azure CSI Storage Class] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/persistent_volumes/azuredisk-csi/tasks/main.yml:10
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/persistent_volumes/gcp-pd-csi : Kubernetes Persistent Volu
        mes | Copy GCP PD CSI Storage Class template] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/persistent_volumes/gcp-pd-csi/tasks/main.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/persistent_volumes/gcp-pd-csi : Kubernetes Persistent Volu
        mes | Add GCP PD CSI Storage Class] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/persistent_volumes/gcp-pd-csi/tasks/main.yml:10
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/snapshots/snapshot-controller : Snapshot Controller | Gene
        rate Manifests] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/snapshots/snapshot-controller/tasks/main.yml:2
        skipping: [kubenode01] => (item={'name': 'rbac-snapshot-controller', 'file': 'rb
        ac-snapshot-controller.yml'})  => {"ansible_loop_var": "item", "changed": false,
         "item": {"file": "rbac-snapshot-controller.yml", "name": "rbac-snapshot-control
        ler"}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'name': 'snapshot-controller', 'file': 'snapsho
        t-controller.yml'})  => {"ansible_loop_var": "item", "changed": false, "item": {
        "file": "snapshot-controller.yml", "name": "snapshot-controller"}, "skip_reason"
        : "Conditional result was False"}
        
        TASK [kubernetes-apps/snapshots/snapshot-controller : Snapshot Controller | Appl
        y Manifests] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/snapshots/snapshot-controller/tasks/main.yml:13
        skipping: [kubenode01] => (item=rbac-snapshot-controller.yml)  => {"ansible_loop
        _var": "item", "changed": false, "item": {"ansible_loop_var": "item", "changed":
         false, "item": {"file": "rbac-snapshot-controller.yml", "name": "rbac-snapshot-
        controller"}, "skip_reason": "Conditional result was False", "skipped": true}, "
        skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item=snapshot-controller.yml)  => {"ansible_loop_var"
        : "item", "changed": false, "item": {"ansible_loop_var": "item", "changed": fals
        e, "item": {"file": "snapshot-controller.yml", "name": "snapshot-controller"}, "
        skip_reason": "Conditional result was False", "skipped": true}, "skip_reason": "
        Conditional result was False"}
        
        TASK [kubernetes-apps/snapshots/cinder-csi : Kubernetes Snapshots | Copy Cinder
        CSI Snapshot Class template] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/snapshots/cinder-csi/tasks/main.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/snapshots/cinder-csi : Kubernetes Snapshots | Add Cinder C
        SI Snapshot Class] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/snapshots/cinder-csi/tasks/main.yml:10
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/container_runtimes/kata_containers : Kata Containers | Cre
        ate addon dir] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/container_runtimes/kata_containers/tasks/main.yaml:3
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/container_runtimes/kata_containers : Kata Containers | Tem
        plates list] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/container_runtimes/kata_containers/tasks/main.yaml:11
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/container_runtimes/kata_containers : Kata Containers | Cre
        ate manifests] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/container_runtimes/kata_containers/tasks/main.yaml:16
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/container_runtimes/kata_containers : Kata Containers | App
        ly manifests] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/container_runtimes/kata_containers/tasks/main.yaml:25
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/container_runtimes/crun : crun | Copy runtime class manife
        st] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/container_runtimes/crun/tasks/main.yaml:3
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/container_runtimes/crun : crun | Apply manifests] ********
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/container_runtimes/crun/tasks/main.yaml:11
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/container_engine_accelerator/nvidia_gpu : Container Engine
         Acceleration Nvidia GPU| gather os specific variables] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/container_engine_accelerator/nvidia_gpu/tasks/main.yml:3
        skipping: [kubenode01] => (item=/home/wire/wire-server-deploy/ansible/roles-exte
        rnal/kubespray/roles/kubernetes-apps/container_engine_accelerator/nvidia_gpu/var
        s/ubuntu-18.yml)  => {"ansible_loop_var": "item", "changed": false, "item": "/ho
        me/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/kubernetes-app
        s/container_engine_accelerator/nvidia_gpu/vars/ubuntu-18.yml", "skip_reason": "C
        onditional result was False"}
        
        TASK [kubernetes-apps/container_engine_accelerator/nvidia_gpu : Container Engine
         Acceleration Nvidia GPU | Set fact of download url Tesla] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/container_engine_accelerator/nvidia_gpu/tasks/main.yml:14
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/container_engine_accelerator/nvidia_gpu : Container Engine
         Acceleration Nvidia GPU | Set fact of download url GTX] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/container_engine_accelerator/nvidia_gpu/tasks/main.yml:19
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/container_engine_accelerator/nvidia_gpu : Container Engine
         Acceleration Nvidia GPU | Create addon dir] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/container_engine_accelerator/nvidia_gpu/tasks/main.yml:24
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/container_engine_accelerator/nvidia_gpu : Container Engine
         Acceleration Nvidia GPU | Create manifests for nvidia accelerators] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/container_engine_accelerator/nvidia_gpu/tasks/main.yml:32
        skipping: [kubenode01] => (item={'name': 'nvidia-driver-install-daemonset', 'fil
        e': 'nvidia-driver-install-daemonset.yml', 'type': 'daemonset'})  => {"ansible_l
        oop_var": "item", "changed": false, "item": {"file": "nvidia-driver-install-daem
        onset.yml", "name": "nvidia-driver-install-daemonset", "type": "daemonset"}, "sk
        ip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'name': 'k8s-device-plugin-nvidia-daemonset', '
        file': 'k8s-device-plugin-nvidia-daemonset.yml', 'type': 'daemonset'})  => {"ans
        ible_loop_var": "item", "changed": false, "item": {"file": "k8s-device-plugin-nv
        idia-daemonset.yml", "name": "k8s-device-plugin-nvidia-daemonset", "type": "daem
        onset"}, "skip_reason": "Conditional result was False"}
        
        TASK [kubernetes-apps/container_engine_accelerator/nvidia_gpu : Container Engine
         Acceleration Nvidia GPU | Apply manifests for nvidia accelerators] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/container_engine_accelerator/nvidia_gpu/tasks/main.yml:43
        skipping: [kubenode01] => (item={'changed': False, 'skipped': True, 'skip_reason
        ': 'Conditional result was False', 'item': {'name': 'nvidia-driver-install-daemo
        nset', 'file': 'nvidia-driver-install-daemonset.yml', 'type': 'daemonset'}, 'ans
        ible_loop_var': 'item'})  => {"ansible_loop_var": "item", "changed": false, "ite
        m": {"ansible_loop_var": "item", "changed": false, "item": {"file": "nvidia-driv
        er-install-daemonset.yml", "name": "nvidia-driver-install-daemonset", "type": "d
        aemonset"}, "skip_reason": "Conditional result was False", "skipped": true}, "sk
        ip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'changed': False, 'skipped': True, 'skip_reason
        ': 'Conditional result was False', 'item': {'name': 'k8s-device-plugin-nvidia-da
        emonset', 'file': 'k8s-device-plugin-nvidia-daemonset.yml', 'type': 'daemonset'}
        , 'ansible_loop_var': 'item'})  => {"ansible_loop_var": "item", "changed": false
        , "item": {"ansible_loop_var": "item", "changed": false, "item": {"file": "k8s-d
        evice-plugin-nvidia-daemonset.yml", "name": "k8s-device-plugin-nvidia-daemonset"
        , "type": "daemonset"}, "skip_reason": "Conditional result was False", "skipped"
        : true}, "skip_reason": "Conditional result was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Credentials
        Check | oci_private_key] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/cloud_controller/oci/tasks/credentials-check.yml:3
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Credentials
        Check | oci_region_id] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/cloud_controller/oci/tasks/credentials-check.yml:10
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Credentials
        Check | oci_tenancy_id] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/cloud_controller/oci/tasks/credentials-check.yml:17
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Credentials
        Check | oci_user_id] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/cloud_controller/oci/tasks/credentials-check.yml:24
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Credentials
        Check | oci_user_fingerprint] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/cloud_controller/oci/tasks/credentials-check.yml:31
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Credentials
        Check | oci_compartment_id] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/cloud_controller/oci/tasks/credentials-check.yml:38
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Credentials
        Check | oci_vnc_id] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/cloud_controller/oci/tasks/credentials-check.yml:44
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Credentials
        Check | oci_subnet1_id] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/cloud_controller/oci/tasks/credentials-check.yml:50
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Credentials
        Check | oci_subnet2_id] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/cloud_controller/oci/tasks/credentials-check.yml:56
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Credentials
        Check | oci_security_list_management] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/cloud_controller/oci/tasks/credentials-check.yml:63
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Generate Clo
        ud Provider Configuration] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/cloud_controller/oci/tasks/main.yml:6
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Slurp Config
        uration] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/cloud_controller/oci/tasks/main.yml:13
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Encode Confi
        guration] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/cloud_controller/oci/tasks/main.yml:18
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Generate Man
        ifests] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/cloud_controller/oci/tasks/main.yml:24
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/cloud_controller/oci : OCI Cloud Controller | Apply Manife
        sts] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/cloud_controller/oci/tasks/main.yml:31
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/metallb : Kubernetes Apps | Check cluster settings for Met
        alLB] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/metallb/tasks/main.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/metallb : Kubernetes Apps | Check cluster settings for Met
        alLB] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/metallb/tasks/main.yml:8
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/metallb : Kubernetes Apps | Check BGP peers for MetalLB] *
        **
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/metallb/tasks/main.yml:14
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/metallb : Kubernetes Apps | Check AppArmor status] *******
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/metallb/tasks/main.yml:20
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/metallb : Kubernetes Apps | Set apparmor_enabled] ********
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/metallb/tasks/main.yml:28
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/metallb : Kubernetes Apps | Lay Down MetalLB] ************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/metallb/tasks/main.yml:35
        skipping: [kubenode01] => (item=metallb.yml)  => {"ansible_loop_var": "item", "c
        hanged": false, "item": "metallb.yml", "skip_reason": "Conditional result was Fa
        lse"}
        skipping: [kubenode01] => (item=metallb-config.yml)  => {"ansible_loop_var": "it
        em", "changed": false, "item": "metallb-config.yml", "skip_reason": "Conditional
         result was False"}
        
        TASK [kubernetes-apps/metallb : Kubernetes Apps | Install and configure MetalLB]
         ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/metallb/tasks/main.yml:43
        skipping: [kubenode01] => (item={'changed': False, 'skipped': True, 'skip_reason
        ': 'Conditional result was False', 'item': 'metallb.yml', 'ansible_loop_var': 'i
        tem'})  => {"ansible_loop_var": "item", "changed": false, "item": {"ansible_loop
        _var": "item", "changed": false, "item": "metallb.yml", "skip_reason": "Conditio
        nal result was False", "skipped": true}, "skip_reason": "Conditional result was
        False"}
        skipping: [kubenode01] => (item={'changed': False, 'skipped': True, 'skip_reason
        ': 'Conditional result was False', 'item': 'metallb-config.yml', 'ansible_loop_v
        ar': 'item'})  => {"ansible_loop_var": "item", "changed": false, "item": {"ansib
        le_loop_var": "item", "changed": false, "item": "metallb-config.yml", "skip_reas
        on": "Conditional result was False", "skipped": true}, "skip_reason": "Condition
        al result was False"}
        
        TASK [kubernetes-apps/metallb : Kubernetes Apps | Check existing secret of Metal
        LB] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/metallb/tasks/main.yml:54
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/metallb : Kubernetes Apps | Create random bytes for MetalL
        B] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/metallb/tasks/main.yml:62
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes-apps/metallb : Kubernetes Apps | Install secret of MetalLB if n
        ot existing] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes-apps/metallb/tasks/main.yml:69
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        META: ran handlers
        META: ran handlers
        
        PLAY [k8s-cluster] *************************************************************
        META: ran handlers
        
        TASK [prep_download | Set a few facts] *****************************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        download/tasks/prep_download.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [prep_download | Set image info command for containerd and crio] **********
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        download/tasks/prep_download.yml:8
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [prep_download | Set image info command for containerd and crio on localhos
        t] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        download/tasks/prep_download.yml:14
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [prep_download | On localhost, check if passwordless root is possible] ****
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        download/tasks/prep_download.yml:20
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [prep_download | On localhost, check if user has access to docker without u
        sing sudo] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        download/tasks/prep_download.yml:35
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [prep_download | Parse the outputs of the previous commands] **************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        download/tasks/prep_download.yml:50
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [prep_download | Check that local user is in group or can become root] ****
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        download/tasks/prep_download.yml:60
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [prep_download | Register docker images info] *****************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        download/tasks/prep_download.yml:71
        skipping: [kubenode01] => {"censored": "the output has been hidden due to the fa
        ct that 'no_log: true' was specified for this result", "changed": false}
        
        TASK [prep_download | Create staging directory on remote node] *****************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        download/tasks/prep_download.yml:80
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [prep_download | Create local cache for files and images on control node] *
        **
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        download/tasks/prep_download.yml:90
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [container-engine/crictl : install crictĺ] ********************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        container-engine/crictl/tasks/main.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [download | Get kubeadm binary and list of required images] ***************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        download/tasks/main.yml:17
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [download | Download files / images] **************************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        download/tasks/main.yml:26
        skipping: [kubenode01] => (item={'key': 'netcheck_server', 'value': {'enabled':
        False, 'container': True, 'repo': 'quay.io/l23network/k8s-netchecker-server', 't
        ag': 'v1.0', 'sha256': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var":
         "item", "changed": false, "item": {"key": "netcheck_server", "value": {"contain
        er": true, "enabled": false, "groups": ["k8s-cluster"], "repo": "quay.io/l23netw
        ork/k8s-netchecker-server", "sha256": "", "tag": "v1.0"}}, "skip_reason": "Condi
        tional result was False"}
        skipping: [kubenode01] => (item={'key': 'netcheck_agent', 'value': {'enabled': F
        alse, 'container': True, 'repo': 'quay.io/l23network/k8s-netchecker-agent', 'tag
        ': 'v1.0', 'sha256': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "
        item", "changed": false, "item": {"key": "netcheck_agent", "value": {"container"
        : true, "enabled": false, "groups": ["k8s-cluster"], "repo": "quay.io/l23network
        /k8s-netchecker-agent", "sha256": "", "tag": "v1.0"}}, "skip_reason": "Condition
        al result was False"}
        skipping: [kubenode01] => (item={'key': 'etcd', 'value': {'container': True, 'fi
        le': False, 'enabled': True, 'version': 'v3.4.13', 'dest': '/tmp/releases/etcd-v
        3.4.13-linux-amd64.tar.gz', 'repo': 'quay.io/coreos/etcd', 'tag': 'v3.4.13', 'sh
        a256': '', 'url': 'https://github.com/coreos/etcd/releases/download/v3.4.13/etcd
        -v3.4.13-linux-amd64.tar.gz', 'unarchive': False, 'owner': 'root', 'mode': '0755
        ', 'groups': ['etcd']}})  => {"ansible_loop_var": "item", "changed": false, "ite
        m": {"key": "etcd", "value": {"container": true, "dest": "/tmp/releases/etcd-v3.
        4.13-linux-amd64.tar.gz", "enabled": true, "file": false, "groups": ["etcd"], "m
        ode": "0755", "owner": "root", "repo": "quay.io/coreos/etcd", "sha256": "", "tag
        ": "v3.4.13", "unarchive": false, "url": "https://github.com/coreos/etcd/release
        s/download/v3.4.13/etcd-v3.4.13-linux-amd64.tar.gz", "version": "v3.4.13"}}, "sk
        ip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'cni', 'value': {'enabled': True, 'file'
        : True, 'version': 'v0.9.0', 'dest': '/tmp/releases/cni-plugins-linux-amd64-v0.9
        .0.tgz', 'sha256': '58a58d389895ba9f9bbd3ef330f186c0bb7484136d0bfb9b50152eed55d9
        ec24', 'url': 'https://github.com/containernetworking/plugins/releases/download/
        v0.9.0/cni-plugins-linux-amd64-v0.9.0.tgz', 'unarchive': False, 'owner': 'root',
         'mode': '0755', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "
        changed": false, "item": {"key": "cni", "value": {"dest": "/tmp/releases/cni-plu
        gins-linux-amd64-v0.9.0.tgz", "enabled": true, "file": true, "groups": ["k8s-clu
        ster"], "mode": "0755", "owner": "root", "sha256": "58a58d389895ba9f9bbd3ef330f1
        86c0bb7484136d0bfb9b50152eed55d9ec24", "unarchive": false, "url": "https://githu
        b.com/containernetworking/plugins/releases/download/v0.9.0/cni-plugins-linux-amd
        64-v0.9.0.tgz", "version": "v0.9.0"}}, "skip_reason": "Conditional result was Fa
        lse"}
        skipping: [kubenode01] => (item={'key': 'kubeadm', 'value': {'enabled': True, 'f
        ile': True, 'version': 'v1.19.7', 'dest': '/tmp/releases/kubeadm-v1.19.7-amd64',
         'sha256': 'c63ef1842533cd7888c7452cab9f320dcf45fc1c173e9d40abb712d45992db24', '
        url': 'https://storage.googleapis.com/kubernetes-release/release/v1.19.7/bin/lin
        ux/amd64/kubeadm', 'unarchive': False, 'owner': 'root', 'mode': '0755', 'groups'
        : ['k8s-cluster']}})  => {"ansible_loop_var": "item", "changed": false, "item":
        {"key": "kubeadm", "value": {"dest": "/tmp/releases/kubeadm-v1.19.7-amd64", "ena
        bled": true, "file": true, "groups": ["k8s-cluster"], "mode": "0755", "owner": "
        root", "sha256": "c63ef1842533cd7888c7452cab9f320dcf45fc1c173e9d40abb712d45992db
        24", "unarchive": false, "url": "https://storage.googleapis.com/kubernetes-relea
        se/release/v1.19.7/bin/linux/amd64/kubeadm", "version": "v1.19.7"}}, "skip_reaso
        n": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'kubelet', 'value': {'enabled': True, 'f
        ile': True, 'version': 'v1.19.7', 'dest': '/tmp/releases/kubelet-v1.19.7-amd64',
         'sha256': 'd8b296825f6dd7a17287b73cd6604d32210abbba86c88fb68c1b1c5016906c54', '
        url': 'https://storage.googleapis.com/kubernetes-release/release/v1.19.7/bin/lin
        ux/amd64/kubelet', 'unarchive': False, 'owner': 'root', 'mode': '0755', 'groups'
        : ['k8s-cluster']}})  => {"ansible_loop_var": "item", "changed": false, "item":
        {"key": "kubelet", "value": {"dest": "/tmp/releases/kubelet-v1.19.7-amd64", "ena
        bled": true, "file": true, "groups": ["k8s-cluster"], "mode": "0755", "owner": "
        root", "sha256": "d8b296825f6dd7a17287b73cd6604d32210abbba86c88fb68c1b1c5016906c
        54", "unarchive": false, "url": "https://storage.googleapis.com/kubernetes-relea
        se/release/v1.19.7/bin/linux/amd64/kubelet", "version": "v1.19.7"}}, "skip_reaso
        n": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'kubectl', 'value': {'enabled': True, 'f
        ile': True, 'version': 'v1.19.7', 'dest': '/tmp/releases/kubectl-v1.19.7-amd64',
         'sha256': 'd46eb3bbe2575e5b6bedbc6d3519424b4f2f57929d7da1ef7e11c09068f37297', '
        url': 'https://storage.googleapis.com/kubernetes-release/release/v1.19.7/bin/lin
        ux/amd64/kubectl', 'unarchive': False, 'owner': 'root', 'mode': '0755', 'groups'
        : ['kube-master']}})  => {"ansible_loop_var": "item", "changed": false, "item":
        {"key": "kubectl", "value": {"dest": "/tmp/releases/kubectl-v1.19.7-amd64", "ena
        bled": true, "file": true, "groups": ["kube-master"], "mode": "0755", "owner": "
        root", "sha256": "d46eb3bbe2575e5b6bedbc6d3519424b4f2f57929d7da1ef7e11c09068f372
        97", "unarchive": false, "url": "https://storage.googleapis.com/kubernetes-relea
        se/release/v1.19.7/bin/linux/amd64/kubectl", "version": "v1.19.7"}}, "skip_reaso
        n": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'crictl', 'value': {'file': True, 'enabl
        ed': False, 'version': 'v1.19.0', 'dest': '/tmp/releases/crictl-v1.19.0-linux-am
        d64.tar.gz', 'sha256': '87d8ef70b61f2fe3d8b4a48f6f712fd798c6e293ed3723c1e4bbb505
        2098f0ae', 'url': 'https://github.com/kubernetes-sigs/cri-tools/releases/downloa
        d/v1.19.0/crictl-v1.19.0-linux-amd64.tar.gz', 'unarchive': True, 'owner': 'root'
        , 'mode': '0755', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item",
        "changed": false, "item": {"key": "crictl", "value": {"dest": "/tmp/releases/cri
        ctl-v1.19.0-linux-amd64.tar.gz", "enabled": false, "file": true, "groups": ["k8s
        -cluster"], "mode": "0755", "owner": "root", "sha256": "87d8ef70b61f2fe3d8b4a48f
        6f712fd798c6e293ed3723c1e4bbb5052098f0ae", "unarchive": true, "url": "https://gi
        thub.com/kubernetes-sigs/cri-tools/releases/download/v1.19.0/crictl-v1.19.0-linu
        x-amd64.tar.gz", "version": "v1.19.0"}}, "skip_reason": "Conditional result was
        False"}
        skipping: [kubenode01] => (item={'key': 'cilium', 'value': {'enabled': False, 'c
        ontainer': True, 'repo': 'quay.io/cilium/cilium', 'tag': 'v1.8.6', 'sha256': '',
         'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "changed": false,
         "item": {"key": "cilium", "value": {"container": true, "enabled": false, "group
        s": ["k8s-cluster"], "repo": "quay.io/cilium/cilium", "sha256": "", "tag": "v1.8
        .6"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'cilium_init', 'value': {'enabled': Fals
        e, 'container': True, 'repo': 'quay.io/cilium/cilium-init', 'tag': '2019-04-05',
         'sha256': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "ch
        anged": false, "item": {"key": "cilium_init", "value": {"container": true, "enab
        led": false, "groups": ["k8s-cluster"], "repo": "quay.io/cilium/cilium-init", "s
        ha256": "", "tag": "2019-04-05"}}, "skip_reason": "Conditional result was False"
        }
        skipping: [kubenode01] => (item={'key': 'cilium_operator', 'value': {'enabled':
        False, 'container': True, 'repo': 'quay.io/cilium/operator', 'tag': 'v1.8.6', 's
        ha256': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "chang
        ed": false, "item": {"key": "cilium_operator", "value": {"container": true, "ena
        bled": false, "groups": ["k8s-cluster"], "repo": "quay.io/cilium/operator", "sha
        256": "", "tag": "v1.8.6"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'multus', 'value': {'enabled': False, 'c
        ontainer': True, 'repo': 'docker.io/nfvpe/multus', 'tag': 'v3.6', 'sha256': '',
        'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "changed": false,
        "item": {"key": "multus", "value": {"container": true, "enabled": false, "groups
        ": ["k8s-cluster"], "repo": "docker.io/nfvpe/multus", "sha256": "", "tag": "v3.6
        "}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'flannel', 'value': {'enabled': True, 'c
        ontainer': True, 'repo': 'quay.io/coreos/flannel', 'tag': 'v0.13.0', 'sha256': '
        ', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "changed": fals
        e, "item": {"key": "flannel", "value": {"container": true, "enabled": true, "gro
        ups": ["k8s-cluster"], "repo": "quay.io/coreos/flannel", "sha256": "", "tag": "v
        0.13.0"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'calicoctl', 'value': {'enabled': False,
         'file': True, 'version': 'v3.16.6', 'dest': '/tmp/releases/calicoctl', 'sha256'
        : '9b82230446d4749a1043dddd8d466d275a460e570a412e6ced003368ab9c72d8', 'url': 'ht
        tps://github.com/projectcalico/calicoctl/releases/download/v3.16.6/calicoctl-lin
        ux-amd64', 'unarchive': False, 'owner': 'root', 'mode': '0755', 'groups': ['k8s-
        cluster']}})  => {"ansible_loop_var": "item", "changed": false, "item": {"key":
        "calicoctl", "value": {"dest": "/tmp/releases/calicoctl", "enabled": false, "fil
        e": true, "groups": ["k8s-cluster"], "mode": "0755", "owner": "root", "sha256":
        "9b82230446d4749a1043dddd8d466d275a460e570a412e6ced003368ab9c72d8", "unarchive":
         false, "url": "https://github.com/projectcalico/calicoctl/releases/download/v3.
        16.6/calicoctl-linux-amd64", "version": "v3.16.6"}}, "skip_reason": "Conditional
         result was False"}
        skipping: [kubenode01] => (item={'key': 'calico_node', 'value': {'enabled': Fals
        e, 'container': True, 'repo': 'quay.io/calico/node', 'tag': 'v3.16.6', 'sha256':
         '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "changed": fa
        lse, "item": {"key": "calico_node", "value": {"container": true, "enabled": fals
        e, "groups": ["k8s-cluster"], "repo": "quay.io/calico/node", "sha256": "", "tag"
        : "v3.16.6"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'calico_cni', 'value': {'enabled': False
        , 'container': True, 'repo': 'quay.io/calico/cni', 'tag': 'v3.16.6', 'sha256': '
        ', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "changed": fals
        e, "item": {"key": "calico_cni", "value": {"container": true, "enabled": false,
        "groups": ["k8s-cluster"], "repo": "quay.io/calico/cni", "sha256": "", "tag": "v
        3.16.6"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'calico_policy', 'value': {'enabled': Fa
        lse, 'container': True, 'repo': 'quay.io/calico/kube-controllers', 'tag': 'v3.16
        .6', 'sha256': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item",
         "changed": false, "item": {"key": "calico_policy", "value": {"container": true,
         "enabled": false, "groups": ["k8s-cluster"], "repo": "quay.io/calico/kube-contr
        ollers", "sha256": "", "tag": "v3.16.6"}}, "skip_reason": "Conditional result wa
        s False"}
        skipping: [kubenode01] => (item={'key': 'calico_typha', 'value': {'enabled': Fal
        se, 'container': True, 'repo': 'quay.io/calico/typha', 'tag': 'v3.16.6', 'sha256
        ': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "changed":
        false, "item": {"key": "calico_typha", "value": {"container": true, "enabled": f
        alse, "groups": ["k8s-cluster"], "repo": "quay.io/calico/typha", "sha256": "", "
        tag": "v3.16.6"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'weave_kube', 'value': {'enabled': False
        , 'container': True, 'repo': 'docker.io/weaveworks/weave-kube', 'tag': '2.7.0',
        'sha256': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "cha
        nged": false, "item": {"key": "weave_kube", "value": {"container": true, "enable
        d": false, "groups": ["k8s-cluster"], "repo": "docker.io/weaveworks/weave-kube",
         "sha256": "", "tag": "2.7.0"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'weave_npc', 'value': {'enabled': False,
         'container': True, 'repo': 'docker.io/weaveworks/weave-npc', 'tag': '2.7.0', 's
        ha256': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "chang
        ed": false, "item": {"key": "weave_npc", "value": {"container": true, "enabled":
         false, "groups": ["k8s-cluster"], "repo": "docker.io/weaveworks/weave-npc", "sh
        a256": "", "tag": "2.7.0"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'ovn4nfv', 'value': {'enabled': False, '
        container': True, 'repo': 'docker.io/integratedcloudnative/ovn4nfv-k8s-plugin',
        'tag': 'v1.1.0', 'sha256': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_v
        ar": "item", "changed": false, "item": {"key": "ovn4nfv", "value": {"container":
         true, "enabled": false, "groups": ["k8s-cluster"], "repo": "docker.io/integrate
        dcloudnative/ovn4nfv-k8s-plugin", "sha256": "", "tag": "v1.1.0"}}, "skip_reason"
        : "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'kube_ovn', 'value': {'enabled': False,
        'container': True, 'repo': 'docker.io/kubeovn/kube-ovn', 'tag': 'v1.5.2', 'sha25
        6': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "changed":
         false, "item": {"key": "kube_ovn", "value": {"container": true, "enabled": fals
        e, "groups": ["k8s-cluster"], "repo": "docker.io/kubeovn/kube-ovn", "sha256": ""
        , "tag": "v1.5.2"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'kube_router', 'value': {'enabled': Fals
        e, 'container': True, 'repo': 'docker.io/cloudnativelabs/kube-router', 'tag': 'v
        1.1.1', 'sha256': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "ite
        m", "changed": false, "item": {"key": "kube_router", "value": {"container": true
        , "enabled": false, "groups": ["k8s-cluster"], "repo": "docker.io/cloudnativelab
        s/kube-router", "sha256": "", "tag": "v1.1.1"}}, "skip_reason": "Conditional res
        ult was False"}
        skipping: [kubenode01] => (item={'key': 'pod_infra', 'value': {'enabled': True,
        'container': True, 'repo': 'k8s.gcr.io/pause', 'tag': '3.3', 'sha256': '', 'grou
        ps': ['k8s-cluster']}})  => {"ansible_loop_var": "item", "changed": false, "item
        ": {"key": "pod_infra", "value": {"container": true, "enabled": true, "groups":
        ["k8s-cluster"], "repo": "k8s.gcr.io/pause", "sha256": "", "tag": "3.3"}}, "skip
        _reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'install_socat', 'value': {'enabled': Fa
        lse, 'container': True, 'repo': 'docker.io/xueshanf/install-socat', 'tag': 'late
        st', 'sha256': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item",
         "changed": false, "item": {"key": "install_socat", "value": {"container": true,
         "enabled": false, "groups": ["k8s-cluster"], "repo": "docker.io/xueshanf/instal
        l-socat", "sha256": "", "tag": "latest"}}, "skip_reason": "Conditional result wa
        s False"}
        skipping: [kubenode01] => (item={'key': 'nginx', 'value': {'enabled': True, 'con
        tainer': True, 'repo': 'docker.io/library/nginx', 'tag': 1.19, 'sha256': '', 'gr
        oups': ['kube-node']}})  => {"ansible_loop_var": "item", "changed": false, "item
        ": {"key": "nginx", "value": {"container": true, "enabled": true, "groups": ["ku
        be-node"], "repo": "docker.io/library/nginx", "sha256": "", "tag": 1.19}}, "skip
        _reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'haproxy', 'value': {'enabled': False, '
        container': True, 'repo': 'docker.io/library/haproxy', 'tag': 2.3, 'sha256': '',
         'groups': ['kube-node']}})  => {"ansible_loop_var": "item", "changed": false, "
        item": {"key": "haproxy", "value": {"container": true, "enabled": false, "groups
        ": ["kube-node"], "repo": "docker.io/library/haproxy", "sha256": "", "tag": 2.3}
        }, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'coredns', 'value': {'enabled': True, 'c
        ontainer': True, 'repo': 'k8s.gcr.io/coredns', 'tag': '1.7.0', 'sha256': '', 'gr
        oups': ['kube-master']}})  => {"ansible_loop_var": "item", "changed": false, "it
        em": {"key": "coredns", "value": {"container": true, "enabled": true, "groups":
        ["kube-master"], "repo": "k8s.gcr.io/coredns", "sha256": "", "tag": "1.7.0"}}, "
        skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'nodelocaldns', 'value': {'enabled': Tru
        e, 'container': True, 'repo': 'k8s.gcr.io/dns/k8s-dns-node-cache', 'tag': '1.16.
        0', 'sha256': '', 'groups': ['k8s-cluster']}})  => {"ansible_loop_var": "item",
        "changed": false, "item": {"key": "nodelocaldns", "value": {"container": true, "
        enabled": true, "groups": ["k8s-cluster"], "repo": "k8s.gcr.io/dns/k8s-dns-node-
        cache", "sha256": "", "tag": "1.16.0"}}, "skip_reason": "Conditional result was
        False"}
        skipping: [kubenode01] => (item={'key': 'dnsautoscaler', 'value': {'enabled': Tr
        ue, 'container': True, 'repo': 'k8s.gcr.io/cpa/cluster-proportional-autoscaler-a
        md64', 'tag': '1.8.3', 'sha256': '', 'groups': ['kube-master']}})  => {"ansible_
        loop_var": "item", "changed": false, "item": {"key": "dnsautoscaler", "value": {
        "container": true, "enabled": true, "groups": ["kube-master"], "repo": "k8s.gcr.
        io/cpa/cluster-proportional-autoscaler-amd64", "sha256": "", "tag": "1.8.3"}}, "
        skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'testbox', 'value': {'enabled': False, '
        container': True, 'repo': 'k8s.gcr.io/busybox', 'tag': 'latest', 'sha256': ''}})
          => {"ansible_loop_var": "item", "changed": false, "item": {"key": "testbox", "
        value": {"container": true, "enabled": false, "repo": "k8s.gcr.io/busybox", "sha
        256": "", "tag": "latest"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'helm', 'value': {'enabled': False, 'fil
        e': True, 'version': 'v3.5.2', 'dest': '/tmp/releases/helm-v3.5.2/helm-v3.5.2-li
        nux-amd64.tar.gz', 'sha256': '01b317c506f8b6ad60b11b1dc3f093276bb703281cb1ae0113
        2752253ec706a2', 'url': 'https://get.helm.sh/helm-v3.5.2-linux-amd64.tar.gz', 'u
        narchive': True, 'owner': 'root', 'mode': '0755', 'groups': ['kube-master']}})
        => {"ansible_loop_var": "item", "changed": false, "item": {"key": "helm", "value
        ": {"dest": "/tmp/releases/helm-v3.5.2/helm-v3.5.2-linux-amd64.tar.gz", "enabled
        ": false, "file": true, "groups": ["kube-master"], "mode": "0755", "owner": "roo
        t", "sha256": "01b317c506f8b6ad60b11b1dc3f093276bb703281cb1ae01132752253ec706a2"
        , "unarchive": true, "url": "https://get.helm.sh/helm-v3.5.2-linux-amd64.tar.gz"
        , "version": "v3.5.2"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'registry', 'value': {'enabled': False,
        'container': True, 'repo': 'docker.io/library/registry', 'tag': '2.7.1', 'sha256
        ': '', 'groups': ['kube-node']}})  => {"ansible_loop_var": "item", "changed": fa
        lse, "item": {"key": "registry", "value": {"container": true, "enabled": false,
        "groups": ["kube-node"], "repo": "docker.io/library/registry", "sha256": "", "ta
        g": "2.7.1"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'registry_proxy', 'value': {'enabled': F
        alse, 'container': True, 'repo': 'k8s.gcr.io/kube-registry-proxy', 'tag': '0.4',
         'sha256': '', 'groups': ['kube-node']}})  => {"ansible_loop_var": "item", "chan
        ged": false, "item": {"key": "registry_proxy", "value": {"container": true, "ena
        bled": false, "groups": ["kube-node"], "repo": "k8s.gcr.io/kube-registry-proxy",
         "sha256": "", "tag": "0.4"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'metrics_server', 'value': {'enabled': F
        alse, 'container': True, 'repo': 'k8s.gcr.io/metrics-server/metrics-server', 'ta
        g': 'v0.3.7', 'sha256': '', 'groups': ['kube-master']}})  => {"ansible_loop_var"
        : "item", "changed": false, "item": {"key": "metrics_server", "value": {"contain
        er": true, "enabled": false, "groups": ["kube-master"], "repo": "k8s.gcr.io/metr
        ics-server/metrics-server", "sha256": "", "tag": "v0.3.7"}}, "skip_reason": "Con
        ditional result was False"}
        skipping: [kubenode01] => (item={'key': 'addon_resizer', 'value': {'enabled': Fa
        lse, 'container': True, 'repo': 'k8s.gcr.io/addon-resizer', 'tag': '1.8.11', 'sh
        a256': '', 'groups': ['kube-master']}})  => {"ansible_loop_var": "item", "change
        d": false, "item": {"key": "addon_resizer", "value": {"container": true, "enable
        d": false, "groups": ["kube-master"], "repo": "k8s.gcr.io/addon-resizer", "sha25
        6": "", "tag": "1.8.11"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'local_volume_provisioner', 'value': {'e
        nabled': False, 'container': True, 'repo': 'quay.io/external_storage/local-volum
        e-provisioner', 'tag': 'v2.3.4', 'sha256': '', 'groups': ['kube-node']}})  => {"
        ansible_loop_var": "item", "changed": false, "item": {"key": "local_volume_provi
        sioner", "value": {"container": true, "enabled": false, "groups": ["kube-node"],
         "repo": "quay.io/external_storage/local-volume-provisioner", "sha256": "", "tag
        ": "v2.3.4"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'cephfs_provisioner', 'value': {'enabled
        ': False, 'container': True, 'repo': 'quay.io/external_storage/cephfs-provisione
        r', 'tag': 'v2.1.0-k8s1.11', 'sha256': '', 'groups': ['kube-node']}})  => {"ansi
        ble_loop_var": "item", "changed": false, "item": {"key": "cephfs_provisioner", "
        value": {"container": true, "enabled": false, "groups": ["kube-node"], "repo": "
        quay.io/external_storage/cephfs-provisioner", "sha256": "", "tag": "v2.1.0-k8s1.
        11"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'rbd_provisioner', 'value': {'enabled':
        False, 'container': True, 'repo': 'quay.io/external_storage/rbd-provisioner', 't
        ag': 'v2.1.1-k8s1.11', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_lo
        op_var": "item", "changed": false, "item": {"key": "rbd_provisioner", "value": {
        "container": true, "enabled": false, "groups": ["kube-node"], "repo": "quay.io/e
        xternal_storage/rbd-provisioner", "sha256": "", "tag": "v2.1.1-k8s1.11"}}, "skip
        _reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'local_path_provisioner', 'value': {'ena
        bled': False, 'container': True, 'repo': 'docker.io/rancher/local-path-provision
        er', 'tag': 'v0.0.19', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_lo
        op_var": "item", "changed": false, "item": {"key": "local_path_provisioner", "va
        lue": {"container": true, "enabled": false, "groups": ["kube-node"], "repo": "do
        cker.io/rancher/local-path-provisioner", "sha256": "", "tag": "v0.0.19"}}, "skip
        _reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'ingress_nginx_controller', 'value': {'e
        nabled': False, 'container': True, 'repo': 'k8s.gcr.io/ingress-nginx/controller'
        , 'tag': 'v0.41.2', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_loop_
        var": "item", "changed": false, "item": {"key": "ingress_nginx_controller", "val
        ue": {"container": true, "enabled": false, "groups": ["kube-node"], "repo": "k8s
        .gcr.io/ingress-nginx/controller", "sha256": "", "tag": "v0.41.2"}}, "skip_reaso
        n": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'ingress_ambassador_controller', 'value'
        : {'enabled': False, 'container': True, 'repo': 'quay.io/datawire/ambassador-ope
        rator', 'tag': 'v1.2.9', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_
        loop_var": "item", "changed": false, "item": {"key": "ingress_ambassador_control
        ler", "value": {"container": true, "enabled": false, "groups": ["kube-node"], "r
        epo": "quay.io/datawire/ambassador-operator", "sha256": "", "tag": "v1.2.9"}}, "
        skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'ingress_alb_controller', 'value': {'ena
        bled': False, 'container': True, 'repo': 'docker.io/amazon/aws-alb-ingress-contr
        oller', 'tag': 'v1.1.9', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_
        loop_var": "item", "changed": false, "item": {"key": "ingress_alb_controller", "
        value": {"container": true, "enabled": false, "groups": ["kube-node"], "repo": "
        docker.io/amazon/aws-alb-ingress-controller", "sha256": "", "tag": "v1.1.9"}}, "
        skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'cert_manager_controller', 'value': {'en
        abled': False, 'container': True, 'repo': 'quay.io/jetstack/cert-manager-control
        ler', 'tag': 'v1.0.4', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_lo
        op_var": "item", "changed": false, "item": {"key": "cert_manager_controller", "v
        alue": {"container": true, "enabled": false, "groups": ["kube-node"], "repo": "q
        uay.io/jetstack/cert-manager-controller", "sha256": "", "tag": "v1.0.4"}}, "skip
        _reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'cert_manager_cainjector', 'value': {'en
        abled': False, 'container': True, 'repo': 'quay.io/jetstack/cert-manager-cainjec
        tor', 'tag': 'v1.0.4', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_lo
        op_var": "item", "changed": false, "item": {"key": "cert_manager_cainjector", "v
        alue": {"container": true, "enabled": false, "groups": ["kube-node"], "repo": "q
        uay.io/jetstack/cert-manager-cainjector", "sha256": "", "tag": "v1.0.4"}}, "skip
        _reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'cert_manager_webhook', 'value': {'enabl
        ed': False, 'container': True, 'repo': 'quay.io/jetstack/cert-manager-webhook',
        'tag': 'v1.0.4', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_loop_var
        ": "item", "changed": false, "item": {"key": "cert_manager_webhook", "value": {"
        container": true, "enabled": false, "groups": ["kube-node"], "repo": "quay.io/je
        tstack/cert-manager-webhook", "sha256": "", "tag": "v1.0.4"}}, "skip_reason": "C
        onditional result was False"}
        skipping: [kubenode01] => (item={'key': 'csi_attacher', 'value': {'enabled': Fal
        se, 'container': True, 'repo': 'quay.io/k8scsi/csi-attacher', 'tag': 'v2.2.0', '
        sha256': '', 'groups': ['kube-node']}})  => {"ansible_loop_var": "item", "change
        d": false, "item": {"key": "csi_attacher", "value": {"container": true, "enabled
        ": false, "groups": ["kube-node"], "repo": "quay.io/k8scsi/csi-attacher", "sha25
        6": "", "tag": "v2.2.0"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'csi_provisioner', 'value': {'enabled':
        False, 'container': True, 'repo': 'quay.io/k8scsi/csi-provisioner', 'tag': 'v1.6
        .0', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_loop_var": "item", "
        changed": false, "item": {"key": "csi_provisioner", "value": {"container": true,
         "enabled": false, "groups": ["kube-node"], "repo": "quay.io/k8scsi/csi-provisio
        ner", "sha256": "", "tag": "v1.6.0"}}, "skip_reason": "Conditional result was Fa
        lse"}
        skipping: [kubenode01] => (item={'key': 'csi_snapshotter', 'value': {'enabled':
        False, 'container': True, 'repo': 'quay.io/k8scsi/csi-snapshotter', 'tag': 'v2.1
        .1', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_loop_var": "item", "
        changed": false, "item": {"key": "csi_snapshotter", "value": {"container": true,
         "enabled": false, "groups": ["kube-node"], "repo": "quay.io/k8scsi/csi-snapshot
        ter", "sha256": "", "tag": "v2.1.1"}}, "skip_reason": "Conditional result was Fa
        lse"}
        skipping: [kubenode01] => (item={'key': 'snapshot_controller', 'value': {'enable
        d': False, 'container': True, 'repo': 'quay.io/k8scsi/snapshot-controller', 'tag
        ': 'v2.0.1', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_loop_var": "
        item", "changed": false, "item": {"key": "snapshot_controller", "value": {"conta
        iner": true, "enabled": false, "groups": ["kube-node"], "repo": "quay.io/k8scsi/
        snapshot-controller", "sha256": "", "tag": "v2.0.1"}}, "skip_reason": "Condition
        al result was False"}
        skipping: [kubenode01] => (item={'key': 'csi_resizer', 'value': {'enabled': Fals
        e, 'container': True, 'repo': 'quay.io/k8scsi/csi-resizer', 'tag': 'v0.5.0', 'sh
        a256': '', 'groups': ['kube-node']}})  => {"ansible_loop_var": "item", "changed"
        : false, "item": {"key": "csi_resizer", "value": {"container": true, "enabled":
        false, "groups": ["kube-node"], "repo": "quay.io/k8scsi/csi-resizer", "sha256":
        "", "tag": "v0.5.0"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'csi_node_driver_registrar', 'value': {'
        enabled': False, 'container': True, 'repo': 'quay.io/k8scsi/csi-node-driver-regi
        strar', 'tag': 'v1.3.0', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_
        loop_var": "item", "changed": false, "item": {"key": "csi_node_driver_registrar"
        , "value": {"container": true, "enabled": false, "groups": ["kube-node"], "repo"
        : "quay.io/k8scsi/csi-node-driver-registrar", "sha256": "", "tag": "v1.3.0"}}, "
        skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'cinder_csi_plugin', 'value': {'enabled'
        : False, 'container': True, 'repo': 'docker.io/k8scloudprovider/cinder-csi-plugi
        n', 'tag': 'v1.18.0', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_loo
        p_var": "item", "changed": false, "item": {"key": "cinder_csi_plugin", "value":
        {"container": true, "enabled": false, "groups": ["kube-node"], "repo": "docker.i
        o/k8scloudprovider/cinder-csi-plugin", "sha256": "", "tag": "v1.18.0"}}, "skip_r
        eason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'aws_ebs_csi_plugin', 'value': {'enabled
        ': False, 'container': True, 'repo': 'docker.io/amazon/aws-ebs-csi-driver', 'tag
        ': 'v0.5.0', 'sha256': '', 'groups': ['kube-node']}})  => {"ansible_loop_var": "
        item", "changed": false, "item": {"key": "aws_ebs_csi_plugin", "value": {"contai
        ner": true, "enabled": false, "groups": ["kube-node"], "repo": "docker.io/amazon
        /aws-ebs-csi-driver", "sha256": "", "tag": "v0.5.0"}}, "skip_reason": "Condition
        al result was False"}
        skipping: [kubenode01] => (item={'key': 'dashboard', 'value': {'enabled': False,
         'container': True, 'repo': 'docker.io/kubernetesui/dashboard-amd64', 'tag': 'v2
        .1.0', 'sha256': '', 'groups': ['kube-master']}})  => {"ansible_loop_var": "item
        ", "changed": false, "item": {"key": "dashboard", "value": {"container": true, "
        enabled": false, "groups": ["kube-master"], "repo": "docker.io/kubernetesui/dash
        board-amd64", "sha256": "", "tag": "v2.1.0"}}, "skip_reason": "Conditional resul
        t was False"}
        skipping: [kubenode01] => (item={'key': 'dashboard_metrics_scrapper', 'value': {
        'enabled': False, 'container': True, 'repo': 'docker.io/kubernetesui/metrics-scr
        aper', 'tag': 'v1.0.6', 'sha256': '', 'groups': ['kube-master']}})  => {"ansible
        _loop_var": "item", "changed": false, "item": {"key": "dashboard_metrics_scrappe
        r", "value": {"container": true, "enabled": false, "groups": ["kube-master"], "r
        epo": "docker.io/kubernetesui/metrics-scraper", "sha256": "", "tag": "v1.0.6"}},
         "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'kubeadm_kube-apiserver', 'value': {'ena
        bled': True, 'container': True, 'repo': 'k8s.gcr.io/kube-apiserver', 'tag': 'v1.
        19.7', 'groups': 'k8s-cluster'}})  => {"ansible_loop_var": "item", "changed": fa
        lse, "item": {"key": "kubeadm_kube-apiserver", "value": {"container": true, "ena
        bled": true, "groups": "k8s-cluster", "repo": "k8s.gcr.io/kube-apiserver", "tag"
        : "v1.19.7"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'kubeadm_kube-controller-manager', 'valu
        e': {'enabled': True, 'container': True, 'repo': 'k8s.gcr.io/kube-controller-man
        ager', 'tag': 'v1.19.7', 'groups': 'k8s-cluster'}})  => {"ansible_loop_var": "it
        em", "changed": false, "item": {"key": "kubeadm_kube-controller-manager", "value
        ": {"container": true, "enabled": true, "groups": "k8s-cluster", "repo": "k8s.gc
        r.io/kube-controller-manager", "tag": "v1.19.7"}}, "skip_reason": "Conditional r
        esult was False"}
        skipping: [kubenode01] => (item={'key': 'kubeadm_kube-scheduler', 'value': {'ena
        bled': True, 'container': True, 'repo': 'k8s.gcr.io/kube-scheduler', 'tag': 'v1.
        19.7', 'groups': 'k8s-cluster'}})  => {"ansible_loop_var": "item", "changed": fa
        lse, "item": {"key": "kubeadm_kube-scheduler", "value": {"container": true, "ena
        bled": true, "groups": "k8s-cluster", "repo": "k8s.gcr.io/kube-scheduler", "tag"
        : "v1.19.7"}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': 'kubeadm_kube-proxy', 'value': {'enabled
        ': True, 'container': True, 'repo': 'k8s.gcr.io/kube-proxy', 'tag': 'v1.19.7', '
        groups': 'k8s-cluster'}})  => {"ansible_loop_var": "item", "changed": false, "it
        em": {"key": "kubeadm_kube-proxy", "value": {"container": true, "enabled": true,
         "groups": "k8s-cluster", "repo": "k8s.gcr.io/kube-proxy", "tag": "v1.19.7"}}, "
        skip_reason": "Conditional result was False"}
        
        TASK [kubespray-defaults : Configure defaults] *********************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubespray-defaults/tasks/main.yaml:2
        ok: [kubenode01] => {
            "msg": "Check roles/kubespray-defaults/defaults/main.yml"
        }
        
        TASK [kubespray-defaults : Gather ansible_default_ipv4 from all hosts] *********
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubespray-defaults/tasks/fallback_ips.yml:6
        skipping: [kubenode01] => (item=kubenode01)  => {"ansible_loop_var": "delegate_h
        ost_to_gather_facts", "changed": false, "delegate_host_to_gather_facts": "kubeno
        de01", "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item=kubenode01)  => {"ansible_loop_var": "delegate_h
        ost_to_gather_facts", "changed": false, "delegate_host_to_gather_facts": "kubeno
        de01", "skip_reason": "Conditional result was False"}
        
        TASK [kubespray-defaults : create fallback_ips_base] ***************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubespray-defaults/tasks/fallback_ips.yml:15
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubespray-defaults : set fallback_ips] ***********************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubespray-defaults/tasks/fallback_ips.yml:29
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubespray-defaults : Set no_proxy to all assigned cluster IPs and hostname
        s] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubespray-defaults/tasks/no_proxy.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubespray-defaults : Populates no_proxy to all hosts] ********************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubespray-defaults/tasks/no_proxy.yml:32
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [adduser : User | Create User Group] **************************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        adduser/tasks/main.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [adduser : User | Create User] ********************************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        adduser/tasks/main.yml:7
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Remove swapfile from /etc/fstab] *****************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0010-swapoff.yml:2
        skipping: [kubenode01] => (item=swap)  => {"ansible_loop_var": "item", "changed"
        : false, "item": "swap", "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item=none)  => {"ansible_loop_var": "item", "changed"
        : false, "item": "none", "skip_reason": "Conditional result was False"}
        
        TASK [kubernetes/preinstall : check swap] **************************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0010-swapoff.yml:12
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Disable swap] ************************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0010-swapoff.yml:16
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if either kube-master or kube-node group is e
        mpty] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:2
        skipping: [kubenode01] => (item=kube-master)  => {"ansible_loop_var": "item", "c
        hanged": false, "item": "kube-master", "skip_reason": "Conditional result was Fa
        lse"}
        skipping: [kubenode01] => (item=kube-node)  => {"ansible_loop_var": "item", "cha
        nged": false, "item": "kube-node", "skip_reason": "Conditional result was False"
        }
        
        TASK [kubernetes/preinstall : Stop if etcd group is empty in external etcd mode]
         ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:11
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if non systemd OS type] *********************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:20
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if unknown OS] ******************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:25
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if unknown network plugin] ******************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:31
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if incompatible network plugin and cloudprovi
        der] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:39
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if unsupported version of Kubernetes] *******
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:47
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if known booleans are set as strings (Use JSO
        N format on CLI: -e "{'key': true }")] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:54
        skipping: [kubenode01] => (item={'name': 'download_run_once', 'value': False})
        => {"ansible_loop_var": "item", "changed": false, "item": {"name": "download_run
        _once", "value": false}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'name': 'deploy_netchecker', 'value': False})
        => {"ansible_loop_var": "item", "changed": false, "item": {"name": "deploy_netch
        ecker", "value": false}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'name': 'download_always_pull', 'value': False}
        )  => {"ansible_loop_var": "item", "changed": false, "item": {"name": "download_
        always_pull", "value": false}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'name': 'helm_enabled', 'value': False})  => {"
        ansible_loop_var": "item", "changed": false, "item": {"name": "helm_enabled", "v
        alue": false}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'name': 'openstack_lbaas_enabled', 'value': Fal
        se})  => {"ansible_loop_var": "item", "changed": false, "item": {"name": "openst
        ack_lbaas_enabled", "value": false}, "skip_reason": "Conditional result was Fals
        e"}
        
        TASK [kubernetes/preinstall : Stop if even number of etcd hosts] ***************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:67
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if memory is too small for masters] *********
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:74
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if memory is too small for nodes] ***********
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:81
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Guarantee that enough network address space is ava
        ilable for all pods] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:93
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if ip var does not match local ips] *********
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:103
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if access_ip is not pingable] ***************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:111
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if RBAC is not enabled when dashboard is enab
        led] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:118
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if RBAC is not enabled when OCI cloud control
        ler is enabled] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:125
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if RBAC and anonymous-auth are not enabled wh
        en insecure port is disabled] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:132
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if kernel version is too low] ***************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:139
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if bad hostname] ****************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:146
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : check cloud_provider value] **********************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:152
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Ensure minimum calico version] *******************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:163
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Get current calico cluster version] **************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:171
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Check that current calico version is enough for up
        grade] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:184
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Check that cluster_id is set if calico_rr enabled]
         ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:196
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Check that calico_rr nodes are in k8s-cluster grou
        p] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:207
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Check that kube_service_addresses is a network ran
        ge] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:216
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Check that kube_pods_subnet is a network range] **
        *
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:223
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Check that kube_pods_subnet does not collide with
        kube_service_addresses] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:230
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if unknown dns mode] ************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:237
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if unknown kube proxy mode] *****************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:244
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if vault is chose] **************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:251
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if unknown cert_management] *****************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:258
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if unknown resolvconf_mode] *****************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:264
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if etcd deployment type is not host or docker
        ] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:271
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if etcd deployment type is not host when cont
        ainer_manager != docker] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:279
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if download_localhost is enabled but download
        _run_once is not] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:288
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if kata_containers_enabled is enabled when co
        ntainer_manager is docker] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:294
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stop if download_localhost is enabled for Flatcar
        Container Linux] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0020-verify-settings.yml:300
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Force binaries directory for Flatcar Container Lin
        ux by Kinvolk] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0040-set_facts.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : check if booted with ostree] *********************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0040-set_facts.yml:9
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : set is_fedora_coreos] ****************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0040-set_facts.yml:14
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : set is_fedora_coreos] ****************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0040-set_facts.yml:23
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : check resolvconf] ********************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0040-set_facts.yml:27
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : check systemd-resolved] **************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0040-set_facts.yml:34
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : set dns facts] ***********************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0040-set_facts.yml:42
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : check if kubelet is configured] ******************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0040-set_facts.yml:59
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : check if early DNS configuration stage] **********
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0040-set_facts.yml:65
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : target resolv.conf files] ************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0040-set_facts.yml:70
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : target temporary resolvconf cloud init file (Flatc
        ar Container Linux by Kinvolk / Fedora CoreOS)] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0040-set_facts.yml:79
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : check if /etc/dhclient.conf exists] **************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0040-set_facts.yml:84
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : target dhclient conf file for /etc/dhclient.conf]
        ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0040-set_facts.yml:89
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : check if /etc/dhcp/dhclient.conf exists] *********
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0040-set_facts.yml:94
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : target dhclient conf file for /etc/dhcp/dhclient.c
        onf] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0040-set_facts.yml:99
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : target dhclient hook file for Red Hat family] ****
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0040-set_facts.yml:104
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : target dhclient hook file for Debian family] *****
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0040-set_facts.yml:109
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : generate search domains to resolvconf] ***********
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0040-set_facts.yml:114
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : pick coredns cluster IP or default resolver] *****
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0040-set_facts.yml:125
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : generate nameservers to resolvconf] **************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0040-set_facts.yml:140
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : gather os specific variables] ********************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0040-set_facts.yml:147
        skipping: [kubenode01] => (item=/home/wire/wire-server-deploy/ansible/roles-exte
        rnal/kubespray/roles/kubernetes/preinstall/vars/../vars/ubuntu.yml)  => {"ansibl
        e_loop_var": "item", "changed": false, "item": "/home/wire/wire-server-deploy/an
        sible/roles-external/kubespray/roles/kubernetes/preinstall/vars/../vars/ubuntu.y
        ml", "skip_reason": "Conditional result was False"}
        
        TASK [kubernetes/preinstall : set etcd vars if using kubeadm mode] *************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0040-set_facts.yml:161
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : check /usr readonly] *****************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0040-set_facts.yml:170
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : set alternate flexvolume path] *******************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0040-set_facts.yml:175
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Create kubernetes directories] *******************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0050-create_directories.yml:2
        skipping: [kubenode01] => (item=/etc/kubernetes)  => {"ansible_loop_var": "item"
        , "changed": false, "item": "/etc/kubernetes", "skip_reason": "Conditional resul
        t was False"}
        skipping: [kubenode01] => (item=/etc/kubernetes/ssl)  => {"ansible_loop_var": "i
        tem", "changed": false, "item": "/etc/kubernetes/ssl", "skip_reason": "Condition
        al result was False"}
        skipping: [kubenode01] => (item=/etc/kubernetes/manifests)  => {"ansible_loop_va
        r": "item", "changed": false, "item": "/etc/kubernetes/manifests", "skip_reason"
        : "Conditional result was False"}
        skipping: [kubenode01] => (item=/usr/local/bin/kubernetes-scripts)  => {"ansible
        _loop_var": "item", "changed": false, "item": "/usr/local/bin/kubernetes-scripts
        ", "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item=/usr/libexec/kubernetes/kubelet-plugins/volume/e
        xec)  => {"ansible_loop_var": "item", "changed": false, "item": "/usr/libexec/ku
        bernetes/kubelet-plugins/volume/exec", "skip_reason": "Conditional result was Fa
        lse"}
        
        TASK [kubernetes/preinstall : Create other directories] ************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0050-create_directories.yml:26
        skipping: [kubenode01] => (item=/usr/local/bin)  => {"ansible_loop_var": "item",
         "changed": false, "item": "/usr/local/bin", "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Check if kubernetes kubeadm compat cert dir exists
        ] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0050-create_directories.yml:46
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Create kubernetes kubeadm compat cert dir (kuberne
        tes/kubeadm issue 1498)] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0050-create_directories.yml:54
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Create cni directories] **************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0050-create_directories.yml:64
        skipping: [kubenode01] => (item=/etc/cni/net.d)  => {"ansible_loop_var": "item",
         "changed": false, "item": "/etc/cni/net.d", "skip_reason": "Conditional result
        was False"}
        skipping: [kubenode01] => (item=/opt/cni/bin)  => {"ansible_loop_var": "item", "
        changed": false, "item": "/opt/cni/bin", "skip_reason": "Conditional result was
        False"}
        skipping: [kubenode01] => (item=/var/lib/calico)  => {"ansible_loop_var": "item"
        , "changed": false, "item": "/var/lib/calico", "skip_reason": "Conditional resul
        t was False"}
        
        TASK [kubernetes/preinstall : Create local volume provisioner directories] *****
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0050-create_directories.yml:87
        skipping: [kubenode01] => (item=local-storage)  => {"ansible_loop_var": "item",
        "changed": false, "item": "local-storage", "skip_reason": "Conditional result wa
        s False"}
        
        TASK [kubernetes/preinstall : create temporary resolveconf cloud init file] ****
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0060-resolvconf.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Add domain/search/nameservers/options to resolv.co
        nf] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0060-resolvconf.yml:6
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Remove search/domain/nameserver options before blo
        ck] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0060-resolvconf.yml:23
        skipping: [kubenode01] => (item=['/etc/resolv.conf', 'search '])  => {"ansible_l
        oop_var": "item", "changed": false, "item": ["/etc/resolv.conf", "search "], "sk
        ip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item=['/etc/resolv.conf', 'nameserver '])  => {"ansib
        le_loop_var": "item", "changed": false, "item": ["/etc/resolv.conf", "nameserver
         "], "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item=['/etc/resolv.conf', 'domain '])  => {"ansible_l
        oop_var": "item", "changed": false, "item": ["/etc/resolv.conf", "domain "], "sk
        ip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item=['/etc/resolv.conf', 'options '])  => {"ansible_
        loop_var": "item", "changed": false, "item": ["/etc/resolv.conf", "options "], "
        skip_reason": "Conditional result was False"}
        
        TASK [kubernetes/preinstall : Remove search/domain/nameserver options after bloc
        k] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0060-resolvconf.yml:34
        skipping: [kubenode01] => (item=['/etc/resolv.conf', 'search '])  => {"ansible_l
        oop_var": "item", "changed": false, "item": ["/etc/resolv.conf", "search "], "sk
        ip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item=['/etc/resolv.conf', 'nameserver '])  => {"ansib
        le_loop_var": "item", "changed": false, "item": ["/etc/resolv.conf", "nameserver
         "], "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item=['/etc/resolv.conf', 'domain '])  => {"ansible_l
        oop_var": "item", "changed": false, "item": ["/etc/resolv.conf", "domain "], "sk
        ip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item=['/etc/resolv.conf', 'options '])  => {"ansible_
        loop_var": "item", "changed": false, "item": ["/etc/resolv.conf", "options "], "
        skip_reason": "Conditional result was False"}
        
        TASK [kubernetes/preinstall : get temporary resolveconf cloud init file content]
         ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0060-resolvconf.yml:47
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : persist resolvconf cloud init file] **************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0060-resolvconf.yml:52
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Write resolved.conf] *****************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0061-systemd-resolved.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : NetworkManager | Add nameservers to NM configurati
        on] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0062-networkmanager.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : NetworkManager | Add DNS search to NM configuratio
        n] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0062-networkmanager.yml:12
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : NetworkManager | Add DNS options to NM configurati
        on] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0062-networkmanager.yml:22
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Update package management cache (zypper) - SUSE] *
        **
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0070-system-packages.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Update package management cache (APT)] ***********
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0070-system-packages.yml:12
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Remove legacy docker repo file] ******************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0070-system-packages.yml:20
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Install python3-dnf for latest RedHat versions] **
        *
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0070-system-packages.yml:28
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Install epel-release on RedHat/CentOS] ***********
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0070-system-packages.yml:42
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Update common_required_pkgs with ipvsadm when kube
        _proxy_mode is ipvs] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0070-system-packages.yml:53
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Install packages requirements] *******************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0070-system-packages.yml:58
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Install ipvsadm for ClearLinux] ******************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0070-system-packages.yml:70
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Confirm selinux deployed] ************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0080-system-configurations.yml:3
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Set selinux policy] ******************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0080-system-configurations.yml:11
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Disable IPv6 DNS lookup] *************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0080-system-configurations.yml:23
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Stat sysctl file configuration] ******************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0080-system-configurations.yml:36
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Change sysctl file path to link source if linked]
        ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0080-system-configurations.yml:43
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Make sure sysctl file path folder exists] ********
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0080-system-configurations.yml:52
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Enable ip forwarding] ****************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0080-system-configurations.yml:57
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Ensure kube-bench parameters are set] ************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0080-system-configurations.yml:65
        skipping: [kubenode01] => (item={'name': 'vm.overcommit_memory', 'value': 1})  =
        > {"ansible_loop_var": "item", "changed": false, "item": {"name": "vm.overcommit
        _memory", "value": 1}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'name': 'kernel.panic', 'value': 10})  => {"ans
        ible_loop_var": "item", "changed": false, "item": {"name": "kernel.panic", "valu
        e": 10}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'name': 'kernel.panic_on_oops', 'value': 1})  =
        > {"ansible_loop_var": "item", "changed": false, "item": {"name": "kernel.panic_
        on_oops", "value": 1}, "skip_reason": "Conditional result was False"}
        
        TASK [kubernetes/preinstall : Hosts | create list from inventory] **************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0090-etchosts.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Hosts | populate inventory into hosts file] ******
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0090-etchosts.yml:16
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Hosts | populate kubernetes loadbalancer address i
        nto hosts file] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0090-etchosts.yml:27
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Hosts | Retrieve hosts file content] *************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0090-etchosts.yml:39
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Hosts | Extract existing entries for localhost fro
        m hosts file] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0090-etchosts.yml:44
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Hosts | Update target hosts file entries dict with
         required entries] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0090-etchosts.yml:54
        skipping: [kubenode01] => (item={'key': '127.0.0.1', 'value': {'expected': ['loc
        alhost', 'localhost.localdomain']}})  => {"ansible_loop_var": "item", "changed":
         false, "item": {"key": "127.0.0.1", "value": {"expected": ["localhost", "localh
        ost.localdomain"]}}, "skip_reason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': '::1', 'value': {'expected': ['localhost
        6', 'localhost6.localdomain'], 'unexpected': ['localhost', 'localhost.localdomai
        n']}})  => {"ansible_loop_var": "item", "changed": false, "item": {"key": "::1",
         "value": {"expected": ["localhost6", "localhost6.localdomain"], "unexpected": [
        "localhost", "localhost.localdomain"]}}, "skip_reason": "Conditional result was
        False"}
        
        TASK [kubernetes/preinstall : Hosts | Update (if necessary) hosts file] ********
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0090-etchosts.yml:61
        skipping: [kubenode01] => (item={'key': '127.0.0.1', 'value': ['localhost', 'loc
        alhost.localdomain']})  => {"ansible_loop_var": "item", "changed": false, "item"
        : {"key": "127.0.0.1", "value": ["localhost", "localhost.localdomain"]}, "skip_r
        eason": "Conditional result was False"}
        skipping: [kubenode01] => (item={'key': '::1', 'value': ['ip6-localhost', 'ip6-l
        oopback', 'localhost6', 'localhost6.localdomain']})  => {"ansible_loop_var": "it
        em", "changed": false, "item": {"key": "::1", "value": ["ip6-localhost", "ip6-lo
        opback", "localhost6", "localhost6.localdomain"]}, "skip_reason": "Conditional r
        esult was False"}
        
        TASK [kubernetes/preinstall : Update facts] ************************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0090-etchosts.yml:72
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Configure dhclient to supersede search/domain/name
        servers] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0100-dhclient-hooks.yml:2
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Configure dhclient hooks for resolv.conf (non-RH)]
         ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0100-dhclient-hooks.yml:17
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Configure dhclient hooks for resolv.conf (RH-only)
        ] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0100-dhclient-hooks.yml:26
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Remove kubespray specific config from dhclient con
        fig] ***
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0110-dhclient-hooks-undo.yml:6
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : Remove kubespray specific dhclient hook] *********
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0110-dhclient-hooks-undo.yml:15
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        META: ran handlers
        
        TASK [kubernetes/preinstall : Check if we are running inside a Azure VM] *******
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/main.yml:92
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : install growpart] ********************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0120-growpart-azure-centos-7.yml:5
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : check if growpart needs to be run] ***************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0120-growpart-azure-centos-7.yml:10
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : check fs type] ***********************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0120-growpart-azure-centos-7.yml:18
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : run growpart] ************************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0120-growpart-azure-centos-7.yml:23
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        
        TASK [kubernetes/preinstall : run xfs_growfs] **********************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/kubespray/roles/
        kubernetes/preinstall/tasks/0120-growpart-azure-centos-7.yml:29
        skipping: [kubenode01] => {"changed": false, "skip_reason": "Conditional result
        was False"}
        META: ran handlers
        META: ran handlers
        
        PLAY [k8s-cluster] *************************************************************
        META: ran handlers
        
        TASK [Annotate nodes] **********************************************************
        task path: /home/wire/wire-server-deploy/ansible/kubernetes.yml:12
        META: ran handlers
        META: ran handlers
        
        PLAY [k8s-cluster] *************************************************************
        META: ran handlers
        
        TASK [nickhammond.logrotate | Install logrotate] *******************************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/logrotate/tasks/
        main.yml:2
        ok: [kubenode01] => {"cache_update_time": 1626373164, "cache_updated": false, "c
        hanged": false}
        
        TASK [nickhammond.logrotate | Setup logrotate.d scripts] ***********************
        task path: /home/wire/wire-server-deploy/ansible/roles-external/logrotate/tasks/
        main.yml:8
        ok: [kubenode01] => (item={'name': 'podlogs', 'path': '/var/lib/docker/container
        s/*/*.log', 'options': ['daily', 'missingok', 'rotate 2', 'maxage 1', 'copytrunc
        ate', 'nocreate', 'nocompress']}) => {"ansible_loop_var": "item", "changed": fal
        se, "checksum": "cbb2a32ef27e84ded753a3d749c86e0aed963f33", "dest": "/etc/logrot
        ate.d/podlogs", "gid": 0, "group": "root", "item": {"name": "podlogs", "options"
        : ["daily", "missingok", "rotate 2", "maxage 1", "copytruncate", "nocreate", "no
        compress"], "path": "/var/lib/docker/containers/*/*.log"}, "mode": "0600", "owne
        r": "root", "path": "/etc/logrotate.d/podlogs", "size": 143, "state": "file", "u
        id": 0}
        META: ran handlers
        META: ran handlers
        
        PLAY [Bringing kubeconfig in place] ********************************************
        META: ran handlers
        
        TASK [Checking if 'kubeconfig' file already exists] ****************************
        task path: /home/wire/wire-server-deploy/ansible/kubernetes.yml:24
        ok: [kubenode01] => {"changed": false, "stat": {"exists": false}}
        
        TASK [Renaming kubeconfig file provided by Kubespray] **************************
        task path: /home/wire/wire-server-deploy/ansible/kubernetes.yml:30
        [WARNING]: File '/home/wire/wire-server-
        deploy/ansible/inventory/demo/../kubeconfig.dec' created with default
        permissions '600'. The previous default was '666'. Specify 'mode' to avoid this
        warning.
        changed: [kubenode01] => {"changed": true, "checksum": "179a02f27dc2291907e21368
        77c19acf05e7f7bb", "dest": "/home/wire/wire-server-deploy/ansible/inventory/demo
        /../kubeconfig.dec", "gid": 1000, "group": "wire", "md5sum": "36531c748e07e40ff5
        5a8c885a47a821", "mode": "0600", "owner": "wire", "size": 5638, "src": "/home/wi
        re/.ansible/tmp/ansible-tmp-1626374715.7588642-23816-31720520987195/source", "st
        ate": "file", "uid": 1000}
        
        TASK [debug] *******************************************************************
        task path: /home/wire/wire-server-deploy/ansible/kubernetes.yml:34
        ok: [kubenode01] => {
            "msg": "TODO: Encrypt /home/wire/wire-server-deploy/ansible/inventory/demo/.
        ./kubeconfig.dec with sops"
        }
        META: ran handlers
        META: ran handlers
        
        PLAY [etcd] ********************************************************************
        META: ran handlers
        
        TASK [etcd-helpers : Add etcd helper scripts] **********************************
        task path: /home/wire/wire-server-deploy/ansible/roles/etcd-helpers/tasks/main.y
        ml:3
        ok: [kubenode01] => (item=etcd-health.sh) => {"ansible_loop_var": "item", "chang
        ed": false, "checksum": "9626e733cbc0c5ab3f512aba5031291a45c8ad43", "dest": "/us
        r/local/bin/etcd-health.sh", "gid": 0, "group": "root", "item": "etcd-health.sh"
        , "mode": "0755", "owner": "root", "path": "/usr/local/bin/etcd-health.sh", "siz
        e": 236, "state": "file", "uid": 0}
        ok: [kubenode01] => (item=etcdctl3.sh) => {"ansible_loop_var": "item", "changed"
        : false, "checksum": "cd5c88dfe6cd230dfdd01cd673280a2b5a3b66d0", "dest": "/usr/l
        ocal/bin/etcdctl3.sh", "gid": 0, "group": "root", "item": "etcdctl3.sh", "mode":
         "0755", "owner": "root", "path": "/usr/local/bin/etcdctl3.sh", "size": 249, "st
        ate": "file", "uid": 0}
        META: ran handlers
        META: ran handlers
        
        PLAY RECAP *********************************************************************
        kubenode01                 : ok=480  changed=31   unreachable=0    failed=0    s
        kipped=1036 rescued=0    ignored=0
        localhost                  : ok=1    changed=0    unreachable=0    failed=0    s
        kipped=0    rescued=0    ignored=0
        
[20:45:29] # Running command «find . -name 'admin.conf'» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ find . -name 'admin.conf'
        ./inventory/demo/artifacts/admin.conf
[20:45:35] # Running command «mkdir -p ~/.kube/» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ mkdir -p ~/.kube/
[20:45:40] # Running command «cp ./inventory/demo/artifacts/admin.conf ~/.kube/config» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ cp ./inventory/demo/artifacts/adm
        in.conf ~/.kube/config
[20:45:46] # Running command «KUBECONFIG=~/.kube/config» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ KUBECONFIG=~/.kube/config
[20:45:52] # Running command «which kubectl» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ which kubectl
        /home/wire/wire-server-deploy/.nix-env/bin/kubectl
[20:45:58] # Running command «kubectl version» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ kubectl version
        Client Version: version.Info{Major:"1", Minor:"19", GitVersion:"v1.19.7", GitCom
        mit:"1dd5338295409edcfff11505e7bb246f0d325d15", GitTreeState:"clean", BuildDate:
        "2021-01-13T13:23:52Z", GoVersion:"go1.15.5", Compiler:"gc", Platform:"linux/amd
        64"}
        Server Version: version.Info{Major:"1", Minor:"19", GitVersion:"v1.19.7", GitCom
        mit:"1dd5338295409edcfff11505e7bb246f0d325d15", GitTreeState:"clean", BuildDate:
        "2021-01-13T13:15:20Z", GoVersion:"go1.15.5", Compiler:"gc", Platform:"linux/amd
        64"}
[20:46:14] # Running command «kubectl get pods -A » on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ kubectl get pods -A
        NAMESPACE     NAME                                       READY   STATUS      RES
        TARTS   AGE
        default       brig-c8dd6c7ff-l9dnn                       1/1     Running     0
                8h
        default       brig-index-migrate-data-jmdlr              0/1     Completed   0
                8h
        default       cannon-0                                   1/1     Running     0
                8h
        default       cargohold-d75ffb69b-8qf6f                  1/1     Running     0
                8h
        default       cassandra-ephemeral-0                      1/1     Running     0
                21h
        default       cassandra-migrations-xctmr                 0/1     Completed   0
                8h
        default       demo-smtp-85557f6877-nb99z                 1/1     Running     0
                21h
        default       elasticsearch-ephemeral-86f4b8ff6f-jctn6   1/1     Running     0
                21h
        default       elasticsearch-index-create-4k5tl           0/1     Completed   0
                8h
        default       fake-aws-s3-77d9447b8f-r48k7               1/1     Running     0
                21h
        default       fake-aws-s3-reaper-78d9f58dd4-zq98j        1/1     Running     0
                21h
        default       fake-aws-sns-6c7c4b7479-xxdcp              2/2     Running     0
                21h
        default       fake-aws-sqs-59fbfbcbd4-8vfmd              2/2     Running     0
                21h
        default       galley-6db66679-gxkbk                      1/1     Running     0
                8h
        default       galley-migrate-data-cxg2j                  0/1     Completed   0
                8h
        default       gundeck-6f9bfbd8d8-9gjmf                   1/1     Running     0
                8h
        default       nginz-c88484d9c-gdhk8                      2/2     Running     0
                8h
        default       redis-ephemeral-master-0                   1/1     Running     0
                21h
        default       spar-5889cfcfc-6gbsc                       1/1     Running     0
                8h
        default       spar-5889cfcfc-97hr9                       1/1     Running     0
                8h
        default       spar-5889cfcfc-fg7n8                       1/1     Running     0
                8h
        default       spar-migrate-data-dq7wj                    0/1     Completed   0
                8h
        default       webapp-7678f9457-7rwk8                     1/1     Running     0
                8h
        kube-system   coredns-7677f9bb54-rx7g5                   0/1     Pending     0
                21h
        kube-system   coredns-7677f9bb54-s4cmj                   1/1     Running     0
                21h
        kube-system   dns-autoscaler-5b7b5c9b6f-6hds6            1/1     Running     0
                21h
        kube-system   kube-apiserver-kubenode01                  1/1     Running     0
                21h
        kube-system   kube-controller-manager-kubenode01         1/1     Running     0
                21h
        kube-system   kube-flannel-xktqb                         1/1     Running     0
                21h
        kube-system   kube-proxy-bdh7c                           1/1     Running     0
                117s
        kube-system   kube-scheduler-kubenode01                  1/1     Running     0
                21h
        kube-system   nodelocaldns-7v7dw                         1/1     Running     0
                21h
[20:46:19] # Running command «free -m && uptime» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ free -m && uptime
                      total        used        free      shared  buff/cache   available
        Mem:           2928         199         929           0        1799        2554
        Swap:          3943           1        3942
         18:46:17 up 33 min,  0 users,  load average: 0.77, 0.96, 1.25
[20:46:24] # Running command «free -m && uptime» on 95.216.208.159
        wire@arthur-demo:~$ free -m && uptime
                      total        used        free      shared  buff/cache   available
        Mem:          15661        5599         650           4        9410       10149
        Swap:             0           0           0
         20:46:22 up 3 days, 10:23,  0 users,  load average: 2.50, 1.57, 1.31
[20:46:32] # Running command «helm version» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ helm version
        version.BuildInfo{Version:"v3.5.2", GitCommit:"", GitTreeState:"", GoVersion:"go
        1.16"}
[20:46:40] # Running command «helm repo add wire https://s3-eu-west-1.amazonaws.com/public.wire.com/charts» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ helm repo add wire https://s3-eu-
        west-1.amazonaws.com/public.wire.com/charts
        "wire" has been added to your repositories
[20:46:46] # Running command «helm search repo wire/» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ helm search repo wire/
        NAME                            CHART VERSION   APP VERSION     DESCRIPTION
        
        wire/account-pages              0.130.0                         A Helm chart for
         the Wire account pages in Kube...
        wire/aws-ingress                2.110.0                         A Helm chart for
         ingresses (AWS specific) on Ku...
        wire/aws-storage                0.130.0                         AWS storage clas
        ses
        wire/backoffice                 2.110.0                         Backoffice tool
        
        wire/brig                       0.130.0                         Brig (part of Wi
        re Server) - User management
        wire/calling-test               2.110.0         1.0.14          Network testing
        tool for audio/video/signalling...
        wire/cannon                     0.130.0                         A Helm chart for
         cannon in Kubernetes
        wire/cargohold                  0.130.0                         Cargohold (part
        of Wire Server) - Asset storage
        wire/cassandra-ephemeral        0.130.0                         Wrapper chart fo
        r incubator/cassandra with cust...
        wire/cassandra-external         2.110.0                         Refer to cassand
        ra IPs located outside kubernet...
        wire/cassandra-migrations       0.130.0                         cassandra databa
        se schema migration for gundeck...
        wire/databases-ephemeral        2.110.0                         A Helm chart in-
        memory, ephemeral databases for...
        wire/demo-smtp                  2.110.0         1.0             A demo helm char
        t to send emails. Not productio...
        wire/elasticsearch-curator      2.110.0                         Wrapper chart fo
        r stable/elasticsearch-curator
        wire/elasticsearch-ephemeral    2.110.0                         Dummy ephemeral
        elasticsearch
        wire/elasticsearch-external     2.110.0                         Refer to elastic
        search IPs located outside kube...
        wire/elasticsearch-index        0.130.0                         Elasticsearch in
        dex for brig
        wire/fake-aws                   2.110.0                         A Helm chart for
         fake-aws services (replacing r...
        wire/fake-aws-dynamodb          0.130.0                         Dummy ephemeral
        DynamoDB service
        wire/fake-aws-s3                2.110.0                         Wrapper chart fo
        r stable/minio
        wire/fake-aws-ses               0.130.0                         Dummy ephemeral
        SES service (based on localstack)
        wire/fake-aws-sns               0.130.0                         Dummy ephemeral
        SNS service (based on localstack)
        wire/fake-aws-sqs               2.110.0                         Dummy ephemeral
        SQS service
        wire/fluent-bit                 2.110.0                         Wrapper chart fo
        r stable/fluent-bit
        wire/galley                     0.130.0                         Galley (part of
        Wire Server) - Conversations
        wire/gundeck                    0.130.0                         Gundeck (part of
         Wire Server) - Push Notificati...
        wire/kibana                     2.110.0                         Wrapper chart fo
        r stable/kibana
        wire/legalhold                  0.130.0                         A Helm chart for
         legalhold
        wire/metallb                    0.130.0                         A Helm chart for
         metallb on Kubernetes
        wire/minio-external             2.110.0                         Refer to minio I
        Ps located outside kubernetes b...
        wire/nginx-ingress-controller   2.110.0                         A Helm chart for
         an ingress controller (using n...
        wire/nginx-ingress-services     2.110.0                         A Helm chart for
         ingresses and services on Kube...
        wire/nginx-lb-ingress           0.1.3                           A Helm chart for
         ingresses (using nginx) on Kub...
        wire/nginz                      0.130.0                         A Helm chart for
         nginz in Kubernetes
        wire/proxy                      0.130.0                         Proxy (part of W
        ire Server) - 3rd party proxy s...
        wire/reaper                     2.110.0         0.1.0           A helm charts to
         restart cannons if redis-ephem...
        wire/redis-ephemeral            2.110.0                         Wrapper chart fo
        r stable/redis
        wire/sftd                       2.110.0         1.0.88          SFTD is a compon
        ent for engaging in conference ...
        wire/spar                       0.130.0                         Spar (part of Wi
        re Server) - SSO Service
        wire/team-settings              0.130.0                         A Helm chart for
         the Wire team-settings in Kube...
        wire/webapp                     0.130.0                         A Helm chart for
         the Wire webapp in Kubernetes
        wire/wire-server                2.110.0                         A Helm chart for
         wire-server https://github.com...
        wire/wire-server-metrics        2.110.0         1.0             Adds monitoring
        for the kubernetes cluster and ...
[20:46:52] # Running command «kubectl get pods -A» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ kubectl get pods -A
        NAMESPACE     NAME                                       READY   STATUS      RES
        TARTS   AGE
        default       brig-c8dd6c7ff-l9dnn                       1/1     Running     0
                8h
        default       brig-index-migrate-data-jmdlr              0/1     Completed   0
                8h
        default       cannon-0                                   1/1     Running     0
                8h
        default       cargohold-d75ffb69b-8qf6f                  1/1     Running     0
                8h
        default       cassandra-ephemeral-0                      1/1     Running     0
                21h
        default       cassandra-migrations-xctmr                 0/1     Completed   0
                8h
        default       demo-smtp-85557f6877-nb99z                 1/1     Running     0
                21h
        default       elasticsearch-ephemeral-86f4b8ff6f-jctn6   1/1     Running     0
                21h
        default       elasticsearch-index-create-4k5tl           0/1     Completed   0
                8h
        default       fake-aws-s3-77d9447b8f-r48k7               1/1     Running     0
                21h
        default       fake-aws-s3-reaper-78d9f58dd4-zq98j        1/1     Running     0
                21h
        default       fake-aws-sns-6c7c4b7479-xxdcp              2/2     Running     0
                21h
        default       fake-aws-sqs-59fbfbcbd4-8vfmd              2/2     Running     0
                21h
        default       galley-6db66679-gxkbk                      1/1     Running     0
                8h
        default       galley-migrate-data-cxg2j                  0/1     Completed   0
                8h
        default       gundeck-6f9bfbd8d8-9gjmf                   1/1     Running     0
                8h
        default       nginz-c88484d9c-gdhk8                      2/2     Running     0
                8h
        default       redis-ephemeral-master-0                   1/1     Running     0
                21h
        default       spar-5889cfcfc-6gbsc                       1/1     Running     0
                8h
        default       spar-5889cfcfc-97hr9                       1/1     Running     0
                8h
        default       spar-5889cfcfc-fg7n8                       1/1     Running     0
                8h
        default       spar-migrate-data-dq7wj                    0/1     Completed   0
                8h
        default       webapp-7678f9457-7rwk8                     1/1     Running     0
                8h
        kube-system   coredns-7677f9bb54-rx7g5                   0/1     Pending     0
                21h
        kube-system   coredns-7677f9bb54-s4cmj                   1/1     Running     0
                21h
        kube-system   dns-autoscaler-5b7b5c9b6f-6hds6            1/1     Running     0
                21h
        kube-system   kube-apiserver-kubenode01                  1/1     Running     0
                21h
        kube-system   kube-controller-manager-kubenode01         1/1     Running     0
                21h
        kube-system   kube-flannel-xktqb                         1/1     Running     0
                21h
        kube-system   kube-proxy-bdh7c                           1/1     Running     0
                2m35s
        kube-system   kube-scheduler-kubenode01                  1/1     Running     0
                21h
        kube-system   nodelocaldns-7v7dw                         1/1     Running     0
                21h
[20:47:16] # Running command «helm upgrade --install databases-ephemeral wire/databases-ephemeral --wait --debug» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ helm upgrade --install databases-
        ephemeral wire/databases-ephemeral --wait --debug
        history.go:56: [debug] getting history for release databases-ephemeral
        upgrade.go:123: [debug] preparing upgrade for databases-ephemeral
        upgrade.go:131: [debug] performing update for databases-ephemeral
        upgrade.go:303: [debug] creating upgraded release for databases-ephemeral
        client.go:201: [debug] checking 10 resources for changes
        client.go:464: [debug] Looks like there are no changes for ConfigMap "redis-ephe
        meral-scripts"
        client.go:464: [debug] Looks like there are no changes for ConfigMap "redis-ephe
        meral"
        client.go:464: [debug] Looks like there are no changes for ConfigMap "redis-ephe
        meral-health"
        client.go:464: [debug] Looks like there are no changes for Service "cassandra-ep
        hemeral"
        client.go:464: [debug] Looks like there are no changes for Service "elasticsearc
        h-ephemeral"
        client.go:464: [debug] Looks like there are no changes for Service "redis-epheme
        ral-headless"
        client.go:464: [debug] Looks like there are no changes for Service "redis-epheme
        ral-master"
        wait.go:53: [debug] beginning wait for 10 resources with timeout of 5m0s
        upgrade.go:138: [debug] updating status for upgraded release for databases-ephem
        eral
        Release "databases-ephemeral" has been upgraded. Happy Helming!
        NAME: databases-ephemeral
        LAST DEPLOYED: Thu Jul 15 18:47:07 2021
        NAMESPACE: default
        STATUS: deployed
        REVISION: 4
        TEST SUITE: None
        USER-SUPPLIED VALUES:
        {}
        
        COMPUTED VALUES:
        cassandra-ephemeral:
          cassandra-ephemeral:
            affinity: {}
            argsOverrides: []
            backup:
              annotations:
                iam.amazonaws.com/role: cain
              destination: s3://bucket/cassandra
              enabled: false
              env:
              - name: AWS_REGION
                value: us-east-1
              extraArgs: []
              image:
                repository: maorfr/cain
                tag: 0.6.0
              resources:
                limits:
                  cpu: 1
                  memory: 1Gi
                requests:
                  cpu: 1
                  memory: 1Gi
              schedule:
              - cron: 0 7 * * *
                keyspace: keyspace1
              - cron: 30 7 * * *
                keyspace: keyspace2
            commandOverrides: []
            config:
              cluster_domain: cluster.local
              cluster_name: cassandra
              cluster_size: 1
              dc_name: DC1
              endpoint_snitch: SimpleSnitch
              heap_new_size: 1024M
              max_heap_size: 2048M
              num_tokens: 256
              ports:
                cql: 9042
                thrift: 9160
              rack_name: RAC1
              seed_size: 1
              start_rpc: false
            configOverrides: {}
            env: {}
            exporter:
              enabled: false
              image:
                repo: criteord/cassandra_exporter
                tag: 2.0.2
              jvmOpts: ""
              port: 5556
              resources: {}
              servicemonitor: true
            global: {}
            hostNetwork: false
            image:
              pullPolicy: IfNotPresent
              repo: cassandra
              tag: 3.11.3
            livenessProbe:
              failureThreshold: 3
              initialDelaySeconds: 90
              periodSeconds: 30
              successThreshold: 1
              timeoutSeconds: 5
            persistence:
              accessMode: ReadWriteOnce
              enabled: false
              size: 10Gi
            podAnnotations: {}
            podDisruptionBudget: {}
            podLabels: {}
            podManagementPolicy: OrderedReady
            podSettings:
              terminationGracePeriodSeconds: 30
            rbac:
              create: true
            readinessProbe:
              address: ${POD_IP}
              failureThreshold: 3
              initialDelaySeconds: 90
              periodSeconds: 30
              successThreshold: 1
              timeoutSeconds: 5
            resources:
              limits:
                cpu: "4"
                memory: 4.0Gi
              requests:
                cpu: "1"
                memory: 2.0Gi
            securityContext:
              enabled: false
              fsGroup: 999
              runAsUser: 999
            service:
              type: ClusterIP
            serviceAccount:
              create: true
            tolerations: []
            updateStrategy:
              type: RollingUpdate
          global: {}
        elasticsearch-ephemeral:
          global: {}
          image:
            repository: elasticsearch
            tag: 6.7.1
          resources:
            limits:
              cpu: 2000m
              memory: 4Gi
            requests:
              cpu: 250m
              memory: 500Mi
          service:
            httpPort: 9200
            transportPort: 9300
        redis-ephemeral:
          global: {}
          redis-ephemeral:
            cluster:
              enabled: false
              slaveCount: 2
            clusterDomain: cluster.local
            configmap: |-
              # Enable AOF https://redis.io/topics/persistence#append-only-file
              appendonly yes
              # Disable RDB persistence, AOF persistence already enabled.
              save ""
            containerSecurityContext:
              enabled: true
              runAsUser: 1001
            global:
              redis: {}
            image:
              pullPolicy: IfNotPresent
              registry: docker.io
              repository: bitnami/redis
              tag: 6.0.9-debian-10-r0
            master:
              affinity: {}
              command: /run.sh
              configmap: null
              customLivenessProbe: {}
              customReadinessProbe: {}
              disableCommands:
              - FLUSHDB
              - FLUSHALL
              extraEnvVars: []
              extraEnvVarsCM: []
              extraEnvVarsSecret: []
              extraFlags: []
              livenessProbe:
                enabled: true
                failureThreshold: 5
                initialDelaySeconds: 5
                periodSeconds: 5
                successThreshold: 1
                timeoutSeconds: 5
              persistence:
                accessModes:
                - ReadWriteOnce
                enabled: false
                matchExpressions: {}
                matchLabels: {}
                path: /data
                size: 8Gi
                subPath: ""
              podAnnotations: {}
              podLabels: {}
              preExecCmds: ""
              priorityClassName: ""
              readinessProbe:
                enabled: true
                failureThreshold: 5
                initialDelaySeconds: 5
                periodSeconds: 5
                successThreshold: 1
                timeoutSeconds: 1
              resources:
                limits:
                  cpu: 1000m
                  memory: 1024Mi
                requests:
                  cpu: 500m
                  memory: 512Mi
              service:
                annotations: {}
                labels: {}
                port: 6379
                type: ClusterIP
              shareProcessNamespace: false
              statefulset:
                labels: {}
                updateStrategy: RollingUpdate
            metrics:
              enabled: false
              image:
                pullPolicy: IfNotPresent
                registry: docker.io
                repository: bitnami/redis-exporter
                tag: 1.12.1-debian-10-r11
              podAnnotations:
                prometheus.io/port: "9121"
                prometheus.io/scrape: "true"
              prometheusRule:
                additionalLabels: {}
                enabled: false
                namespace: ""
                rules: []
              service:
                annotations: {}
                labels: {}
                type: ClusterIP
              serviceMonitor:
                enabled: false
                selector:
                  prometheus: kube-prometheus
            networkPolicy:
              enabled: false
              ingressNSMatchLabels: {}
              ingressNSPodMatchLabels: {}
            password: ""
            persistence: {}
            podDisruptionBudget:
              enabled: false
              minAvailable: 1
            podSecurityPolicy:
              create: false
            rbac:
              create: false
              role:
                rules: []
            redisPort: 6379
            securityContext:
              enabled: true
              fsGroup: 1001
            sentinel:
              customLivenessProbe: {}
              customReadinessProbe: {}
              downAfterMilliseconds: 60000
              enabled: false
              failoverTimeout: 18000
              image:
                pullPolicy: IfNotPresent
                registry: docker.io
                repository: bitnami/redis-sentinel
                tag: 6.0.8-debian-10-r55
              initialCheckTimeout: 5
              livenessProbe:
                enabled: true
                failureThreshold: 5
                initialDelaySeconds: 5
                periodSeconds: 5
                successThreshold: 1
                timeoutSeconds: 5
              masterSet: mymaster
              parallelSyncs: 1
              port: 26379
              quorum: 2
              readinessProbe:
                enabled: true
                failureThreshold: 5
                initialDelaySeconds: 5
                periodSeconds: 5
                successThreshold: 1
                timeoutSeconds: 1
              service:
                annotations: {}
                labels: {}
                redisPort: 6379
                sentinelPort: 26379
                type: ClusterIP
              staticID: false
              usePassword: true
            serviceAccount:
              create: false
            slave:
              affinity: {}
              command: /run.sh
              customLivenessProbe: {}
              customReadinessProbe: {}
              disableCommands:
              - FLUSHDB
              - FLUSHALL
              extraEnvVars: []
              extraEnvVarsCM: []
              extraEnvVarsSecret: []
              extraFlags: []
              livenessProbe:
                enabled: true
                failureThreshold: 5
                initialDelaySeconds: 30
                periodSeconds: 10
                successThreshold: 1
                timeoutSeconds: 5
              persistence:
                accessModes:
                - ReadWriteOnce
                enabled: true
                matchExpressions: {}
                matchLabels: {}
                path: /data
                size: 8Gi
                subPath: ""
              podAnnotations: {}
              podLabels: {}
              port: 6379
              preExecCmds: ""
              readinessProbe:
                enabled: true
                failureThreshold: 5
                initialDelaySeconds: 5
                periodSeconds: 10
                successThreshold: 1
                timeoutSeconds: 10
              service:
                annotations: {}
                labels: {}
                port: 6379
                type: ClusterIP
              shareProcessNamespace: false
              spreadConstraints: {}
              statefulset:
                labels: {}
                updateStrategy: RollingUpdate
            sysctlImage:
              command: []
              enabled: false
              mountHostSys: false
              pullPolicy: Always
              registry: docker.io
              repository: bitnami/minideb
              resources: {}
              tag: buster
            tls:
              authClients: true
              enabled: false
            usePassword: false
            usePasswordFile: false
            volumePermissions:
              enabled: false
              image:
                pullPolicy: Always
                registry: docker.io
                repository: bitnami/minideb
                tag: buster
              resources: {}
              securityContext:
                runAsUser: 0
        
        HOOKS:
        MANIFEST:
        ---
        # Source: databases-ephemeral/charts/redis-ephemeral/charts/redis-ephemeral/temp
        lates/configmap-scripts.yaml
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: redis-ephemeral-scripts
          namespace: "default"
          labels:
            app: redis-ephemeral
            chart: redis-ephemeral-11.3.4
            heritage: Helm
            release: databases-ephemeral
        data:
          start-master.sh: |
            #!/bin/bash
            useradd redis
            chown -R redis /data
            if [[ -n $REDIS_PASSWORD_FILE ]]; then
              password_aux=`cat ${REDIS_PASSWORD_FILE}`
              export REDIS_PASSWORD=$password_aux
            fi
            if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then
              cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/maste
        r.conf
            fi
            if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then
              cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.
        conf
            fi
            ARGS=("--port" "${REDIS_PORT}")
            ARGS+=("--protected-mode" "no")
            ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
            ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
            exec /run.sh "${ARGS[@]}"
        ---
        # Source: databases-ephemeral/charts/redis-ephemeral/charts/redis-ephemeral/temp
        lates/configmap.yaml
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: redis-ephemeral
          namespace: "default"
          labels:
            app: redis-ephemeral
            chart: redis-ephemeral-11.3.4
            heritage: Helm
            release: databases-ephemeral
        data:
          redis.conf: |-
            # User-supplied configuration:
            # Enable AOF https://redis.io/topics/persistence#append-only-file
            appendonly yes
            # Disable RDB persistence, AOF persistence already enabled.
            save ""
          master.conf: |-
            dir /data
            rename-command FLUSHDB ""
            rename-command FLUSHALL ""
          replica.conf: |-
            dir /data
            slave-read-only yes
            rename-command FLUSHDB ""
            rename-command FLUSHALL ""
        ---
        # Source: databases-ephemeral/charts/redis-ephemeral/charts/redis-ephemeral/temp
        lates/health-configmap.yaml
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: redis-ephemeral-health
          namespace: "default"
          labels:
            app: redis-ephemeral
            chart: redis-ephemeral-11.3.4
            heritage: Helm
            release: databases-ephemeral
        data:
          ping_readiness_local.sh: |-
            #!/bin/bash
            response=$(
              timeout -s 3 $1 \
              redis-cli \
                -h localhost \
                -p $REDIS_PORT \
                ping
            )
            if [ "$response" != "PONG" ]; then
              echo "$response"
              exit 1
            fi
          ping_liveness_local.sh: |-
            #!/bin/bash
            response=$(
              timeout -s 3 $1 \
              redis-cli \
                -h localhost \
                -p $REDIS_PORT \
                ping
            )
            if [ "$response" != "PONG" ] && [ "$response" != "LOADING Redis is loading t
        he dataset in memory" ]; then
              echo "$response"
              exit 1
            fi
          ping_readiness_master.sh: |-
            #!/bin/bash
             response=$(
              timeout -s 3 $1 \
              redis-cli \
                -h $REDIS_MASTER_HOST \
                -p $REDIS_MASTER_PORT_NUMBER \
                ping
            )
            if [ "$response" != "PONG" ]; then
              echo "$response"
              exit 1
            fi
          ping_liveness_master.sh: |-
            #!/bin/bash
            response=$(
              timeout -s 3 $1 \
              redis-cli \
                -h $REDIS_MASTER_HOST \
                -p $REDIS_MASTER_PORT_NUMBER \
                ping
            )
            if [ "$response" != "PONG" ] && [ "$response" != "LOADING Redis is loading t
        he dataset in memory" ]; then
              echo "$response"
              exit 1
            fi
          ping_readiness_local_and_master.sh: |-
            script_dir="$(dirname "$0")"
            exit_status=0
            "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
            "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
            exit $exit_status
          ping_liveness_local_and_master.sh: |-
            script_dir="$(dirname "$0")"
            exit_status=0
            "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
            "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
            exit $exit_status
        ---
        # Source: databases-ephemeral/charts/cassandra-ephemeral/charts/cassandra-epheme
        ral/templates/service.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: cassandra-ephemeral
          labels:
            app: cassandra-ephemeral
            chart: cassandra-ephemeral-0.13.3
            release: databases-ephemeral
            heritage: Helm
        spec:
          clusterIP: None
          type: ClusterIP
          ports:
          - name: intra
            port: 7000
            targetPort: 7000
          - name: tls
            port: 7001
            targetPort: 7001
          - name: jmx
            port: 7199
            targetPort: 7199
          - name: cql
            port: 9042
            targetPort: 9042
          - name: thrift
            port: 9160
            targetPort: 9160
          selector:
            app: cassandra-ephemeral
            release: databases-ephemeral
        ---
        # Source: databases-ephemeral/charts/elasticsearch-ephemeral/templates/es-svc.ya
        ml
        apiVersion: v1
        kind: Service
        metadata:
          name: elasticsearch-ephemeral
          labels:
            wireService: elasticsearch-ephemeral
            app: elasticsearch-ephemeral
            chart: "elasticsearch-ephemeral-2.110.0"
            release: "databases-ephemeral"
            heritage: "Helm"
            component: elasticsearch-ephemeral
        spec:
          type: ClusterIP
          selector:
            component: elasticsearch-ephemeral
          ports:
          - name: http
            port: 9200
            targetPort: 9200
            protocol: TCP
          - name: transport
            port: 9300
            targetPort: 9300
            protocol: TCP
        ---
        # Source: databases-ephemeral/charts/redis-ephemeral/charts/redis-ephemeral/temp
        lates/headless-svc.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: redis-ephemeral-headless
          namespace: "default"
          labels:
            app: redis-ephemeral
            chart: redis-ephemeral-11.3.4
            release: databases-ephemeral
            heritage: Helm
        spec:
          type: ClusterIP
          clusterIP: None
          ports:
            - name: redis
              port: 6379
              targetPort: redis
          selector:
            app: redis-ephemeral
            release: databases-ephemeral
        ---
        # Source: databases-ephemeral/charts/redis-ephemeral/charts/redis-ephemeral/temp
        lates/redis-master-svc.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: redis-ephemeral-master
          namespace: "default"
          labels:
            app: redis-ephemeral
            chart: redis-ephemeral-11.3.4
            release: databases-ephemeral
            heritage: Helm
        spec:
          type: ClusterIP
          ports:
            - name: redis
              port: 6379
              targetPort: redis
          selector:
            app: redis-ephemeral
            release: databases-ephemeral
            role: master
        ---
        # Source: databases-ephemeral/charts/elasticsearch-ephemeral/templates/es.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: elasticsearch-ephemeral
          labels:
            wireService: elasticsearch-ephemeral
            app: elasticsearch-ephemeral
            chart: "elasticsearch-ephemeral-2.110.0"
            release: "databases-ephemeral"
            heritage: "Helm"
            component: elasticsearch-ephemeral
        spec:
          replicas: 1
          selector:
            matchLabels:
              component: elasticsearch-ephemeral
          template:
            metadata:
              labels:
                component: elasticsearch-ephemeral
            spec:
              containers:
              - name: es
                image: "elasticsearch:6.7.1"
                env:
                - name: MAX_HEAP_SIZE
                  value: "2048"
                - name: HEAP_NEWSIZE
                  value: "800M"
                - name: "bootstrap.system_call_filter"
                  value: "false"
                - name: "discovery.type"
                  value: "single-node"
                ports:
                - containerPort: 9200
                  name: http
                  protocol: TCP
                - containerPort: 9300
                  name: transport
                  protocol: TCP
                volumeMounts:
                - name: storage
                  mountPath: /data
                resources:
                    limits:
                      cpu: 2000m
                      memory: 4Gi
                    requests:
                      cpu: 250m
                      memory: 500Mi
              volumes:
                - emptyDir:
                    medium: ""
                  name: "storage"
        ---
        # Source: databases-ephemeral/charts/cassandra-ephemeral/charts/cassandra-epheme
        ral/templates/statefulset.yaml
        apiVersion: apps/v1
        kind: StatefulSet
        metadata:
          name: cassandra-ephemeral
          labels:
            app: cassandra-ephemeral
            chart: cassandra-ephemeral-0.13.3
            release: databases-ephemeral
            heritage: Helm
        spec:
          selector:
            matchLabels:
              app: cassandra-ephemeral
              release: databases-ephemeral
          serviceName: cassandra-ephemeral
          replicas: 1
          podManagementPolicy: OrderedReady
          updateStrategy:
            type: RollingUpdate
          template:
            metadata:
              labels:
                app: cassandra-ephemeral
                release: databases-ephemeral
            spec:
              hostNetwork: false
              containers:
              - name: cassandra-ephemeral
                image: "cassandra:3.11.3"
                imagePullPolicy: "IfNotPresent"
                resources:
                  limits:
                    cpu: "4"
                    memory: 4.0Gi
                  requests:
                    cpu: "1"
                    memory: 2.0Gi
                env:
                - name: CASSANDRA_SEEDS
                  value: "cassandra-ephemeral-0.cassandra-ephemeral.default.svc.cluster.
        local"
                - name: MAX_HEAP_SIZE
                  value: "2048M"
                - name: HEAP_NEWSIZE
                  value: "1024M"
                - name: CASSANDRA_ENDPOINT_SNITCH
                  value: "SimpleSnitch"
                - name: CASSANDRA_CLUSTER_NAME
                  value: "cassandra"
                - name: CASSANDRA_DC
                  value: "DC1"
                - name: CASSANDRA_RACK
                  value: "RAC1"
                - name: CASSANDRA_START_RPC
                  value: "false"
                - name: POD_IP
                  valueFrom:
                    fieldRef:
                      fieldPath: status.podIP
                livenessProbe:
                  exec:
                    command: [ "/bin/sh", "-c", "nodetool status" ]
                  initialDelaySeconds: 90
                  periodSeconds: 30
                  timeoutSeconds: 5
                  successThreshold: 1
                  failureThreshold: 3
                readinessProbe:
                  exec:
                    command: [ "/bin/sh", "-c", "nodetool status | grep -E \"^UN\\s+${PO
        D_IP}\"" ]
                  initialDelaySeconds: 90
                  periodSeconds: 30
                  timeoutSeconds: 5
                  successThreshold: 1
                  failureThreshold: 3
                ports:
                - name: intra
                  containerPort: 7000
                - name: tls
                  containerPort: 7001
                - name: jmx
                  containerPort: 7199
                - name: cql
                  containerPort: 9042
                - name: thrift
                  containerPort: 9160
                volumeMounts:
                - name: data
                  mountPath: /var/lib/cassandra
                lifecycle:
                  preStop:
                    exec:
                      command: ["/bin/sh", "-c", "exec nodetool decommission"]
              terminationGracePeriodSeconds: 30
              volumes:
              - name: data
                emptyDir: {}
        ---
        # Source: databases-ephemeral/charts/redis-ephemeral/charts/redis-ephemeral/temp
        lates/redis-master-statefulset.yaml
        apiVersion: apps/v1
        kind: StatefulSet
        metadata:
          name: redis-ephemeral-master
          namespace: "default"
          labels:
            app: redis-ephemeral
            chart: redis-ephemeral-11.3.4
            release: databases-ephemeral
            heritage: Helm
        spec:
          selector:
            matchLabels:
              app: redis-ephemeral
              release: databases-ephemeral
              role: master
          serviceName: redis-ephemeral-headless
          template:
            metadata:
              labels:
                app: redis-ephemeral
                chart: redis-ephemeral-11.3.4
                release: databases-ephemeral
                role: master
              annotations:
                checksum/health: e8c90bae4ee68e3ea33f86992512385397556e9993eb7bbd6130358
        80121814b
                checksum/configmap: f3cb092ae316f13c0503c3bcf837885791c9f5365d6168ea83b3
        c45f40743f5a
                checksum/secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991
        b7852b855
            spec:
        
              securityContext:
                fsGroup: 1001
              serviceAccountName: default
              containers:
                - name: redis-ephemeral
                  image: docker.io/bitnami/redis:6.0.9-debian-10-r0
                  imagePullPolicy: "IfNotPresent"
                  securityContext:
                    runAsUser: 1001
                  command:
                    - /bin/bash
                    - -c
                    - /opt/bitnami/scripts/start-scripts/start-master.sh
                  env:
                    - name: REDIS_REPLICATION_MODE
                      value: master
                    - name: ALLOW_EMPTY_PASSWORD
                      value: "yes"
                    - name: REDIS_TLS_ENABLED
                      value: "no"
                    - name: REDIS_PORT
                      value: "6379"
                  ports:
                    - name: redis
                      containerPort: 6379
                  livenessProbe:
                    initialDelaySeconds: 5
                    periodSeconds: 5
                    # One second longer than command timeout should prevent generation o
        f zombie processes.
                    timeoutSeconds: 6
                    successThreshold: 1
                    failureThreshold: 5
                    exec:
                      command:
                        - sh
                        - -c
                        - /health/ping_liveness_local.sh 5
                  readinessProbe:
                    initialDelaySeconds: 5
                    periodSeconds: 5
                    timeoutSeconds: 2
                    successThreshold: 1
                    failureThreshold: 5
                    exec:
                      command:
                        - sh
                        - -c
                        - /health/ping_readiness_local.sh 1
                  resources:
                    limits:
                      cpu: 1000m
                      memory: 1024Mi
                    requests:
                      cpu: 500m
                      memory: 512Mi
                  volumeMounts:
                    - name: start-scripts
                      mountPath: /opt/bitnami/scripts/start-scripts
                    - name: health
                      mountPath: /health
                    - name: redis-data
                      mountPath: /data
                      subPath:
                    - name: config
                      mountPath: /opt/bitnami/redis/mounted-etc
                    - name: redis-tmp-conf
                      mountPath: /opt/bitnami/redis/etc/
              volumes:
                - name: start-scripts
                  configMap:
                    name: redis-ephemeral-scripts
                    defaultMode: 0755
                - name: health
                  configMap:
                    name: redis-ephemeral-health
                    defaultMode: 0755
                - name: config
                  configMap:
                    name: redis-ephemeral
                - name: "redis-data"
                  emptyDir: {}
                - name: redis-tmp-conf
                  emptyDir: {}
          updateStrategy:
            type: RollingUpdate
        
        NOTES:
        You now have an in-memory, non-persistent, non-highly-available set of databases
        :
        
        * cassandra-ephemeral
        * elasticsearch-ephemeral
        * redis-ephemeral
        
        !! WARNING WARNING !!
        This is fine for testing and demo purposes, but NOT for a production use case.
        !! WARNING WARNING !!
        
        Note that before use of these databases for wire-server components, an index (in
         the case of elasticsearch) and a set of cassandra-migrations (in the case of ca
        ssandra) have to be applied. This comes bundled with the wire-server chart (see
        cassandra-migrations and elasticsearch-index charts for details)
[20:47:43] # Running command «helm upgrade --install fake-aws wire/fake-aws --wait --debug» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ helm upgrade --install fake-aws w
        ire/fake-aws --wait --debug
        history.go:56: [debug] getting history for release fake-aws
        upgrade.go:123: [debug] preparing upgrade for fake-aws
        upgrade.go:131: [debug] performing update for fake-aws
        upgrade.go:303: [debug] creating upgraded release for fake-aws
        client.go:201: [debug] checking 13 resources for changes
        client.go:464: [debug] Looks like there are no changes for ServiceAccount "fake-
        aws-s3-update-prometheus-secret"
        client.go:464: [debug] Looks like there are no changes for ServiceAccount "fake-
        aws-s3"
        client.go:464: [debug] Looks like there are no changes for Secret "fake-aws-s3"
        client.go:464: [debug] Looks like there are no changes for ConfigMap "fake-aws-s
        3"
        client.go:464: [debug] Looks like there are no changes for Role "fake-aws-s3-upd
        ate-prometheus-secret"
        client.go:464: [debug] Looks like there are no changes for RoleBinding "fake-aws
        -s3-update-prometheus-secret"
        client.go:464: [debug] Looks like there are no changes for Service "fake-aws-s3"
        client.go:464: [debug] Looks like there are no changes for Service "fake-aws-sns
        "
        client.go:464: [debug] Looks like there are no changes for Service "fake-aws-sqs
        "
        client.go:464: [debug] Looks like there are no changes for Deployment "fake-aws-
        s3-reaper"
        client.go:464: [debug] Looks like there are no changes for Deployment "fake-aws-
        sns"
        wait.go:53: [debug] beginning wait for 13 resources with timeout of 5m0s
        client.go:282: [debug] Starting delete for "fake-aws-s3-make-bucket-job" Job
        client.go:311: [debug] jobs.batch "fake-aws-s3-make-bucket-job" not found
        client.go:122: [debug] creating 1 resource(s)
        client.go:491: [debug] Watching for changes to Job fake-aws-s3-make-bucket-job w
        ith timeout of 5m0s
        client.go:519: [debug] Add/Modify event for fake-aws-s3-make-bucket-job: ADDED
        client.go:558: [debug] fake-aws-s3-make-bucket-job: Jobs active: 1, jobs failed:
         0, jobs succeeded: 0
        client.go:519: [debug] Add/Modify event for fake-aws-s3-make-bucket-job: MODIFIE
        D
        client.go:282: [debug] Starting delete for "fake-aws-s3-make-bucket-job" Job
        upgrade.go:138: [debug] updating status for upgraded release for fake-aws
        Release "fake-aws" has been upgraded. Happy Helming!
        NAME: fake-aws
        LAST DEPLOYED: Thu Jul 15 18:47:30 2021
        NAMESPACE: default
        STATUS: deployed
        REVISION: 4
        TEST SUITE: None
        USER-SUPPLIED VALUES:
        {}
        
        COMPUTED VALUES:
        fake-aws-s3:
          enabled: true
          global: {}
          minio:
            DeploymentUpdate:
              maxSurge: 100%
              maxUnavailable: 0
              type: RollingUpdate
            StatefulSetUpdate:
              updateStrategy: RollingUpdate
            accessKey: dummykey
            affinity: {}
            azuregateway:
              enabled: false
              replicas: 4
            bucketRoot: ""
            buckets:
            - name: dummy-bucket
              policy: none
              purge: true
            - name: assets
              policy: none
              purge: false
            - name: public
              policy: public
              purge: false
            certsPath: /etc/minio/certs/
            clusterDomain: cluster.local
            configPathmc: /etc/minio/mc/
            defaultBucket:
              enabled: false
              name: bucket
              policy: none
              purge: false
            drivesPerNode: 1
            environment:
              MINIO_BROWSER: "off"
            etcd:
              clientCert: ""
              clientCertKey: ""
              corednsPathPrefix: ""
              endpoints: []
              pathPrefix: ""
            existingSecret: ""
            extraArgs: []
            fullnameOverride: fake-aws-s3
            gcsgateway:
              enabled: false
              gcsKeyJson: ""
              projectId: ""
              replicas: 4
            global: {}
            helmKubectlJqImage:
              pullPolicy: IfNotPresent
              repository: bskim45/helm-kubectl-jq
              tag: 3.1.0
            image:
              pullPolicy: IfNotPresent
              repository: minio/minio
              tag: RELEASE.2020-10-18T21-54-12Z
            imagePullSecrets: []
            ingress:
              annotations: {}
              enabled: false
              hosts:
              - chart-example.local
              labels: {}
              path: /
              tls: []
            makeBucketJob:
              resources:
                requests:
                  memory: 128Mi
              securityContext:
                enabled: false
                fsGroup: 1000
                runAsGroup: 1000
                runAsUser: 1000
            mcImage:
              pullPolicy: IfNotPresent
              repository: minio/mc
              tag: RELEASE.2020-10-03T02-54-56Z
            metrics:
              serviceMonitor:
                additionalLabels: {}
                enabled: false
            mode: standalone
            mountPath: /export
            nameOverride: ""
            nasgateway:
              enabled: false
              replicas: 4
            networkPolicy:
              allowExternal: true
              enabled: false
            nodeSelector: {}
            persistence:
              VolumeName: ""
              accessMode: ReadWriteOnce
              enabled: false
              existingClaim: ""
              size: 500Gi
              storageClass: ""
              subPath: ""
            podAnnotations: {}
            podDisruptionBudget:
              enabled: false
              maxUnavailable: 1
            podLabels: {}
            priorityClassName: ""
            replicas: 4
            resources:
              requests:
                memory: 4Gi
            s3gateway:
              accessKey: ""
              enabled: false
              replicas: 4
              secretKey: ""
              serviceEndpoint: ""
            secretKey: dummysecret
            securityContext:
              enabled: true
              fsGroup: 1000
              runAsGroup: 1000
              runAsUser: 1000
            service:
              annotations: {}
              externalIPs: []
              nodePort: 32000
              port: 9000
              type: ClusterIP
            serviceAccount:
              create: true
            tls:
              certSecret: ""
              enabled: false
              privateKey: private.key
              publicCrt: public.crt
            tolerations: []
            trustedCertsSecret: ""
            updatePrometheusJob:
              securityContext:
                enabled: false
                fsGroup: 1000
                runAsGroup: 1000
                runAsUser: 1000
            zones: 1
        fake-aws-ses:
          enabled: false
          global: {}
          image:
            repository: localstack/localstack
            tag: 0.8.7
          resources:
            limits:
              cpu: 200m
              memory: 500Mi
            requests:
              cpu: 100m
              memory: 100Mi
          service:
            externalPort: 4569
            internalPort: 4579
        fake-aws-sns:
          applications:
          - credential: testkey
            name: integration-test
            platform: GCM
          - credential: testprivatekey
            name: integration-test
            platform: APNS_SANDBOX
          - credential: testprivatekey
            name: integration-com.wire.ent
            platform: APNS_SANDBOX
          enabled: true
          global: {}
          image:
            repository: localstack/localstack
            tag: 0.8.7
          resources:
            limits:
              cpu: 200m
              memory: 500Mi
            requests:
              cpu: 100m
              memory: 100Mi
          service:
            externalPort: 4575
            internalPort: 4575
        fake-aws-sqs:
          enabled: true
          global: {}
          image:
            repository: airdock/fake-sqs
            tag: 0.3.1
          queueNames:
          - integration-team-events.fifo
          - integration-brig-events
          - integration-brig-events-internal
          - integration-gundeck-events
          resources:
            limits:
              cpu: 1000m
              memory: 1000Mi
            requests:
              cpu: 100m
              memory: 256Mi
          service:
            httpPort: 4568
        
        HOOKS:
        ---
        # Source: fake-aws/charts/fake-aws-s3/charts/minio/templates/post-install-create
        -bucket-job.yaml
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: fake-aws-s3-make-bucket-job
          labels:
            app: minio-make-bucket-job
            chart: minio-8.0.3
            release: fake-aws
            heritage: Helm
          annotations:
            "helm.sh/hook": post-install,post-upgrade
            "helm.sh/hook-delete-policy": hook-succeeded,before-hook-creation
        spec:
          template:
            metadata:
              labels:
                app: minio-job
                release: fake-aws
            spec:
              restartPolicy: OnFailure
              volumes:
                - name: minio-configuration
                  projected:
                    sources:
                    - configMap:
                        name: fake-aws-s3
                    - secret:
                        name: fake-aws-s3
              serviceAccountName: "fake-aws-s3"
              containers:
              - name: minio-mc
                image: "minio/mc:RELEASE.2020-10-03T02-54-56Z"
                imagePullPolicy: IfNotPresent
                command: ["/bin/sh", "/config/initialize"]
                env:
                  - name: MINIO_ENDPOINT
                    value: fake-aws-s3
                  - name: MINIO_PORT
                    value: "9000"
                volumeMounts:
                  - name: minio-configuration
                    mountPath: /config
                resources:
                  requests:
                    memory: 128Mi
        MANIFEST:
        ---
        # Source: fake-aws/charts/fake-aws-s3/charts/minio/templates/post-install-promet
        heus-metrics-serviceaccount.yaml
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: fake-aws-s3-update-prometheus-secret
          labels:
            app: minio-update-prometheus-secret
            chart: minio-8.0.3
            release: fake-aws
            heritage: Helm
        ---
        # Source: fake-aws/charts/fake-aws-s3/charts/minio/templates/serviceaccount.yaml
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: "fake-aws-s3"
          namespace: "default"
          labels:
            app: minio
            chart: minio-8.0.3
            release: "fake-aws"
        ---
        # Source: fake-aws/charts/fake-aws-s3/charts/minio/templates/secrets.yaml
        apiVersion: v1
        kind: Secret
        metadata:
          name: fake-aws-s3
          labels:
            app: minio
            chart: minio-8.0.3
            release: fake-aws
            heritage: Helm
        type: Opaque
        data:
          accesskey: "ZHVtbXlrZXk="
          secretkey: "ZHVtbXlzZWNyZXQ="
        ---
        # Source: fake-aws/charts/fake-aws-s3/charts/minio/templates/configmap.yaml
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: fake-aws-s3
          labels:
            app: minio
            chart: minio-8.0.3
            release: fake-aws
            heritage: Helm
        data:
          initialize: |-
            #!/bin/sh
            set -e ; # Have script exit in the event of a failed command.
            MC_CONFIG_DIR="/etc/minio/mc/"
            MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"
        
            # connectToMinio
            # Use a check-sleep-check loop to wait for Minio service to be available
            connectToMinio() {
              SCHEME=$1
              ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
              set -e ; # fail if we can't read the keys.
              ACCESS=$(cat /config/accesskey) ; SECRET=$(cat /config/secretkey) ;
              set +e ; # The connections to minio are allowed to fail.
              echo "Connecting to Minio server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
              MC_COMMAND="${MC} config host add myminio $SCHEME://$MINIO_ENDPOINT:$MINIO
        _PORT $ACCESS $SECRET" ;
              $MC_COMMAND ;
              STATUS=$? ;
              until [ $STATUS = 0 ]
              do
                ATTEMPTS=`expr $ATTEMPTS + 1` ;
                echo \"Failed attempts: $ATTEMPTS\" ;
                if [ $ATTEMPTS -gt $LIMIT ]; then
                  exit 1 ;
                fi ;
                sleep 2 ; # 1 second intervals between attempts
                $MC_COMMAND ;
                STATUS=$? ;
              done ;
              set -e ; # reset `e` as active
              return 0
            }
        
            # checkBucketExists ($bucket)
            # Check if the bucket exists, by using the exit code of `mc ls`
            checkBucketExists() {
              BUCKET=$1
              CMD=$(${MC} ls myminio/$BUCKET > /dev/null 2>&1)
              return $?
            }
        
            # createBucket ($bucket, $policy, $purge)
            # Ensure bucket exists, purging if asked to
            createBucket() {
              BUCKET=$1
              POLICY=$2
              PURGE=$3
              VERSIONING=$4
        
              # Purge the bucket, if set & exists
              # Since PURGE is user input, check explicitly for `true`
              if [ $PURGE = true ]; then
                if checkBucketExists $BUCKET ; then
                  echo "Purging bucket '$BUCKET'."
                  set +e ; # don't exit if this fails
                  ${MC} rm -r --force myminio/$BUCKET
                  set -e ; # reset `e` as active
                else
                  echo "Bucket '$BUCKET' does not exist, skipping purge."
                fi
              fi
        
              # Create the bucket if it does not exist
              if ! checkBucketExists $BUCKET ; then
                echo "Creating bucket '$BUCKET'"
                ${MC} mb myminio/$BUCKET
              else
                echo "Bucket '$BUCKET' already exists."
              fi
        
        
              # set versioning for bucket
              if [ ! -z $VERSIONING ] ; then
                if [ $VERSIONING = true ] ; then
                    echo "Enabling versioning for '$BUCKET'"
                    ${MC} version enable myminio/$BUCKET
                elif [ $VERSIONING = false ] ; then
                    echo "Suspending versioning for '$BUCKET'"
                    ${MC} version suspend myminio/$BUCKET
                fi
              else
                  echo "Bucket '$BUCKET' versioning unchanged."
              fi
        
              # At this point, the bucket should exist, skip checking for existence
              # Set policy on the bucket
              echo "Setting policy of bucket '$BUCKET' to '$POLICY'."
              ${MC} policy set $POLICY myminio/$BUCKET
            }
        
            # Try connecting to Minio instance
            scheme=http
            connectToMinio $scheme
            # Create the buckets
            createBucket dummy-bucket none true
            createBucket assets none false
            createBucket public public false
        ---
        # Source: fake-aws/charts/fake-aws-s3/charts/minio/templates/post-install-promet
        heus-metrics-role.yaml
        apiVersion: rbac.authorization.k8s.io/v1
        kind: Role
        metadata:
          name: fake-aws-s3-update-prometheus-secret
          labels:
            app: minio-update-prometheus-secret
            chart: minio-8.0.3
            release: fake-aws
            heritage: Helm
        rules:
          - apiGroups:
              - ""
            resources:
              - secrets
            verbs:
              - get
              - create
              - update
              - patch
            resourceNames:
              - fake-aws-s3-prometheus
          - apiGroups:
              - ""
            resources:
              - secrets
            verbs:
              - create
          - apiGroups:
              - monitoring.coreos.com
            resources:
              - servicemonitors
            verbs:
              - get
            resourceNames:
              - fake-aws-s3
        ---
        # Source: fake-aws/charts/fake-aws-s3/charts/minio/templates/post-install-promet
        heus-metrics-rolebinding.yaml
        apiVersion: rbac.authorization.k8s.io/v1
        kind: RoleBinding
        metadata:
          name: fake-aws-s3-update-prometheus-secret
          labels:
            app: minio-update-prometheus-secret
            chart: minio-8.0.3
            release: fake-aws
            heritage: Helm
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: Role
          name: fake-aws-s3-update-prometheus-secret
        subjects:
          - kind: ServiceAccount
            name: fake-aws-s3-update-prometheus-secret
            namespace: "default"
        ---
        # Source: fake-aws/charts/fake-aws-s3/charts/minio/templates/service.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: fake-aws-s3
          labels:
            app: minio
            chart: minio-8.0.3
            release: fake-aws
            heritage: Helm
        spec:
          type: ClusterIP
          ports:
            - name: http
              port: 9000
              protocol: TCP
              targetPort: 9000
          selector:
            app: minio
            release: fake-aws
        ---
        # Source: fake-aws/charts/fake-aws-sns/templates/service.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: fake-aws-sns
          labels:
            app: fake-aws-sns
            chart: "fake-aws-sns-2.110.0"
            release: "fake-aws"
            heritage: "Helm"
        spec:
          type: ClusterIP
          selector:
            app: fake-aws-sns
          ports:
            - name: http
              port: 4575
              targetPort: 4575
              protocol: TCP
        ---
        # Source: fake-aws/charts/fake-aws-sqs/templates/service.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: fake-aws-sqs
          labels:
            app: fake-aws-sqs
            chart: "fake-aws-sqs-2.110.0"
            release: "fake-aws"
            heritage: "Helm"
        spec:
          type: ClusterIP
          selector:
            app: fake-aws-sqs
          ports:
            - name: http
              port: 4568
              targetPort: 4568
              protocol: TCP
        ---
        # Source: fake-aws/charts/fake-aws-s3/charts/minio/templates/deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: fake-aws-s3
          labels:
            app: minio
            chart: minio-8.0.3
            release: fake-aws
            heritage: Helm
        spec:
          strategy:
            type: RollingUpdate
            rollingUpdate:
              maxSurge: 100%
              maxUnavailable: 0
          selector:
            matchLabels:
              app: minio
              release: fake-aws
          template:
            metadata:
              name: fake-aws-s3
              labels:
                app: minio
                release: fake-aws
              annotations:
                checksum/secrets: e309d074ad8b38e7ca5627c8f7d956390f5af83e0d5a16c45aa561
        1fb2519f52
                checksum/config: cc64be52d6c6c9e047232e55d24877c2227a6b5a0c1f5fded18ef03
        75936a50b
            spec:
              serviceAccountName: "fake-aws-s3"
              containers:
                - name: minio
                  image: "minio/minio:RELEASE.2020-10-18T21-54-12Z"
                  imagePullPolicy: IfNotPresent
                  command:
                    - "/bin/sh"
                    - "-ce"
                    - "/usr/bin/docker-entrypoint.sh minio -S /etc/minio/certs/ server /
        export"
                  volumeMounts:
                  ports:
                    - name: http
                      containerPort: 9000
                  env:
                    - name: MINIO_ACCESS_KEY
                      valueFrom:
                        secretKeyRef:
                          name: fake-aws-s3
                          key: accesskey
                    - name: MINIO_SECRET_KEY
                      valueFrom:
                        secretKeyRef:
                          name: fake-aws-s3
                          key: secretkey
                    - name: MINIO_BROWSER
                      value: "off"
                  resources:
                    requests:
                      memory: 4Gi
              volumes:
                - name: export
                  emptyDir: {}
                - name: minio-user
                  secret:
                    secretName: fake-aws-s3
        ---
        # Source: fake-aws/charts/fake-aws-s3/templates/reaper.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: fake-aws-s3-reaper
          labels:
            app: fake-aws-s3-reaper
            chart: "fake-aws-s3-2.110.0"
            release: "fake-aws"
            heritage: "Helm"
        spec:
          replicas: 1
          selector:
            matchLabels:
              app: fake-aws-s3-reaper
          template:
            metadata:
              labels:
                app: fake-aws-s3-reaper
            spec:
              containers:
              - name: initiate-fake-aws-s3
                image: mesosphere/aws-cli:1.14.5
                command: [/bin/sh]
                args:
                - -c
                - |
                  echo 'Creating AWS resources'
                  while true
                  do
                      AWS_SECRET_ACCESS_KEY=dummysecret AWS_ACCESS_KEY_ID=dummykey aws s
        3 --endpoint http://fake-aws-s3:9000 mb s3://bucket | grep -ev "BucketAlreadyOwn
        edByYou"
                      sleep 10
                  done
        ---
        # Source: fake-aws/charts/fake-aws-sns/templates/deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: fake-aws-sns
          labels:
            app: fake-aws-sns
            chart: "fake-aws-sns-2.110.0"
            release: "fake-aws"
            heritage: "Helm"
        spec:
          replicas: 1
          selector:
            matchLabels:
              app: fake-aws-sns
          template:
            metadata:
              labels:
                app: fake-aws-sns
            spec:
              containers:
              - name: fake-aws-sns
                image: "localstack/localstack:0.8.7"
                env:
                  - name: DEBUG
                    value: "1"
                  - name: DEFAULT_REGION
                    value: "eu-west-1"
                  - name: SERVICES
                    value: "sns"
                ports:
                - containerPort: 4575
                  name: http
                  protocol: TCP
                volumeMounts:
                - name: storage
                  mountPath: /data
                resources:
                    limits:
                      cpu: 200m
                      memory: 500Mi
                    requests:
                      cpu: 100m
                      memory: 100Mi
              - name: initiate-fake-aws-sns
                image: mesosphere/aws-cli:1.14.5
                command: [/bin/sh]
                args:
                - -c
                - |
                  exec_until_ready() {
                      until $1; do echo 'service not ready yet'; sleep 1; done
                  }
                  application_exists() {
                      OUTPUT=$(aws --endpoint-url=http://localhost:4575 sns list-platfor
        m-applications | grep $1 | wc -l)
                      echo $OUTPUT
                  }
                  echo 'Creating AWS resources'
                  aws configure set aws_access_key_id dummy
                  aws configure set aws_secret_access_key dummy
                  aws configure set region eu-west-1
        
                  while true
                  do
        
                          APPLICATION=$(application_exists "GCM/integration-test")
                          if [ "$APPLICATION" == "1" ]
                            then echo "Application integration-test exists, no need to r
        e-create"
                            else exec_until_ready "aws --endpoint-url=http://localhost:4
        575 sns create-platform-application --name integration-test --platform GCM --att
        ributes PlatformCredential=testkey"
                          fi
        
                          APPLICATION=$(application_exists "APNS_SANDBOX/integration-tes
        t")
                          if [ "$APPLICATION" == "1" ]
                            then echo "Application integration-test exists, no need to r
        e-create"
                            else exec_until_ready "aws --endpoint-url=http://localhost:4
        575 sns create-platform-application --name integration-test --platform APNS_SAND
        BOX --attributes PlatformCredential=testprivatekey"
                          fi
        
                          APPLICATION=$(application_exists "APNS_SANDBOX/integration-com
        .wire.ent")
                          if [ "$APPLICATION" == "1" ]
                            then echo "Application integration-com.wire.ent exists, no n
        eed to re-create"
                            else exec_until_ready "aws --endpoint-url=http://localhost:4
        575 sns create-platform-application --name integration-com.wire.ent --platform A
        PNS_SANDBOX --attributes PlatformCredential=testprivatekey"
                          fi
        
                      echo "Resources created, sleeping for 10, to keep this container (
        and thus the pod) alive"
                      sleep 10
                  done
              volumes:
                - emptyDir: {}
                  name: "storage"
        ---
        # Source: fake-aws/charts/fake-aws-sqs/templates/deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: fake-aws-sqs
          labels:
            app: fake-aws-sqs
            chart: "fake-aws-sqs-2.110.0"
            release: "fake-aws"
            heritage: "Helm"
        spec:
          replicas: 1
          selector:
            matchLabels:
              app: fake-aws-sqs
          template:
            metadata:
              labels:
                app: fake-aws-sqs
            spec:
              containers:
              - name: fake-aws-sqs
                image: "airdock/fake-sqs:0.3.1"
                ports:
                - containerPort: 4568
                  name: http
                  protocol: TCP
                volumeMounts:
                - name: storage
                  mountPath: /data
                resources:
                    limits:
                      cpu: 1000m
                      memory: 1000Mi
                    requests:
                      cpu: 100m
                      memory: 256Mi
              - name: initiate-fake-aws-sqs
                image: mesosphere/aws-cli:1.14.5
                command: [/bin/sh]
                args:
                - -c
                - |
                  exec_until_ready() {
                      until $1; do echo 'service not ready yet'; sleep 1; done
                  }
                  queue_exists() {
                      # NOTE: we use the '"' to match the queue name more exactly (other
        wise there is some overlap)
                      OUTPUT=$(aws --endpoint-url=http://localhost:4568 sqs list-queues
        | grep $1'"' | wc -l)
                      echo $OUTPUT
                  }
        
                  echo 'Creating AWS resources'
                  aws configure set aws_access_key_id dummy
                  aws configure set aws_secret_access_key dummy
                  aws configure set region eu-west-1
        
                  while true
                  do
                      # Recreate resources if needed
        
                          QUEUE=$(queue_exists "integration-team-events.fifo")
                          if [ "$QUEUE" == "1" ]
                            then echo "Queue integration-team-events.fifo exists, no nee
        d to re-create"
                            else exec_until_ready "aws --endpoint-url=http://localhost:4
        568 sqs create-queue --queue-name integration-team-events.fifo"
                          fi
        
                          QUEUE=$(queue_exists "integration-brig-events")
                          if [ "$QUEUE" == "1" ]
                            then echo "Queue integration-brig-events exists, no need to
        re-create"
                            else exec_until_ready "aws --endpoint-url=http://localhost:4
        568 sqs create-queue --queue-name integration-brig-events"
                          fi
        
                          QUEUE=$(queue_exists "integration-brig-events-internal")
                          if [ "$QUEUE" == "1" ]
                            then echo "Queue integration-brig-events-internal exists, no
         need to re-create"
                            else exec_until_ready "aws --endpoint-url=http://localhost:4
        568 sqs create-queue --queue-name integration-brig-events-internal"
                          fi
        
                          QUEUE=$(queue_exists "integration-gundeck-events")
                          if [ "$QUEUE" == "1" ]
                            then echo "Queue integration-gundeck-events exists, no need
        to re-create"
                            else exec_until_ready "aws --endpoint-url=http://localhost:4
        568 sqs create-queue --queue-name integration-gundeck-events"
                          fi
        
        
                      echo 'Sleeping 10'
                      sleep 10
                  done
              volumes:
                - emptyDir: {}
                  name: "storage"
        
        NOTES:
        You can reach the fake AWS services at:
        SNS      : http://fake-aws-sns:4575
        SQS      : http://fake-aws-sqs:4568
          queues:
            - integration-team-events.fifo
            - integration-brig-events
            - integration-brig-events-internal
            - integration-gundeck-events
        S3       : http://fake-aws-s3:9000
          bucket: bucket
[20:47:48] # Running command «kubectl get pods -A» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ kubectl get pods -A
        NAMESPACE     NAME                                       READY   STATUS      RES
        TARTS   AGE
        default       brig-c8dd6c7ff-l9dnn                       1/1     Running     0
                8h
        default       brig-index-migrate-data-jmdlr              0/1     Completed   0
                8h
        default       cannon-0                                   1/1     Running     0
                8h
        default       cargohold-d75ffb69b-8qf6f                  1/1     Running     0
                8h
        default       cassandra-ephemeral-0                      1/1     Running     0
                21h
        default       cassandra-migrations-xctmr                 0/1     Completed   0
                8h
        default       demo-smtp-85557f6877-nb99z                 1/1     Running     0
                21h
        default       elasticsearch-ephemeral-86f4b8ff6f-jctn6   1/1     Running     0
                21h
        default       elasticsearch-index-create-4k5tl           0/1     Completed   0
                8h
        default       fake-aws-s3-77d9447b8f-r48k7               1/1     Running     0
                21h
        default       fake-aws-s3-reaper-78d9f58dd4-zq98j        1/1     Running     0
                21h
        default       fake-aws-sns-6c7c4b7479-xxdcp              2/2     Running     0
                21h
        default       fake-aws-sqs-59fbfbcbd4-8vfmd              2/2     Running     0
                21h
        default       galley-6db66679-gxkbk                      1/1     Running     0
                8h
        default       galley-migrate-data-cxg2j                  0/1     Completed   0
                8h
        default       gundeck-6f9bfbd8d8-9gjmf                   1/1     Running     0
                8h
        default       nginz-c88484d9c-gdhk8                      2/2     Running     0
                8h
        default       redis-ephemeral-master-0                   1/1     Running     0
                21h
        default       spar-5889cfcfc-6gbsc                       1/1     Running     0
                8h
        default       spar-5889cfcfc-97hr9                       1/1     Running     0
                8h
        default       spar-5889cfcfc-fg7n8                       1/1     Running     0
                8h
        default       spar-migrate-data-dq7wj                    0/1     Completed   0
                8h
        default       webapp-7678f9457-7rwk8                     1/1     Running     0
                8h
        kube-system   coredns-7677f9bb54-rx7g5                   0/1     Pending     0
                21h
        kube-system   coredns-7677f9bb54-s4cmj                   1/1     Running     0
                21h
        kube-system   dns-autoscaler-5b7b5c9b6f-6hds6            1/1     Running     0
                21h
        kube-system   kube-apiserver-kubenode01                  1/1     Running     0
                21h
        kube-system   kube-controller-manager-kubenode01         1/1     Running     0
                21h
        kube-system   kube-flannel-xktqb                         1/1     Running     0
                21h
        kube-system   kube-proxy-bdh7c                           1/1     Running     0
                3m32s
        kube-system   kube-scheduler-kubenode01                  1/1     Running     0
                21h
        kube-system   nodelocaldns-7v7dw                         1/1     Running     0
                21h
[20:48:11] # Running command «helm upgrade --install smtp wire/demo-smtp --wait --debug» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ helm upgrade --install smtp wire/
        demo-smtp --wait --debug
        history.go:56: [debug] getting history for release smtp
        upgrade.go:123: [debug] preparing upgrade for smtp
        upgrade.go:131: [debug] performing update for smtp
        upgrade.go:303: [debug] creating upgraded release for smtp
        client.go:201: [debug] checking 2 resources for changes
        wait.go:53: [debug] beginning wait for 2 resources with timeout of 5m0s
        upgrade.go:138: [debug] updating status for upgraded release for smtp
        Release "smtp" has been upgraded. Happy Helming!
        NAME: smtp
        LAST DEPLOYED: Thu Jul 15 18:48:03 2021
        NAMESPACE: default
        STATUS: deployed
        REVISION: 4
        TEST SUITE: None
        USER-SUPPLIED VALUES:
        {}
        
        COMPUTED VALUES:
        envVars: {}
        fullnameOverride: demo-smtp
        image: quay.io/wire/namshi-smtp:aa63b8
        replicaCount: 1
        resources:
          limits:
            cpu: 500m
            memory: 500Mi
          requests:
            cpu: 100m
            memory: 128Mi
        service:
          port: 25
        
        HOOKS:
        MANIFEST:
        ---
        # Source: demo-smtp/templates/service.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: demo-smtp
          labels:
            app: demo-smtp
            chart: demo-smtp-2.110.0
            release: smtp
            heritage: Helm
        spec:
          type:
          ports:
            - port: 25
              targetPort: smtp
              protocol: TCP
              name: smtp
          selector:
            app: demo-smtp
            release: smtp
        ---
        # Source: demo-smtp/templates/deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: demo-smtp
          labels:
            app: demo-smtp
            chart: demo-smtp-2.110.0
            release: smtp
            heritage: Helm
        spec:
          replicas: 1
          selector:
            matchLabels:
              app: demo-smtp
              release: smtp
          template:
            metadata:
              labels:
                app: demo-smtp
                release: smtp
            spec:
              containers:
                - name: demo-smtp
                  image: "quay.io/wire/namshi-smtp:aa63b8"
                  env:
                  ports:
                    - name: smtp
                      containerPort: 25
                      protocol: TCP
                  resources:
                    limits:
                      cpu: 500m
                      memory: 500Mi
                    requests:
                      cpu: 100m
                      memory: 128Mi
        
[20:48:16] # Running command «kubectl get pods -A» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ kubectl get pods -A
        NAMESPACE     NAME                                       READY   STATUS      RES
        TARTS   AGE
        default       brig-c8dd6c7ff-l9dnn                       1/1     Running     0
                8h
        default       brig-index-migrate-data-jmdlr              0/1     Completed   0
                8h
        default       cannon-0                                   1/1     Running     0
                8h
        default       cargohold-d75ffb69b-8qf6f                  1/1     Running     0
                8h
        default       cassandra-ephemeral-0                      1/1     Running     0
                21h
        default       cassandra-migrations-xctmr                 0/1     Completed   0
                8h
        default       demo-smtp-85557f6877-nb99z                 1/1     Running     0
                21h
        default       elasticsearch-ephemeral-86f4b8ff6f-jctn6   1/1     Running     0
                21h
        default       elasticsearch-index-create-4k5tl           0/1     Completed   0
                8h
        default       fake-aws-s3-77d9447b8f-r48k7               1/1     Running     0
                21h
        default       fake-aws-s3-reaper-78d9f58dd4-zq98j        1/1     Running     0
                21h
        default       fake-aws-sns-6c7c4b7479-xxdcp              2/2     Running     0
                21h
        default       fake-aws-sqs-59fbfbcbd4-8vfmd              2/2     Running     0
                21h
        default       galley-6db66679-gxkbk                      1/1     Running     0
                8h
        default       galley-migrate-data-cxg2j                  0/1     Completed   0
                8h
        default       gundeck-6f9bfbd8d8-9gjmf                   1/1     Running     0
                8h
        default       nginz-c88484d9c-gdhk8                      2/2     Running     0
                8h
        default       redis-ephemeral-master-0                   1/1     Running     0
                21h
        default       spar-5889cfcfc-6gbsc                       1/1     Running     0
                8h
        default       spar-5889cfcfc-97hr9                       1/1     Running     0
                8h
        default       spar-5889cfcfc-fg7n8                       1/1     Running     0
                8h
        default       spar-migrate-data-dq7wj                    0/1     Completed   0
                8h
        default       webapp-7678f9457-7rwk8                     1/1     Running     0
                8h
        kube-system   coredns-7677f9bb54-rx7g5                   0/1     Pending     0
                21h
        kube-system   coredns-7677f9bb54-s4cmj                   1/1     Running     0
                21h
        kube-system   dns-autoscaler-5b7b5c9b6f-6hds6            1/1     Running     0
                21h
        kube-system   kube-apiserver-kubenode01                  1/1     Running     0
                21h
        kube-system   kube-controller-manager-kubenode01         1/1     Running     0
                21h
        kube-system   kube-flannel-xktqb                         1/1     Running     0
                21h
        kube-system   kube-proxy-bdh7c                           1/1     Running     0
                4m
        kube-system   kube-scheduler-kubenode01                  1/1     Running     0
                21h
        kube-system   nodelocaldns-7v7dw                         1/1     Running     0
                21h
[20:48:22] # Running command «free -m && uptime» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ free -m && uptime
                      total        used        free      shared  buff/cache   available
        Mem:           2928         197         889           0        1841        2556
        Swap:          3943           1        3942
         18:48:19 up 35 min,  0 users,  load average: 1.09, 0.99, 1.22
[20:48:27] # Running command «free -m && uptime» on 95.216.208.159
        wire@arthur-demo:~$ free -m && uptime
                      total        used        free      shared  buff/cache   available
        Mem:          15661        5599         649           4        9412       10151
        Swap:             0           0           0
         20:48:25 up 3 days, 10:25,  0 users,  load average: 2.13, 1.78, 1.42
[20:48:33] # Running command «ls -l» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ ls -l
        total 120
        -rw-rw-r--  1 wire wire 2752 Jul 15 18:19 Makefile
        -rw-rw-r--  1 wire wire 8198 Jul 15 18:19 README.md
        -rw-rw-r--  1 wire wire  477 Jul 15 18:19 admin_users.yml
        -rw-rw-r--  1 wire wire  382 Jul 15 18:19 ansible.cfg
        -rw-rw-r--  1 wire wire   75 Jul 15 18:19 bootstrap.yml
        -rw-rw-r--  1 wire wire  511 Jul 15 18:19 cassandra-verify-ntp.yml
        -rw-rw-r--  1 wire wire  918 Jul 15 18:19 cassandra.yml
        -rw-rw-r--  1 wire wire 3068 Jul 15 18:19 elasticsearch.yml
        drwxrwxr-x  3 wire wire 4096 Jul 15 18:19 files
        -rw-rw-r--  1 wire wire 1086 Jul 15 18:19 get-logs.yml
        -rw-rw-r--  1 wire wire 1266 Jul 15 18:19 helm_external.yml
        drwxrwxr-x  3 wire wire 4096 Jul 15 18:19 host_vars
        drwxrwxr-x  5 wire wire 4096 Jul 15 18:45 inventory
        -rw-rw-r--  1 wire wire  689 Jul 15 18:19 iptables.yml
        -rw-rw-r--  1 wire wire 1762 Jul 15 18:19 kube-minio-static-files.yml
        -rw-rw-r--  1 wire wire 1332 Jul 15 18:19 kubernetes.yml
        -rw-rw-r--  1 wire wire  821 Jul 15 18:19 kubernetes_logging.yml
        -rw-rw-r--  1 wire wire 2767 Jul 15 18:19 minio.yml
        -rw-rw-r--  1 wire wire 1552 Jul 15 18:19 provision-sft.yml
        -rw-rw-r--  1 wire wire 3241 Jul 15 18:19 registry.yml
        -rw-rw-r--  1 wire wire  821 Jul 15 18:19 restund.yml
        drwxrwxr-x  4 wire wire 4096 Jul 15 18:19 roles
        drwxrwxr-x 18 wire wire 4096 Jul 15 18:19 roles-external
        -rw-rw-r--  1 wire wire  947 Jul 15 18:19 seed-offline-docker.yml
        -rw-rw-r--  1 wire wire 1928 Jul 15 18:19 setup-offline-sources.yml
        drwxrwxr-x  2 wire wire 4096 Jul 15 18:19 tasks
        drwxrwxr-x  2 wire wire 4096 Jul 15 18:19 templates
        -rw-rw-r--  1 wire wire 1098 Jul 15 18:19 tinc.yml
[20:48:39] # Running command «pwd» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ pwd
        /home/wire/wire-server-deploy/ansible
[20:48:45] # Running command «cd ~/wire-server-deploy» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/ansible$ cd ~/wire-server-deploy
[20:48:51] # Running command «ls -l» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy$ ls -l
        total 112
        -rw-rw-r--  1 wire wire 16072 Jul 15 18:19 CHANGELOG.md
        -rw-rw-r--  1 wire wire  1531 Jul 15 18:19 CONTRIBUTING.md
        -rw-rw-r--  1 wire wire   252 Jul 15 18:19 Dockerfile
        -rw-rw-r--  1 wire wire 34520 Jul 15 18:19 LICENSE
        -rw-rw-r--  1 wire wire  5893 Jul 15 18:19 Makefile
        -rw-rw-r--  1 wire wire  2251 Jul 15 18:19 README.md
        drwxrwxr-x  9 wire wire  4096 Jul 15 18:19 ansible
        drwxrwxr-x  3 wire wire  4096 Jul 15 18:19 bin
        -rw-rw-r--  1 wire wire  2338 Jul 15 18:19 default.nix
        drwxrwxr-x  5 wire wire  4096 Jul 15 18:19 examples
        drwxrwxr-x  2 wire wire  4096 Jul 15 18:19 helm
        drwxrwxr-x  4 wire wire  4096 Jul 15 18:19 nix
        drwxrwxr-x  2 wire wire  4096 Jul 15 18:19 offline
        drwxrwxr-x  5 wire wire  4096 Jul 15 18:19 terraform
        drwxrwxr-x 15 wire wire  4096 Jul 15 18:19 values
        drwxrwxr-x  2 wire wire  4096 Jul 15 18:24 wire-server
[20:48:57] # Running command «pwd» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy$ pwd
        /home/wire/wire-server-deploy
[20:49:03] # Running command «cd wire-server/» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy$ cd wire-server/
[20:49:08] # Running command «ls -l» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ ls -l
        total 44
        -rw-rw-r-- 1 wire wire 12606 Jul 15 18:22 prod-values.yaml
        -rw-rw-r-- 1 wire wire    42 Jul 15 18:21 restund.txt
        -rw-rw-r-- 1 wire wire  2131 Jul 15 18:23 secrets.yaml
        -rw-rw-r-- 1 wire wire   744 Jul 15 18:23 spar.yaml
        -rw-rw-r-- 1 wire wire  8385 Jul 15 18:24 values.yaml
        -rw-rw-r-- 1 wire wire   150 Jul 15 18:22 zauth.txt
[20:49:14] # Running command «pwd» on 192.168.1.121
        wire@wire-client:~/wire-server-deploy/wire-server$ pwd
        /home/wire/wire-server-deploy/wire-server
[20:50:05] # Running command «helm upgrade --install wire-server wire/wire-server -f values.yaml -f secrets.yaml --wait --debug» on 192.168.1.121
        proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location ~* ^/teams/api-docs {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
                    zauth off;
        
                    # If zauth is off, limit by remote address if not part of limit exem
        ptionslimit_req zone=reqs_per_addr burst=5 nodelay;
                    limit_conn conns_per_addr 20;
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://galley;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location ~* ^/teams/([^/]*)/features {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://galley;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
                    proxy_set_header   Authorization  "";
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location ~* ^/teams/([^/]*)/features/([^/])* {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://galley;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
                    proxy_set_header   Authorization  "";
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /galley-api/swagger-ui {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
                    zauth off;
        
                    # If zauth is off, limit by remote address if not part of limit exem
        ptionslimit_req zone=reqs_per_addr burst=5 nodelay;
                    limit_conn conns_per_addr 20;
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://galley;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /push {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://gundeck;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
                    proxy_set_header   Authorization  "";
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /presences {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://gundeck;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
                    proxy_set_header   Authorization  "";
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /notifications {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://gundeck;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
                    proxy_set_header   Authorization  "";
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /billing {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
                    zauth off;
        
                    # If zauth is off, limit by remote address if not part of limit exem
        ptionslimit_req zone=reqs_per_addr burst=5 nodelay;
                    limit_conn conns_per_addr 20;
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://ibis;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location ~* ^/teams/([^/]*)/billing(.*) {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://ibis;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
                    proxy_set_header   Authorization  "";
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /proxy {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://proxy;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
                    proxy_set_header   Authorization  "";
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /identity-providers {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://spar;
                    proxy_http_version 1.1;
                    client_max_body_size 256k;
        
        
                    proxy_set_header   Connection     "";
        
                    proxy_set_header   Authorization  "";
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /sso-initiate-bind {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://spar;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
                    proxy_set_header   Authorization  "";
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /sso/initiate-login {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
                    zauth off;
        
                    # If zauth is off, limit by remote address if not part of limit exem
        ptionslimit_req zone=reqs_per_addr burst=5 nodelay;
                    limit_conn conns_per_addr 20;
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://spar;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Credentials: true';
        
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /sso/finalize-login {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
                    zauth off;
        
                    # If zauth is off, limit by remote address if not part of limit exem
        ptionslimit_req zone=reqs_per_addr burst=5 nodelay;
                    limit_conn conns_per_addr 20;
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://spar;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Credentials: true';
        
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /sso {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
                    zauth off;
        
                    # If zauth is off, limit by remote address if not part of limit exem
        ptionslimit_req zone=reqs_per_addr burst=5 nodelay;
                    limit_conn conns_per_addr 20;
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://spar;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /scim/v2 {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
                    zauth off;
        
                    # If zauth is off, limit by remote address if not part of limit exem
        ptionslimit_req zone=reqs_per_addr burst=5 nodelay;
                    limit_conn conns_per_addr 20;
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://spar;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Credentials: true';
        
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
                location /scim {
        
                    # remove access_token from logs, see 'Note sanitized_request' above.
                    set $sanitized_request $request;
                    if ($sanitized_request ~ (.*)access_token=[^&\s]*(.*)) {
                        set $sanitized_request $1access_token=****$2;
                    }
        
                    if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Methods' "GET, POST, PUT, DELET
        E, OPTIONS";
                        add_header 'Access-Control-Allow-Headers' "$http_access_control_
        request_headers, DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Mod
        ified-Since,Cache-Control,Content-Type";
                        add_header 'Content-Type' 'text/plain; charset=UTF-8';
                        add_header 'Content-Length' 0;
                        return 204;
                    }
        
                    proxy_pass         http://spar;
                    proxy_http_version 1.1;
                    client_max_body_size 64k;
        
        
                    proxy_set_header   Connection     "";
        
                    proxy_set_header   Authorization  "";
        
                    proxy_set_header   Z-Type         $zauth_type;
                    proxy_set_header   Z-User         $zauth_user;
                    proxy_set_header   Z-Connection   $zauth_connection;
                    proxy_set_header   Z-Provider     $zauth_provider;
                    proxy_set_header   Z-Bot          $zauth_bot;
                    proxy_set_header   Z-Conversation $zauth_conversation;
                    proxy_set_header   Request-Id     $request_id;
                    more_set_headers 'Access-Control-Allow-Origin: $http_origin';
        
                    more_set_headers 'Access-Control-Expose-Headers: Request-Id, Locatio
        n';
                    more_set_headers 'Request-Id: $request_id';
                    more_set_headers 'Strict-Transport-Security: max-age=31536000; prelo
        ad';
                }
        
        
        
                # Swagger UI
        
                location /swagger-ui {
                    zauth  off;
                    gzip   off;
                    alias /opt/zwagger-ui;
                    types {
                        application/javascript  js;
                        text/css                css;
                        text/html               html;
                        image/png               png;
                    }
                }
              }
            }
          upstreams.txt: |2
            brig calling-test cannon cargohold galley gundeck ibis proxy spar
          zwagger-config.js: |2
            var environment = 'prod';
          zauth.acl: |
            a (blacklist (path "/provider")
                         (path "/provider/**")
                         (path "/bot")
                         (path "/bot/**")
                         (path "/i/**"))
        
            b (whitelist (path "/bot")
                         (path "/bot/**"))
        
            p (whitelist (path "/provider")
                         (path "/provider/**"))
        
            # LegalHold Access Tokens
            la (whitelist (path "/notifications")
                          (path "/assets/v3/**")
                          (path "/users")
                          (path "/users/**"))
        kind: ConfigMap
        metadata:
          creationTimestamp: null
          name: nginz
        ---
        # Source: wire-server/charts/spar/templates/configmap.yaml
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: "spar"
        data:
          spar.yaml: |
            logNetStrings: True # log using netstrings encoding (see http://cr.yp.to/pro
        to/netstrings.txt)
            logLevel: Debug
        
            brig:
              host: brig
              port: 8080
        
            galley:
              host: galley
              port: 8080
        
            cassandra:
              endpoint:
                host: cassandra-ephemeral
                port: 9042
              keyspace: spar
        
            maxttlAuthreq: 28800
            maxttlAuthresp: 28800
        
            richInfoLimit: 5000
        
            maxScimTokens: 0
        
            saml:
              version:     SAML2.0
              logLevel:    Debug
        
              spHost: 0.0.0.0
              spPort: 8080
              spAppUri: https://nginz-https.example.com
              spSsoUri: https://nginz-https.example.com/sso
        
              contacts:
                    - company: YourCompany
                      email: email:support@example.com
                      type: ContactSupport
        ---
        # Source: wire-server/charts/spar/templates/tests/configmap.yaml
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: "spar-integration"
        data:
          integration.yaml: |
            brig:
              host: brig
              port: 8080
        
            galley:
              host: galley
              port: 8080
        
            spar:
              host: spar
              port: 8080
        
            # Keep this in sync with brigs setTeamInvitationTimeout
            brigSettingsTeamInvitationTimeout: 10
        ---
        # Source: wire-server/charts/brig/templates/service.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: brig
          labels:
            wireService: brig
            chart: brig-2.110.0
            release: wire-server
            heritage: Helm
        spec:
          type: ClusterIP
          ports:
            - name: http
              port: 8080
              targetPort: 8080
          selector:
            wireService: brig
            release: wire-server
        ---
        # Source: wire-server/charts/brig/templates/tests/brig-integration.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: "brig-integration"
          labels:
            wireService: brig-integration
            chart: brig-2.110.0
            release: wire-server
            heritage: Helm
        spec:
          type: ClusterIP
          ports:
            - port: 9000
              targetPort: 9000
          selector:
            wireService: brig-integration
            release: wire-server
        ---
        # Source: wire-server/charts/brig/templates/tests/nginz-service.yaml
        # this service is needed for brig integration tests and allows brig to talk dire
        ctly to nginz over http
        # (this is not how you should normally configure nginz - use an ingress instead)
        apiVersion: v1
        kind: Service
        metadata:
          name: nginz-integration-http
        spec:
          type: ClusterIP
          ports:
            - port: 8080
              targetPort: 8080
          selector:
            wireService: nginz
        ---
        # Source: wire-server/charts/cannon/templates/headless-service.yaml
        # Note, this is a Headless service https://kubernetes.io/docs/concepts/services-
        networking/service/#headless-services
        # We use it this way so we can handle routing requests to specific cannons direc
        tly rather than distributing requests
        # between pods.
        #
        # Read more about this technique in the StatefulSet guide:
        # https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/
        apiVersion: v1
        kind: Service
        metadata:
          name: cannon
          labels:
            wireService: cannon
            chart: cannon-2.110.0
            release: wire-server
            heritage: Helm
        spec:
          type: ClusterIP
          # This is what makes it a Headless Service
          clusterIP: None
          ports:
            - name: http
              port: 8080
              targetPort: 8080
              protocol: TCP
          selector:
            wireService: cannon
            release: wire-server
        ---
        # Source: wire-server/charts/cargohold/templates/service.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: cargohold
          labels:
            wireService: cargohold
            chart: cargohold-2.110.0
            release: wire-server
            heritage: Helm
        spec:
          type: ClusterIP
          ports:
            - name: http
              port: 8080
              targetPort: 8080
          selector:
            wireService: cargohold
            release: wire-server
        ---
        # Source: wire-server/charts/galley/templates/service.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: galley
          labels:
            wireService: galley
            chart: galley-2.110.0
            release: wire-server
            heritage: Helm
        spec:
          type: ClusterIP
          ports:
            - name: http
              port: 8080
              targetPort: 8080
          selector:
            wireService: galley
            release: wire-server
        ---
        # Source: wire-server/charts/galley/templates/tests/galley-integration.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: "galley-integration"
          labels:
            wireService: galley-integration
            chart: galley-2.110.0
            release: wire-server
            heritage: Helm
        spec:
          type: ClusterIP
          ports:
            - port: 9000
              targetPort: 9000
          selector:
            wireService: galley-integration
            release: wire-server
        ---
        # Source: wire-server/charts/gundeck/templates/service.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: gundeck
          labels:
            wireService: gundeck
            chart: gundeck-2.110.0
            release: wire-server
            heritage: Helm
        spec:
          type: ClusterIP
          ports:
            - name: http
              port: 8080
              targetPort: 8080
          selector:
            wireService: gundeck
            release: wire-server
        ---
        # Source: wire-server/charts/spar/templates/service.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: spar
          labels:
            wireService: spar
            chart: spar-2.110.0
            release: wire-server
            heritage: Helm
        spec:
          type: ClusterIP
          ports:
            - name: http
              port: 8080
              targetPort: 8080
          selector:
            wireService: spar
            release: wire-server
        ---
        # Source: wire-server/charts/brig/templates/deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: brig
          labels:
            wireService: brig
            chart: brig-2.110.0
            release: wire-server
            heritage: Helm
        spec:
          replicas: 1
          strategy:
            type: RollingUpdate
            rollingUpdate:
              maxUnavailable: 0
              maxSurge: 1
          selector:
            matchLabels:
              wireService: brig
          template:
            metadata:
              labels:
                wireService: brig
                release: wire-server
              annotations:
                # An annotation of the configmap checksum ensures changes to the configm
        ap cause a redeployment upon `helm upgrade`
                checksum/configmap: d00ab8993f55be4858a352b8e8d395670a3931888dbaa3a56acb
        b41761869f33
                checksum/turnconfigmap: 7c5c574b2c88fcdc70c139860524754f039c9aad790b7f80
        8248abdb04db61c3
                checksum/secret: 2e2576f2618900ccc29ec6073d8d880b3a86fe08c3479d9740adf15
        d64ae5e4a
                fluentbit.io/parser: json
            spec:
              volumes:
                - name: "brig-config"
                  configMap:
                    name: "brig"
                - name: "turn-servers"
                  configMap:
                    name: "turn"
                - name: "brig-secrets"
                  secret:
                    secretName: "brig"
              containers:
                - name: brig
                  image: "quay.io/wire/brig:2.110.0"
                  imagePullPolicy: ""
                  volumeMounts:
                  - name: "brig-secrets"
                    mountPath: "/etc/wire/brig/secrets"
                  - name: "brig-config"
                    mountPath: "/etc/wire/brig/conf"
                  - name: "turn-servers"
                    mountPath: "/etc/wire/brig/turn"
                  env:
                  - name: LOG_LEVEL
                    value: Info
                  - name: AWS_ACCESS_KEY_ID
                    valueFrom:
                      secretKeyRef:
                        name: brig
                        key: awsKeyId
                  - name: AWS_SECRET_ACCESS_KEY
                    valueFrom:
                      secretKeyRef:
                        name: brig
                        key: awsSecretKey
                  # TODO: Is this the best way to do this?
                  - name: AWS_REGION
                    value: "eu-west-1"
                  ports:
                    - containerPort: 8080
                  livenessProbe:
                    httpGet:
                      scheme: HTTP
                      path: /i/status
                      port: 8080
                  readinessProbe:
                    httpGet:
                      scheme: HTTP
                      path: /i/status
                      port: 8080
                  resources:
                    limits:
                      cpu: 500m
                      memory: 512Mi
                    requests:
                      cpu: 100m
                      memory: 256Mi
        ---
        # Source: wire-server/charts/cargohold/templates/deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: cargohold
          labels:
            wireService: cargohold
            chart: cargohold-2.110.0
            release: wire-server
            heritage: Helm
        spec:
          replicas: 1
          strategy:
            type: RollingUpdate
            rollingUpdate:
              maxUnavailable: 0
              maxSurge: 1
          selector:
            matchLabels:
              wireService: cargohold
          template:
            metadata:
              labels:
                wireService: cargohold
                release: wire-server
              annotations:
                # An annotation of the configmap checksum ensures changes to the configm
        ap cause a redeployment upon `helm upgrade`
                checksum/configmap: 3e3db07fdaf882e9641b8fb91433cc67587e451e77a5c60b0815
        bb46f540c2d7
                checksum/secret: db5e54e5ab9403d3dd43fe0d908427c1a667e16062456fa9215be61
        96bd0597e
            spec:
              volumes:
                - name: "cargohold-config"
                  configMap:
                    name: "cargohold"
                - name: "cargohold-secrets"
                  secret:
                    secretName: "cargohold"
              containers:
                - name: cargohold
                  image: "quay.io/wire/cargohold:2.110.0"
                  imagePullPolicy: ""
                  volumeMounts:
                  - name: "cargohold-secrets"
                    mountPath: "/etc/wire/cargohold/secrets"
                  - name: "cargohold-config"
                    mountPath: "/etc/wire/cargohold/conf"
                  env:
                  - name: AWS_ACCESS_KEY_ID
                    valueFrom:
                      secretKeyRef:
                        name: cargohold
                        key: awsKeyId
                  - name: AWS_SECRET_ACCESS_KEY
                    valueFrom:
                      secretKeyRef:
                        name: cargohold
                        key: awsSecretKey
                  - name: AWS_REGION
                    value: "eu-west-1"
                  ports:
                    - containerPort: 8080
                  livenessProbe:
                    httpGet:
                      scheme: HTTP
                      path: /i/status
                      port: 8080
                  readinessProbe:
                    httpGet:
                      scheme: HTTP
                      path: /i/status
                      port: 8080
                  resources:
                    limits:
                      cpu: 500m
                      memory: 512Mi
                    requests:
                      cpu: 100m
                      memory: 256Mi
        ---
        # Source: wire-server/charts/galley/templates/deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: galley
          labels:
            wireService: galley
            chart: galley-2.110.0
            release: wire-server
            heritage: Helm
        spec:
          replicas: 1
          strategy:
            type: RollingUpdate
            rollingUpdate:
              maxUnavailable: 0
              maxSurge: 1
          selector:
            matchLabels:
              wireService: galley
          template:
            metadata:
              labels:
                wireService: galley
                release: wire-server
              annotations:
                # An annotation of the configmap checksum ensures changes to the configm
        ap cause a redeployment upon `helm upgrade`
                checksum/configmap: 8386d4d6a5cdbc27dfe81d4c869d80f7ae1315b005e74aaf498d
        77b4dd319767
                checksum/secret: 9c1ec8ad8c195e74caf9c6c4efed117e555df54f7635ac613ab50e7
        e4bc036ab
            spec:
              volumes:
                - name: "galley-config"
                  configMap:
                    name: "galley"
                - name: "galley-secrets"
                  secret:
                    secretName: "galley"
              containers:
                - name: galley
                  image: "quay.io/wire/galley:2.110.0"
                  imagePullPolicy: ""
                  volumeMounts:
                  - name: "galley-secrets"
                    mountPath: "/etc/wire/galley/secrets"
                  - name: "galley-config"
                    mountPath: "/etc/wire/galley/conf"
                  env:
                  - name: AWS_ACCESS_KEY_ID
                    valueFrom:
                      secretKeyRef:
                        name: galley
                        key: awsKeyId
                  - name: AWS_SECRET_ACCESS_KEY
                    valueFrom:
                      secretKeyRef:
                        name: galley
                        key: awsSecretKey
                  - name: AWS_REGION
                    value: "eu-west-1"
                  ports:
                    - containerPort: 8080
                  livenessProbe:
                    httpGet:
                      scheme: HTTP
                      path: /i/status
                      port: 8080
                  readinessProbe:
                    httpGet:
                      scheme: HTTP
                      path: /i/status
                      port: 8080
                  resources:
                    limits:
                      cpu: 500m
                      memory: 512Mi
                    requests:
                      cpu: 100m
                      memory: 256Mi
        ---
        # Source: wire-server/charts/gundeck/templates/deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: gundeck
          labels:
            wireService: gundeck
            chart: gundeck-2.110.0
            release: wire-server
            heritage: Helm
        spec:
          replicas: 1
          strategy:
            type: RollingUpdate
            rollingUpdate:
              maxUnavailable: 0
              maxSurge: 1
          selector:
            matchLabels:
              wireService: gundeck
          template:
            metadata:
              labels:
                wireService: gundeck
                release: wire-server
              annotations:
                # An annotation of the configmap checksum ensures changes to the configm
        ap cause a redeployment upon `helm upgrade`
                checksum/configmap: 47137e013b588bfff5d8eefb7ebe42908f270301567188993fe6
        666628da46e4
                checksum/secret: c9285bb95e9247536ec032b2548368ae5d6b1262c0cd57d1b992a84
        0efea8a5e
            spec:
              volumes:
                - name: "gundeck-config"
                  configMap:
                    name: "gundeck"
                - name: "gundeck-secrets"
                  secret:
                    secretName: "gundeck"
              containers:
                - name: gundeck
                  image: "quay.io/wire/gundeck:2.110.0"
                  imagePullPolicy: ""
                  volumeMounts:
                  - name: "gundeck-secrets"
                    mountPath: "/etc/wire/gundeck/secrets"
                  - name: "gundeck-config"
                    mountPath: "/etc/wire/gundeck/conf"
                  env:
                  - name: AWS_ACCESS_KEY_ID
                    valueFrom:
                      secretKeyRef:
                        name: gundeck
                        key: awsKeyId
                  - name: AWS_SECRET_ACCESS_KEY
                    valueFrom:
                      secretKeyRef:
                        name: gundeck
                        key: awsSecretKey
                  - name: AWS_REGION
                    value: "eu-west-1"
                  ports:
                    - containerPort: 8080
                  livenessProbe:
                    httpGet:
                      scheme: HTTP
                      path: /i/status
                      port: 8080
                  readinessProbe:
                    httpGet:
                      scheme: HTTP
                      path: /i/status
                      port: 8080
                  resources:
                    limits:
                      cpu: 500m
                      memory: 512Mi
                    requests:
                      cpu: 100m
                      memory: 256Mi
        ---
        # Source: wire-server/charts/nginz/templates/deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: nginz
          labels:
            wireService: nginz
            chart: nginz-2.110.0
            release: wire-server
            heritage: Helm
        spec:
          replicas: 1
          strategy:
            type: RollingUpdate
            rollingUpdate:
              maxUnavailable: 0
              maxSurge: 2
          selector:
            matchLabels:
              wireService: nginz
              app: nginz
          template:
            metadata:
              labels:
                wireService: nginz
                app: nginz
                release: wire-server
              annotations:
                # An annotation of the configmap checksum ensures changes to the configm
        ap cause a redeployment upon `helm upgrade`
                checksum/configmap: cb1f86d1992f55924ea89d360528a28179da445c9ba95d331d99
        629ac36def90
                checksum/secret: 1bf513c99b38636ea509c5489987f3baea4f0155c286728b37e465c
        fb75f1bb2
                fluentbit.io/parser-nginz: nginz
            spec:
              terminationGracePeriodSeconds: 30 # should be higher than the drainTimeout
         (sleep duration of preStop)
              containers:
              - name: nginz-disco
                image: "quay.io/wire/nginz_disco:2.110.0"
                volumeMounts:
                - name: config
                  mountPath: /etc/wire/nginz/conf
                  readOnly: true
                - name: upstreams
                  mountPath: /etc/wire/nginz/upstreams
                  readOnly: false
              - name: nginz
                image: "quay.io/wire/nginz:2.110.0"
                lifecycle:
                  preStop:
                    exec:
                      # kubernetes by default sends a SIGTERM to the container,
                      # which would cause nginz to exit, breaking existing websocket con
        nections.
                      # Instead we sleep for a day, then terminate gracefully.
                      # (SIGTERM is still sent, but afterwards)
                      command: ["sh", "-c", "sleep 10 && nginx -c /etc/wire/nginz/conf/n
        ginx.conf -s quit"]
                volumeMounts:
                - name: secrets
                  mountPath: /etc/wire/nginz/secrets
                  readOnly: true
                - name: config
                  mountPath: /etc/wire/nginz/conf
                  readOnly: true
                - name: upstreams
                  mountPath: /etc/wire/nginz/upstreams
                  readOnly: true
                ports:
                - name: http
                  containerPort: 8080
                - name: tcp
                  containerPort: 8081
                readinessProbe:
                  httpGet:
                    path: /status
                    port: 8080
                    scheme: HTTP
                livenessProbe:
                  initialDelaySeconds: 30
                  timeoutSeconds: 1
                  httpGet:
                    path: /status
                    port: 8080
                    scheme: HTTP
                resources:
                    limits:
                      cpu: "2"
                      memory: 1024Mi
                    requests:
                      cpu: 100m
                      memory: 256Mi
              dnsPolicy: ClusterFirst
              restartPolicy: Always
              volumes:
              - name: config
                configMap:
                  name: nginz
              - name: secrets
                secret:
                  secretName: nginz
              - name: upstreams
                emptyDir: {}
        ---
        # Source: wire-server/charts/spar/templates/deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: spar
          labels:
            wireService: spar
            chart: spar-2.110.0
            release: wire-server
            heritage: Helm
        spec:
          replicas: 3
          strategy:
            type: RollingUpdate
            rollingUpdate:
              maxUnavailable: 0
              maxSurge: 3
          selector:
            matchLabels:
              wireService: spar
          template:
            metadata:
              labels:
                wireService: spar
                release: wire-server
              annotations:
                # An annotation of the configmap checksum ensures changes to the configm
        ap cause a redeployment upon `helm upgrade`
                checksum/configmap: e229d62ed9bcac9bc60222d069e8fa42205cff138eb485eccf34
        f18e4450e473
            spec:
              volumes:
                - name: "spar-config"
                  configMap:
                    name: "spar"
              containers:
                - name: spar
                  image: "quay.io/wire/spar:2.110.0"
                  imagePullPolicy: ""
                  volumeMounts:
                  - name: "spar-config"
                    mountPath: "/etc/wire/spar/conf"
                  env:
                  ports:
                    - containerPort: 8080
                  livenessProbe:
                    httpGet:
                      scheme: HTTP
                      path: /i/status
                      port: 8080
                  readinessProbe:
                    httpGet:
                      scheme: HTTP
                      path: /i/status
                      port: 8080
                  resources:
                    limits:
                      cpu: 500m
                      memory: 512Mi
                    requests:
                      cpu: 100m
                      memory: 128Mi
        ---
        # Source: wire-server/charts/webapp/templates/deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: webapp
          labels:
            wireService: webapp
            chart: webapp-2.110.0
            release: wire-server
            heritage: Helm
        spec:
          replicas: 1
          strategy:
            type: RollingUpdate
            rollingUpdate:
              maxUnavailable: 0
              maxSurge: 2
          selector:
            matchLabels:
              wireService: webapp
              app: webapp
          template:
            metadata:
              labels:
                wireService: webapp
                app: webapp
                release: wire-server
            spec:
              containers:
              - name: webapp
                image: "quay.io/wire/webapp:2021-06-01-production.0-v0.28.15-0a4d64"
                # Check variables here: https://github.com/wireapp/wire-webapp/wiki/Self
        -hosting
                env:
                  # it is vital that you don't add trailing '/' in this section!
                  - name: NODE_PORT
                    value: "8080"
                  - name: APP_BASE
                    value: "https://webapp.example.com"
                  - name: BACKEND_REST
                    value: "https://nginz-https.example.com"
                  - name: BACKEND_WS
                    value: "wss://nginz-ssl.example.com"
                  - name: APP_NAME
                    value: "Webapp"
                  - name: CSP_EXTRA_CONNECT_SRC
                    value: "https://*.example.com, wss://*.example.com"
                  - name: CSP_EXTRA_DEFAULT_SRC
                    value: "https://*.example.com"
                  - name: CSP_EXTRA_FONT_SRC
                    value: "https://*.example.com"
                  - name: CSP_EXTRA_FRAME_SRC
                    value: "https://*.example.com"
                  - name: CSP_EXTRA_IMG_SRC
                    value: "https://*.example.com"
                  - name: CSP_EXTRA_MANIFEST_SRC
                    value: "https://*.example.com"
                  - name: CSP_EXTRA_MEDIA_SRC
                    value: "https://*.example.com"
                  - name: CSP_EXTRA_OBJECT_SRC
                    value: "https://*.example.com"
                  - name: CSP_EXTRA_PREFETCH_SRC
                    value: "https://*.example.com"
                  - name: CSP_EXTRA_SCRIPT_SRC
                    value: "https://*.example.com"
                  - name: CSP_EXTRA_STYLE_SRC
                    value: "https://*.example.com"
                  - name: CSP_EXTRA_WORKER_SRC
                    value: "https://*.example.com"
                  - name: ENFORCE_HTTPS
                    value: "false"
                  - name: FEATURE_CHECK_CONSENT
                    value: "false"
                  - name: FEATURE_ENABLE_ACCOUNT_REGISTRATION
                    value: "true"
                  - name: FEATURE_ENABLE_DEBUG
                    value: "false"
                  - name: FEATURE_ENABLE_PHONE_LOGIN
                    value: "false"
                  - name: FEATURE_ENABLE_SSO
                    value: "false"
                  - name: FEATURE_SHOW_LOADING_INFORMATION
                    value: "false"
                  - name: URL_ACCOUNT_BASE
                    value: "https://account.example.com"
                  - name: URL_PRIVACY_POLICY
                    value: "https://www.example.com/terms-conditions"
                  - name: URL_SUPPORT_BASE
                    value: "https://www.example.com/support"
                  - name: URL_TEAMS_BASE
                    value: "https://teams.example.com"
                  - name: URL_TEAMS_CREATE
                    value: "https://teams.example.com"
                  - name: URL_TERMS_OF_USE_PERSONAL
                    value: "https://www.example.com/terms-conditions"
                  - name: URL_TERMS_OF_USE_TEAMS
                    value: "https://www.example.com/terms-conditions"
                  - name: URL_WEBSITE_BASE
                    value: "https://www.example.com"
                ports:
                - name: http
                  containerPort: 8080
                # NOTE: /test/ returns an HTML document a 200 response code
                readinessProbe:
                  httpGet:
                    path: /_health/
                    port: 8080
                    scheme: HTTP
                livenessProbe:
                  initialDelaySeconds: 30
                  timeoutSeconds: 3
                  httpGet:
                    path: /_health/
                    port: 8080
                    scheme: HTTP
                resources:
                    limits:
                      cpu: "1"
                      memory: 512Mi
                    requests:
                      cpu: 100m
                      memory: 128Mi
              dnsPolicy: ClusterFirst
              restartPolicy: Always
        ---
        # Source: wire-server/charts/cannon/templates/statefulset.yaml
        # Spins up pods with stable names; e.g. cannon-0 ... cannon-<replicaCount>
        # Specific pods can be accessed within the cluster at cannon-<n>.cannon.<namespa
        ce>
        # (the second 'cannon' is the name of the headless service)
        # Note: In fact, cannon-<n>.cannon can also be used to access the service but as
        suming
        # that we can have multiple namespaces accessing the same redis cluster, appendi
        ng `.<namespace>`
        # makes the service unambiguous
        apiVersion: apps/v1
        kind: StatefulSet
        metadata:
          name: cannon
          labels:
            wireService: cannon
            chart: cannon-2.110.0
            release: wire-server
            heritage: Helm
        spec:
          serviceName: cannon
          selector:
            matchLabels:
              wireService: cannon
          replicas: 1
          updateStrategy:
            type: RollingUpdate
          podManagementPolicy: Parallel
          template:
            metadata:
              labels:
                wireService: cannon
                release: wire-server
              annotations:
                checksum/configmap: 7e09622a8605894e3e6f59d44d29f6ce45f3e165b1f6277f6e13
        fcc8facd20b3
            spec:
              terminationGracePeriodSeconds: 10 # should be higher than the sleep durati
        on of preStop
              containers:
              - name: cannon
                image: "quay.io/wire/cannon:2.110.0"
                lifecycle:
                  preStop:
                    # kubernetes by default immediately sends a SIGTERM to the container
        ,
                    # which would cause cannon to exit, breaking existing websocket conn
        ections.
                    # Instead we sleep for a day. (SIGTERM is still sent, but after the
        preStop completes)
                    exec:
                      command: ["sleep", "10" ]
                volumeMounts:
                - name: empty
                  mountPath: /etc/wire/cannon/externalHost
                - name: cannon-config
                  mountPath: /etc/wire/cannon/conf
                ports:
                - name: http
                  containerPort: 8080
                readinessProbe:
                  httpGet:
                    path: /i/status
                    port: 8080
                    scheme: HTTP
                livenessProbe:
                  initialDelaySeconds: 30
                  timeoutSeconds: 1
                  httpGet:
                    path: /i/status
                    port: 8080
                    scheme: HTTP
                resources:
                    limits:
                      cpu: 500m
                      memory: 512Mi
                    requests:
                      cpu: 100m
                      memory: 256Mi
              initContainers:
              - name: cannon-configurator
                image: alpine:3.13.1
                command:
                - /bin/sh
                args:
                - -c
                # e.g. cannon-0.cannon.production
                - echo "${HOSTNAME}.cannon.default" > /etc/wire/cannon/externalHost/host
        .txt
                volumeMounts:
                - name: empty
                  mountPath: /etc/wire/cannon/externalHost
              dnsPolicy: ClusterFirst
              restartPolicy: Always
              volumes:
              - name: cannon-config
                configMap:
                  name: cannon
              - name: empty
                emptyDir: {}
        
        NOTES:
        TODO: write nice NOTES.txt
